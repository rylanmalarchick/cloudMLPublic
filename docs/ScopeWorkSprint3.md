AI Agent Scope of Work: Physics-Constrained CBH Model ValidationDocument ID: SOW-AGENT-CBH-WP-001Version: 1.0Date: 2024-11-05Authorized by: Principal Investigator (Atmospheric Research)Assigned to: Autonomous Agent ``Section 1: Agent Directive and Project Context1.1. Primary ObjectiveThe Agent's primary objective is to execute the research plan detailed in Section 5 ("Path Forward") of the project_status_report.tex.1 The goal is to operationally validate the hypothesis that physics-constrained features (specifically, shadow geometry and atmospheric thermodynamics) are essential for achieving cross-flight generalization in Cloud Base Height (CBH) retrieval.1.2. Context of Strategic Pivot (Mandatory Context)The Agent must operate with the understanding that this Scope of Work (SOW) is a remedial action. Previous modeling approaches for this project have resulted in catastrophic failure. The agent must internalize these failures to understand the criticality of the new constraints.Failure 1: Image-Only ML Fails: Models trained on image features alone, including those derived from Self-Supervised Learning (SSL), fail cross-flight validation. The project's README.md and ONE_PAGE_SUMMARY.md both confirm a final Leave-One-Flight-Out Cross-Validation (LOO CV) $R^2 < 0$.1 This indicates the model predictions are worse than simply guessing the mean.Failure 2: Solar Angles Are Confounders: Models trained on solar angles (Solar Zenith Angle, SZA; Solar Azimuth Angle, SAA) alone achieved a misleadingly high within-flight $R^2$ of $0.70$.1 However, this was proven to be a temporal confounding artifact. The angles correlated with the time of day, which in turn correlated with the diurnal evolution of clouds for that specific flight, not a generalizable physical principle.1 When validated correctly with LOO CV, the $R^2$ was $-4.46 \pm 7.09$, a catastrophic failure.1Failure 3: Reconstruction Objective is Mismatched: The SSL approach using a Masked Autoencoder (MAE) to learn image representations 1 was a fundamental failure. The project_status_report.tex identifies a "Fundamental mismatch: Reconstruction encourages learning what clouds look like; CBH requires learning where clouds are in 3D space".1Failure 4: MAE Embeddings Are Ineffective: The STRATIFIED_RESULTS_ANALYSIS.md 1 and the project report 1 provide a critical finding: randomly initialized embeddings outperformed the trained MAE embeddings when used in a downstream Gradient-Boosted Decision Tree (GBDT) model. This proves the features learned by the MAE during its reconstruction pretraining are not correlated with the geometric property of CBH.This history dictates the agent's new directive. The project no longer assumes that the existing MAE embeddings 1 are inherently valuable. The central hypothesis is that these embeddings might contain latent textural information that only becomes useful when "grounded" by the new, physically-meaningful features. Therefore, the agent must treat the new physical features (from WP-1 and WP-2) as the primary predictive drivers. The MAE features are to be treated as a supplemental, experimental input.1.3. Global Success CriterionThe Agent's work is considered successful if and only if the final hybrid model (developed in WP-4) demonstrates true generalization.Minimum Success Threshold: The "Physical Features Baseline" model (WP-3) must achieve a mean LOO CV $R^2 > 0$. This is the "go/no-go" gate for the project, as defined in the project report.1Target Success Threshold: The final hybrid model (WP-4) must achieve a mean LOO CV $R^2 > 0.3$, as specified in the Technical Analysis and Scope of Work Rec.md as the "Minimum viable" $R^2$.1Section 2: Mandated Evaluation FrameworkThis evaluation framework is non-negotiable. All validation tasks in this SOW must adhere to this protocol.2.1. Protocol: Leave-One-Flight-Out Cross-Validation (LOO CV)The Agent is forbidden from using simple random splitting or stratified splitting (e.g., from src/split_utils.py 1) for final validation. Project archives prove these methods produce "misleadingly optimistic" metrics that mask catastrophic generalization failures.1The history of this project confirms the necessity of this protocol. The CRITICAL_FINDINGS.md document 1 identified a "Test Set Imbalance" where a simple random split was 52.5% composed of a single flight (30Oct24). This imbalance led to an "impossible" result: an angle-only model with $R^2 = 0.83$ despite a near-zero (r = -0.04) true correlation. The model had not learned physics; it had learned flight-specific artifacts, specifically the temporal confounding detailed in the project report.1 When this model was re-evaluated using a rigorous LOO CV, its true performance was revealed to be $R^2 = -4.46$.1Therefore, LOO CV is the only accepted validation protocol, as it is the only method that measures the agent's ability to generalize to a new, unseen flight and its associated meteorological regime. The agent must replicate the validation logic found in scripts like validate_hybrid_loo.py.12.2. Data Splits (L_0...L_4)The agent will iterate through the 5 research flights identified in the dataset.1Dataset: 933 labeled samples total.Flights:F_0: 30 October 2024 (n=501)F_1: 10 February 2025 (n=191)F_2: 23 October 2024 (n=105)F_3: 12 February 2025 (n=92)F_4: 18 February 2025 (n=44)Iteration 1 (Fold 0): Train on [F_1, F_2, F_3, F_4], Test on F_0.Iteration 2 (Fold 1): Train on [F_0, F_2, F_3, F_4], Test on F_1.Iteration 3 (Fold 2): Train on [F_0, F_1, F_3, F_4], Test on F_2.Iteration 4 (Fold 3): Train on [F_0, F_1, F_2, F_4], Test on F_3.Iteration 5 (Fold 4): Train on [F_0, F_1, F_2, F_3], Test on F_4.2.3. Core MetricsFor each fold, the agent must compute and log the following metrics, as defined in src/evaluate_model.py 1:R² (Coefficient of Determination)MAE (Mean Absolute Error, in kilometers)RMSE (Root Mean Squared Error, in kilometers)The final report must aggregate these metrics (mean and standard deviation) across all 5 folds.Section 3: Work Package 1 (WP-1) - Geometric Feature EngineeringThis is a net-new development task. The agent will write and execute code to derive geometric features from source data.3.1. Task: Ingest Source DataInput 1 (Imagery): All labeled grayscale images (n=933).Input 2 (Metadata): All corresponding metadata, specifically Solar Zenith Angle (SZA) and Solar Azimuth Angle (SAA).3.2. Task: Implement Shadow Detection AlgorithmLogic: The agent must implement the algorithm described in the project report.1Steps:Process each image to detect cloud edges (e.g., via gradient-based methods).Process each image to detect dark shadow regions (e.g., via adaptive thresholding or segmentation).Identify corresponding cloud-shadow pairs, likely by projecting cloud edges along the known solar azimuth direction.Constraint (Risk Mitigation): The agent must be aware of and robust to the "Challenges" identified in the project report 1:Challenge 1: Low-contrast surfaces (e.g., open ocean) where shadows are not visible.Challenge 2: Multi-layer clouds, which cause ambiguous shadow attribution.Challenge 3: Broken cloud fields, which have ambiguous or non-existent shadow edges.The agent's algorithm must output a "confidence" score (e.g., 0.0-1.0) for each shadow detection and flag samples as infeasible (confidence = 0) where no robust detection is possible.3.3. Task: Derive Geometric CBH FeaturesLogic: For all samples with a high-confidence (e.g., > 0.5) shadow detection, the agent will implement the geometric formula from the project report.1Formula: $H = L \cdot \tan(90^{\circ} - SZA) \cdot \text{scale factor}$$H$: Derived Cloud Base Height.$L$: Estimated shadow length (in pixels).$SZA$: Solar Zenith Angle (from metadata).$\text{scale factor}$: Agent must compute this factor (in meters/pixel) based on imaging geometry (e.g., aircraft altitude, sensor field of view, pixel pitch).Output Features: The agent will generate a feature vector including, but not limited to, derived_geometric_H, shadow_length_pixels, and shadow_detection_confidence.3.4. Deliverable: Geometric Feature Set (WP1_Features.hdf5)Format: An HDF5 or Parquet file indexed by sample ID.Content: Contains the derived geometric features for all 933 labeled samples (using NaN or 0 for low-confidence detections).Section 4: Work Package 2 (WP-2) - Atmospheric Feature EngineeringThis is a net-new development task. The agent will acquire, process, and align external atmospheric data.4.1. Task: Acquire ERA5 Reanalysis DataSource: ERA5 Reanalysis (e.g., via Copernicus Climate Data Store (CDS) API).Specification: 0.25° resolution, hourly data, as specified in the project report.1Scope: The agent will download multi-level data cubes (surface and pressure levels) that temporally and spatially encompass all 5 flight tracks.4.2. Task: Preprocess and Derive Thermodynamic VariablesLogic: The agent will process the downloaded GRIB/NetCDF data cubes to extract or derive the features specified in the project report.1Key Features to Derive:BLH (Boundary Layer Height): A direct variable from ERA5.LCL (Lifting Condensation Level): Computed from surface temperature ($T_{\text{surface}}$) and dewpoint temperature ($T_{\text{dewpoint}}$) using the parcel theory formula: $\text{LCL} = \frac{T_{\text{surface}} - T_{\text{dewpoint}}}{8 \text{ K/km}}$ (or a more precise thermodynamic formulation).1Inversion_Height: The altitude of the strongest temperature gradient ($dT/dz$) in the vertical profile, indicating the top of the boundary layer.Moisture_Gradient: The vertical gradient of specific humidity ($dq/dz$) at potential cloud base levels.Stability_Index: A measure of atmospheric stability, such as the Bulk Richardson number or the lapse rate ($dT/dz$) in the lower troposphere.4.3. Task: Align and Integrate FeaturesConstraint: The agent must programmatically resolve the "Spatial resolution mismatch (25 km ERA5 vs. 200 m imagery)".1Logic: For each of the 933 high-resolution samples (defined by latitude, longitude, and timestamp), the agent will perform a 4D spatio-temporal interpolation (e.g., nearest-neighbor or linear interpolation) against the coarse ERA5 data cube to find the corresponding atmospheric state vector.4.4. Deliverable: Atmospheric Feature Set (WP2_Features.hdf5)Format: An HDF5 or Parquet file indexed by sample ID.Content: Contains the derived atmospheric features (BLH, LCL, Inversion_Height, etc.) for all 933 labeled samples.Section 5: Work Package 3 (WP-3) - Physical Baseline Model ValidationThis Work Package is the critical control experiment to validate the new features in isolation. This is the "go/no-go" gate for the project's central hypothesis. All previous models (Angles-Only, MAE-Hybrid) have failed LOO CV with $R^2 < 0$. This WP-3 tests the hypothesis that physical features alone can provide a generalizable signal.If this "Physical Baseline" model also fails (i.e., $R^2 < 0$), the core hypothesis is incorrect, and the project requires a new "Path Forward." If it succeeds ($R^2 > 0$), it validates the new features and provides the first credible baseline in the project's history.15.1. Task: Train Physical Baseline ModelModel: Gradient-Boosted Decision Trees (GBDT), consistent with previous project experiments.1Input Features: A feature vector combining ``. No image features or angle features are permitted in this task.Training: The agent will train 5 separate GBDT models, one for each fold of the LOO CV (as defined in Section 2.2).5.2. Task: Execute Mandated Evaluation FrameworkLogic: The agent will apply the protocol from Section 2 to the trained models, testing each one on its held-out flight.5.3. Deliverable: Physical Baseline Validation Report (WP3_Report.json)Format: A JSON file detailing the metrics for this model.Content: Must include aggregate $R^2$ (mean, std), $MAE$ (mean, std), $RMSE$ (mean, std), and all per-fold metrics.Constraint: If the final mean $R^2 < 0$, the agent must halt and report failure of the SOW's primary hypothesis.Section 6: Work Package 4 (WP-4) - Hybrid Model Integration and ValidationUpon successful completion of WP-3 (mean $R^2 > 0$), the agent will proceed to build the final hybrid models.6.1. Task: Ingest All Feature SetsInput 1 (Geometric): WP1_Features.hdf5 (from WP-1)Input 2 (Atmospheric): WP2_Features.hdf5 (from WP-2)Input 3 (Metadata): Solar Angles (SZA, SAA)Input 4 (Image Embeddings): The agent will load the pretrained MAE encoder (outputs/mae_pretrain/mae_encoder_pretrained.pth) 1 and extract image features.A critical directive for this task is the method of feature extraction. The project history has definitively proven that the token is an "information bottleneck" and "ineffective".1 The `Technical Analysis` document explicitly recommends: "Extract spatial feature maps from MAE encoder" and "Use patch tokens instead of CLS token".1 The next logical step, as explored in the project report 1, is to use a spatial-aware representation. Therefore, the agent is directed to **avoid** the token and instead derive its image feature vector using global average pooling of all output patch tokens from the MAE encoder.6.2. Task: Train Hybrid GBDT ModelsLogic: The agent will train and validate (per Section 2) multiple model variants to perform a rigorous ablation study, as defined in the project report.1Variants to be Trained (5-fold LOO CV for each):M_PHYSICAL_ONLY: `` (This is a re-run of WP-3, serving as a control).M_PHYSICAL_ANGLES: ``M_PHYSICAL_MAE: ``M_HYBRID_FULL: ``6.3. Task: Execute Mandated Evaluation FrameworkLogic: The agent will apply the protocol from Section 2 to all model variants trained in Task 6.2.6.4. Deliverable: Final Hybrid Validation Report (WP4_Report.json)Format: A JSON file containing the aggregated and per-fold metrics for all variants (M_PHYSICAL_ONLY, M_PHYSICAL_ANGLES, M_PHYSICAL_MAE, M_HYBRID_FULL).Section 7: Final Agent Deliverables and ArtifactsUpon completion of all Work Packages, the agent will compile and deposit the following artifacts.7.1. Integrated Feature Store (final_features.hdf5)A single, consolidated HDF5 or Parquet file containing all 933 labeled samples and all engineered features:Geometric_Features (from WP-1)Atmospheric_Features (from WP-2)MAE_Spatial_Embeddings (from WP-4)Metadata_Angles (SZA, SAA)Target (CBH)7.2. Trained Model ArtifactsA directory (/models/final_gbdt_models/) containing the 5 pickled GBDT models (one per LOO fold) for the best-performing variant (expected to be M_HYBRID_FULL).7.3. Comprehensive Validation Report (SOW_Validation_Summary.json)A JSON or CSV file containing the final, comprehensive comparison table. This table is the primary deliverable of this SOW and must adhere to the following schema. This table's structure is designed to directly contrast the agent's new, validated models (M4, M5) against the known, failed baselines (M1-M3) documented in the project's research archive, providing a clear "before and after" summary of this strategic pivot.Table 7.3a: Mandated Final Report Table (Primary Deliverable)Title: LOO CV Model Performance: Physics-Constrained vs. BaselineDescription: This table summarizes the agent's findings, comparing the new models (M4, M5) against the known, failed baselines (M1, M2, M3) using the mandated LOO CV protocol.Model IDModel DescriptionFeature SetMean R² (LOO CV)Mean MAE (km)Mean RMSE (km)Old Baselines (Failed)M1Angles-Only GBDT[Angles]-4.460.35[Agent to compute]M2MAE CLS Hybrid GBDT``[Agent to compute][Agent to compute][Agent to compute]M3Spatial MAE (Attn)``-3.920.9[Agent to compute]New Models (Agent Validated)M4Physical Baseline (from WP-3)[Geometric + Atmospheric][Agent to compute][Agent to compute][Agent to compute]M5Full Hybrid Model (from WP-4)``[Agent to compute][Agent to compute][Agent to compute]7.4. Feature Importance Report (WP4_Feature_Importance.json)Logic: For the best-performing model (M_HYBRID_FULL), the agent will run a permutation importance analysis, consistent with the methodology in scripts/analyze_feature_importance.py.1Content: A ranked list of the top 20 most important features, with each feature explicitly categorized as Geometric, Atmospheric, Angle, or MAE.Purpose: This deliverable is required to confirm why the new model works—i.e., to prove that it is relying on the new physical features for its predictions, thus validating the project's core hypothesis.