{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# NASA CloudML - Colab Training\n",
    "\n",
    "Train deep learning models for cloud optical depth prediction from satellite IR imagery.\n",
    "\n",
    "## Quick Start\n",
    "\n",
    "1. Run setup cells to install dependencies and mount Drive\n",
    "2. Run diagnostics to determine if task is learnable\n",
    "3. Choose your training configuration (configs/colab_optimized_full_tuned.yaml recommended)\n",
    "4. Run training cell\n",
    "5. View results in Google Drive under CloudML/results/\n",
    "\n",
    "**Current Status:** Phase 1 fixes applied (variance-preserving loss, proper initialization)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71d849b2"
   },
   "source": [
    "## Setup (Run Once Per Session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fa7b2e4b"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 1: Mount Google Drive\n",
    "# ============================================================================\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create project directories\n",
    "!mkdir -p /content/drive/MyDrive/CloudML/data\n",
    "!mkdir -p /content/drive/MyDrive/CloudML/models\n",
    "!mkdir -p /content/drive/MyDrive/CloudML/plots\n",
    "!mkdir -p /content/drive/MyDrive/CloudML/logs\n",
    "\n",
    "print(\"OK: Google Drive mounted successfully\")\n",
    "print(\"OK: Project directories created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ef3e03b9"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 2: Clone/Update Repository\n",
    "# ============================================================================\n",
    "%cd /content\n",
    "\n",
    "if not os.path.exists('/content/repo'):\n",
    "    print('Cloning repository...')\n",
    "    !git clone https://github.com/rylanmalarchick/cloudMLPublic.git repo\n",
    "else:\n",
    "    print('Repository exists. Pulling latest changes...')\n",
    "    %cd /content/repo\n",
    "    !git pull origin main\n",
    "\n",
    "%cd /content/repo\n",
    "print(\"OK: Repository ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "74070521"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 3: Install Dependencies\n",
    "# ============================================================================\n",
    "print(\"Installing dependencies (this may take 5-10 minutes)...\\n\")\n",
    "\n",
    "# Install PyTorch with CUDA 12.1 support\n",
    "!pip install --quiet torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# Clean up potential conflicts\n",
    "!pip uninstall -y -q mamba-ssm causal-conv1d 2>/dev/null\n",
    "\n",
    "# Install core dependencies\n",
    "!pip install --quiet h5py==3.14.0 netCDF4==1.7.2 pyhdf==0.11.6 scikit-learn matplotlib plotly pyyaml pandas\n",
    "\n",
    "# Install advanced components\n",
    "!pip install --quiet torch_geometric==2.5.3\n",
    "!pip install --quiet causal-conv1d==1.4.0\n",
    "!pip install --quiet mamba-ssm==2.2.2\n",
    "\n",
    "print(\"\\nOK: All dependencies installed successfully\")\n",
    "print(\"\\nVerifying installation...\")\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d69509ec"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 4: Verify Data\n",
    "# ============================================================================\n",
    "import os\n",
    "\n",
    "data_dir = '/content/drive/MyDrive/CloudML/data/'\n",
    "\n",
    "# Expected flights\n",
    "flights = ['10Feb25', '30Oct24', '04Nov24', '23Oct24', '18Feb25', '12Feb25']\n",
    "\n",
    "print(\"Checking data availability...\\n\")\n",
    "missing_data = []\n",
    "\n",
    "for flight in flights:\n",
    "    flight_path = os.path.join(data_dir, flight)\n",
    "    if os.path.exists(flight_path):\n",
    "        files = os.listdir(flight_path)\n",
    "        has_h5 = any(f.endswith('.h5') for f in files)\n",
    "        has_hdf5 = any(f.endswith('.hdf5') for f in files)\n",
    "        has_hdf = any(f.endswith('.hdf') for f in files)\n",
    "        \n",
    "        if has_h5 and has_hdf5 and has_hdf:\n",
    "            print(f\"OK: {flight}: All files present\")\n",
    "        else:\n",
    "            print(f\"WARNING: {flight}: Missing files (h5={has_h5}, hdf5={has_hdf5}, hdf={has_hdf})\")\n",
    "            missing_data.append(flight)\n",
    "    else:\n",
    "        print(f\"Not found: {flight}: Folder not found\")\n",
    "        missing_data.append(flight)\n",
    "\n",
    "if missing_data:\n",
    "    print(f\"\\nWARNING: {len(missing_data)} flight(s) missing data\")\n",
    "    print(\"Training will proceed with available flights only.\")\n",
    "else:\n",
    "    print(\"\\nOK: All data verified successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "diagnostics_header"
   },
   "source": [
    "## Diagnostics (Determine if Task is Learnable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "correlation_analysis"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DIAGNOSTIC 1: Correlation Analysis (30 min)\n",
    "# ============================================================================\n",
    "# This checks if ANY features correlate with optical depth\n",
    "# If no correlations, the task may not be learnable from this data\n",
    "%cd /content/repo\n",
    "\n",
    "print(\"Running Correlation Analysis...\")\n",
    "print(\"This will extract 28 hand-crafted features and compute correlations\")\n",
    "print(\"Expected runtime: ~30 minutes\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "!python diagnostics/1_correlation_analysis.py\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CORRELATION ANALYSIS COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(\"Check results in diagnostics/results/correlation_summary.json\")\n",
    "print(\"If max r² < 0.05, the task may not be learnable from this data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "baseline_models"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DIAGNOSTIC 2: Simple Baseline Models (1 hour)\n",
    "# ============================================================================\n",
    "# This tests if Ridge, Random Forest, etc. can beat mean baseline\n",
    "# If simple models get R² > 0, deep learning should work too\n",
    "%cd /content/repo\n",
    "\n",
    "print(\"Running Simple Baseline Models...\")\n",
    "print(\"This will test 7 classical ML models on hand-crafted features\")\n",
    "print(\"Expected runtime: ~1 hour\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "!python diagnostics/2_simple_baselines.py\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BASELINE MODELS COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(\"Check results in diagnostics/results/baseline_summary.json\")\n",
    "print(\"If best R² > 0, signal exists - proceed to neural network training\")\n",
    "print(\"If best R² < 0, no model can learn - need different data/features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training_header"
   },
   "source": [
    "## Training (Run After Diagnostics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "31aa68e9"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PHASE 1 EXPERIMENT: VARIANCE COLLAPSE FIX (SHORT VALIDATION RUN)\n",
    "# ============================================================================\n",
    "\n",
    "# STEP 1: Pull latest code\n",
    "print(\"Pulling latest code from main branch...\")\n",
    "%cd /content/repo\n",
    "!git pull origin main\n",
    "print(\"OK: Code updated\\n\")\n",
    "\n",
    "# STEP 2: Start Phase 1 experiment\n",
    "import datetime\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "experiment_name = f\"phase1_variance_fix_{timestamp}\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 1: VARIANCE COLLAPSE FIX EXPERIMENT\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Experiment ID: {experiment_name}\")\n",
    "print(f\"Config: phase_1_variance_fix.yaml\")\n",
    "print(f\"\\nEXPERIMENT OBJECTIVE:\")\n",
    "print(f\"  Validate that variance loss can prevent model collapse.\")\n",
    "print(f\"  This is a SHORT run (10 epochs) to test the fix.\")\n",
    "print(f\"\\nKEY SETTINGS:\")\n",
    "print(f\"  - Epochs: 10 (validation run only)\")\n",
    "print(f\"  - variance_lambda: 0.5 (Phase 1b: reduced from 2.0 to prevent loss explosion)\")\n",
    "print(f\"  - min_variance_ratio: 0.1 (safety net - stops if variance collapses)\")\n",
    "print(f\"\\nPHASE 1B IMPROVEMENTS:\")\n",
    "print(f\"  - Reduced variance_lambda to prevent loss explosion\")\n",
    "print(f\"  - Fixed R² calculation to use unscaled (km) values, not z-scores\")\n",
    "print(f\"  - Added debug logging for prediction ranges\")\n",
    "print(f\"\\nBASELINE PERFORMANCE (from diagnostics):\")\n",
    "print(f\"  - GradientBoosting: R² = 0.777 (simple model on hand-crafted features)\")\n",
    "print(f\"  - Previous NN runs: R² < 0 (all negative - model collapsed)\")\n",
    "print(f\"\\nEXPECTED OUTCOME:\")\n",
    "print(f\"  - Loss values should be reasonable (~0.5-2.0, not thousands)\")\n",
    "print(f\"  - R² should be calculated on real km values (not z-scores)\")\n",
    "print(f\"  - If fix works: R² should be POSITIVE and INCREASING over epochs\")\n",
    "print(f\"  - Target for this short run: R² > 0.1\")\n",
    "print(f\"\\nExpected Runtime: ~15-20 minutes\")\n",
    "print(f\"Expected GPU Usage: ~10-12GB\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nTraining started... Monitor GPU with: !nvidia-smi\\n\")\n",
    "\n",
    "%cd /content/repo\n",
    "!python main.py \\\n",
    "    --config configs/phase_1_variance_fix.yaml \\\n",
    "    --save_name {experiment_name} \\\n",
    "    --epochs 10\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 1 EXPERIMENT COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nNEXT STEPS:\")\n",
    "print(f\"  1. Check the DEBUG output showing scaled/unscaled prediction ranges\")\n",
    "print(f\"  2. Verify loss values are reasonable (~0.5-2.0, not thousands)\")\n",
    "print(f\"  3. Check if R² is positive and increasing\")\n",
    "print(f\"  4. If R² > 0.1: GREAT! Scale up to 50 epochs with hyperparameter tuning\")\n",
    "print(f\"  5. If R² still negative: Try Phase 2 (simpler architecture without attention)\")\n",
    "print(f\"\\nModel saved to: /content/drive/MyDrive/CloudML/models/trained/{experiment_name}.pth\")\n",
    "print(f\"Results saved to: /content/drive/MyDrive/CloudML/plots/\")\n",
    "print(f\"Logs saved to: /content/drive/MyDrive/CloudML/logs/\")\n",
    "print(\"\\nCheck TensorBoard: %load_ext tensorboard\")\n",
    "print(\"                   %tensorboard --logdir /content/drive/MyDrive/CloudML/logs/tensorboard/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "phase2_simple_cnn"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PHASE 2 EXPERIMENT: SIMPLE CNN BASELINE (NO ATTENTION)\n",
    "# ============================================================================\n",
    "\n",
    "# STEP 1: Pull latest code\n",
    "print(\"Pulling latest code from main branch...\")\n",
    "%cd /content/repo\n",
    "!git pull origin main\n",
    "print(\"OK: Code updated\\n\")\n",
    "\n",
    "# STEP 2: Start Phase 2 experiment\n",
    "import datetime\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "experiment_name = f\"phase2_simple_cnn_{timestamp}\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 2: SIMPLE CNN BASELINE EXPERIMENT\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Experiment ID: {experiment_name}\")\n",
    "print(f\"Config: phase_2_simple_cnn.yaml\")\n",
    "print(f\"\\nEXPERIMENT OBJECTIVE:\")\n",
    "print(f\"  Test if the complex transformer/attention architecture is preventing learning.\")\n",
    "print(f\"  Use a simple CNN baseline to establish if the task is learnable at all.\")\n",
    "print(f\"\\nKEY SETTINGS:\")\n",
    "print(f\"  - Architecture: SimpleCNNModel (no attention, no transformer)\")\n",
    "print(f\"  - Epochs: 10 (validation run only)\")\n",
    "print(f\"  - variance_lambda: 0.5 (keep variance loss)\")\n",
    "print(f\"  - min_variance_ratio: 0.1 (safety net)\")\n",
    "print(f\"\\nPHASE 1 RESULTS:\")\n",
    "print(f\"  - Variance ratio: GOOD (88-111% - not collapsed)\")\n",
    "print(f\"  - R²: BAD (still negative: -0.89 to -1.16)\")\n",
    "print(f\"  - Loss: BAD (600-900, should be <2.0)\")\n",
    "print(f\"  - Predictions: OFF (some negative values, wrong scale)\")\n",
    "print(f\"\\nHYPOTHESIS:\")\n",
    "print(f\"  Complex architecture cannot learn. Try simpler CNN first.\")\n",
    "print(f\"\\nEXPECTED OUTCOME:\")\n",
    "print(f\"  - Loss should drop significantly (< 2.0 for scaled data)\")\n",
    "print(f\"  - R² should be POSITIVE and INCREASING\")\n",
    "print(f\"  - Predictions should be in valid range (0.1-2.0 km, no negatives)\")\n",
    "print(f\"\\nExpected Runtime: ~15-20 minutes\")\n",
    "print(f\"Expected GPU Usage: ~8-10GB (less than Phase 1)\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nTraining started... Monitor GPU with: !nvidia-smi\\n\")\n",
    "\n",
    "%cd /content/repo\n",
    "!python main.py \\\n",
    "    --config configs/phase_2_simple_cnn.yaml \\\n",
    "    --save_name {experiment_name} \\\n",
    "    --epochs 10\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 2 EXPERIMENT COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nNEXT STEPS:\")\n",
    "print(f\"  1. Check the DEBUG output and loss values\")\n",
    "print(f\"  2. If R² is positive: GREAT! Simple CNN works.\")\n",
    "print(f\"     -> Gradually add complexity (spatial attention first, then temporal)\")\n",
    "print(f\"  3. If R² still negative: Problem is deeper.\")\n",
    "print(f\"     -> Check data pipeline, preprocessing, or data quality\")\n",
    "print(f\"\\nModel saved to: /content/drive/MyDrive/CloudML/models/trained/{experiment_name}.pth\")\n",
    "print(f\"Results saved to: /content/drive/MyDrive/CloudML/plots/\")\n",
    "print(f\"Logs saved to: /content/drive/MyDrive/CloudML/logs/\")\n",
    "print(\"\\nCheck TensorBoard: %load_ext tensorboard\")\n",
    "print(\"                   %tensorboard --logdir /content/drive/MyDrive/CloudML/logs/tensorboard/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "13c7837b"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# OPTION A: FULL MODEL + OPTIMIZATIONS (ORIGINAL)\n",
    "# ============================================================================\n",
    "import datetime\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "baseline_name = f\"baseline_full_{timestamp}\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TRAINING FULL MODEL WITH OPTIMIZATIONS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Experiment ID: {baseline_name}\")\n",
    "print(f\"Config: colab_optimized_full.yaml\")\n",
    "print(f\"Model: 64/128/256 channels (FULL)\")\n",
    "print(f\"Optimizations: Gradient Checkpointing + torch.compile('default' mode)\")\n",
    "print(f\"Expected Runtime: 2-2.5 hours (faster with compile)\")\n",
    "print(f\"Expected GPU Usage: ~9-10GB (batch_size=20)\")\n",
    "print(f\"CUDA Graph Fix: Using 'default' compile mode (compatible with checkpointing)\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nTraining started... Monitor GPU with: !nvidia-smi\\n\")\n",
    "\n",
    "!python main.py \\\n",
    "    --config configs/colab_optimized_full.yaml \\\n",
    "    --save_name {baseline_name} \\\n",
    "    --epochs 50\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FULL MODEL TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Model saved to: /content/drive/MyDrive/CloudML/models/trained/{baseline_name}.pth\")\n",
    "print(f\"Results saved to: /content/drive/MyDrive/CloudML/logs/\")\n",
    "print(f\"Logs saved to: /content/drive/MyDrive/CloudML/plots/\")\n",
    "print(\"\\nCheck TensorBoard: %load_ext tensorboard\")\n",
    "print(\"                   %tensorboard --logdir /content/drive/MyDrive/CloudML/logs/tensorboard/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f8924f37"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# OPTION B: FULL MODEL - MAXIMUM STABILITY (NEW - NO torch.compile)\n",
    "# ============================================================================\n",
    "import datetime\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "baseline_name = f\"baseline_full_stable_{timestamp}\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TRAINING FULL MODEL - STABLE MODE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Experiment ID: {baseline_name}\")\n",
    "print(f\"Config: colab_full_stable.yaml\")\n",
    "print(f\"Model: 64/128/256 channels (FULL)\")\n",
    "print(f\"Optimizations: Gradient Checkpointing only (NO torch.compile)\")\n",
    "print(f\"Expected Runtime: ~3 hours (no compile speedup)\")\n",
    "print(f\"Expected GPU Usage: ~8-9GB (batch_size=16)\")\n",
    "print(f\"Stability: MAXIMUM (no CUDA graph issues)\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nTraining started... Monitor GPU with: !nvidia-smi\\n\")\n",
    "\n",
    "!python main.py \\\n",
    "    --config configs/colab_full_stable.yaml \\\n",
    "    --save_name {baseline_name} \\\n",
    "    --epochs 50\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FULL MODEL STABLE TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Model saved to: /content/drive/MyDrive/CloudML/models/trained/{baseline_name}.pth\")\n",
    "print(f\"Results saved to: /content/drive/MyDrive/CloudML/plots/\")\n",
    "print(f\"Logs saved to: /content/drive/MyDrive/CloudML/logs/\")\n",
    "print(\"\\nCheck TensorBoard: %load_ext tensorboard\")\n",
    "print(\"                   %tensorboard --logdir /content/drive/MyDrive/CloudML/logs/tensorboard/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5aef5561"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# OPTION C: MEMORY-OPTIMIZED MODEL (FALLBACK IF OOM)\n",
    "# ============================================================================\n",
    "import datetime\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "baseline_name = f\"baseline_memopt_{timestamp}\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TRAINING MEMORY-OPTIMIZED MODEL\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Experiment ID: {baseline_name}\")\n",
    "print(f\"Config: colab_optimized.yaml\")\n",
    "print(f\"Model: 32/64/128 channels (memory-optimized)\")\n",
    "print(f\"Expected Runtime: 2.5-3 hours\")\n",
    "print(f\"Expected GPU Usage: ~7-8GB (batch_size=16)\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nTraining started... Monitor GPU with: !nvidia-smi\\n\")\n",
    "\n",
    "!python main.py \\\n",
    "    --config configs/colab_optimized.yaml \\\n",
    "    --save_name {baseline_name} \\\n",
    "    --epochs 50\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MEMORY-OPTIMIZED TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Model saved to: /content/drive/MyDrive/CloudML/models/trained/{baseline_name}.pth\")\n",
    "print(f\"Results saved to: /content/drive/MyDrive/CloudML/plots/\")\n",
    "print(f\"Logs saved to: /content/drive/MyDrive/CloudML/logs/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "utils_header"
   },
   "source": [
    "## Utilities & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ee6f7ee"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# AGGREGATE RESULTS FOR PAPER\n",
    "# ============================================================================\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "print(\"Aggregating results...\\n\")\n",
    "\n",
    "# Find all CSV result files\n",
    "results_dir = '/content/drive/MyDrive/CloudML/logs/csv/'\n",
    "csv_files = glob.glob(os.path.join(results_dir, '*.csv'))\n",
    "\n",
    "if csv_files:\n",
    "    all_results = []\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        exp_name = os.path.basename(csv_file).replace('.csv', '')\n",
    "        df['experiment'] = exp_name\n",
    "        all_results.append(df)\n",
    "    \n",
    "    # Combine all results\n",
    "    combined = pd.concat(all_results, ignore_index=True)\n",
    "    \n",
    "    # Save combined results\n",
    "    output_path = '/content/drive/MyDrive/CloudML/all_results_combined.csv'\n",
    "    combined.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"OK: Combined {len(csv_files)} result files\")\n",
    "    print(f\"OK: Saved to: {output_path}\")\n",
    "    print(\"\\nSummary Statistics by Experiment:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Group by experiment and show key metrics\n",
    "    summary = combined.groupby('experiment').agg({\n",
    "        'mae': 'mean',\n",
    "        'rmse': 'mean',\n",
    "        'r2': 'mean'\n",
    "    }).round(4)\n",
    "    \n",
    "    print(summary)\n",
    "    print(\"\\nOK: Use this table for your paper!\")\n",
    "    \n",
    "else:\n",
    "    print(\"No result files found. Make sure experiments have completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "59e69f2d"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# GPU MONITORING (Run in parallel with training)\n",
    "# ============================================================================\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def monitor_gpu(duration=300, interval=5):\n",
    "    \"\"\"Monitor GPU usage for specified duration\"\"\"\n",
    "    for i in range(duration // interval):\n",
    "        clear_output(wait=True)\n",
    "        print(f\"GPU Monitoring (updating every {interval}s, {i*interval}/{duration}s elapsed)\\n\")\n",
    "        !nvidia-smi --query-gpu=timestamp,memory.used,memory.total,utilization.gpu,temperature.gpu --format=csv\n",
    "        time.sleep(interval)\n",
    "\n",
    "# Run for 5 minutes\n",
    "monitor_gpu(duration=300, interval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "93112d00"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VERIFY TENSORBOARD LOGS EXIST\n",
    "# ============================================================================\n",
    "import os\n",
    "\n",
    "tb_dir = \"/content/drive/MyDrive/CloudML/logs/tensorboard/\"\n",
    "\n",
    "if os.path.exists(tb_dir):\n",
    "    runs = [d for d in os.listdir(tb_dir) if os.path.isdir(os.path.join(tb_dir, d))]\n",
    "    if runs:\n",
    "        print(f\"OK: Found {len(runs)} TensorBoard run(s):\")\n",
    "        for run in sorted(runs):\n",
    "            run_path = os.path.join(tb_dir, run)\n",
    "            files = os.listdir(run_path)\n",
    "            event_files = [f for f in files if 'events.out.tfevents' in f]\n",
    "            print(f\"  - {run}: {len(event_files)} event file(s)\")\n",
    "        print(f\"\\nOK: Ready to launch TensorBoard!\")\n",
    "    else:\n",
    "        print(f\"Not found: TensorBoard directory exists but is empty: {tb_dir}\")\n",
    "        print(\"  Run a training session first.\")\n",
    "else:\n",
    "    print(f\"Not found: TensorBoard directory not found: {tb_dir}\")\n",
    "    print(\"  Make sure:\")\n",
    "    print(\"  1. You've run a training session\")\n",
    "    print(\"  2. Files are saving to Google Drive (check config paths)\")\n",
    "    print(\"  3. Google Drive is mounted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5cc861ce"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TENSORBOARD (View training curves)\n",
    "# ============================================================================\n",
    "# Run this cell to launch TensorBoard in the notebook\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /content/drive/MyDrive/CloudML/logs/tensorboard/\n",
    "\n",
    "# If you see \"No dashboards are active\", it means:\n",
    "# 1. No training runs have been completed yet, OR\n",
    "# 2. Files are not saving to Drive (check verification cell above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "461a0e0e"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LIST ALL TRAINED MODELS\n",
    "# ============================================================================\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "models_dir = '/content/drive/MyDrive/CloudML/models/trained/'\n",
    "\n",
    "if os.path.exists(models_dir):\n",
    "    models = sorted(os.listdir(models_dir))\n",
    "    \n",
    "    print(f\"\\nTrained Models ({len(models)} total)\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"{'Model Name':<60} {'Size (MB)':<15} {'Modified':<20}\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    for model in models:\n",
    "        path = os.path.join(models_dir, model)\n",
    "        size_mb = os.path.getsize(path) / (1024 * 1024)\n",
    "        mtime = datetime.fromtimestamp(os.path.getmtime(path))\n",
    "        print(f\"{model:<60} {size_mb:>10.1f} MB   {mtime.strftime('%Y-%m-%d %H:%M')}\")\n",
    "    \n",
    "    print(\"=\"*100)\n",
    "else:\n",
    "    print(\"Models directory not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d6b90607"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DOWNLOAD RESULTS (Optional - already in Drive)\n",
    "# ============================================================================\n",
    "from google.colab import files\n",
    "\n",
    "# Zip and download results\n",
    "print(\"Zipping results for download...\")\n",
    "\n",
    "!cd /content/drive/MyDrive/CloudML && \\\n",
    "    zip -r results_export.zip plots/ logs/csv/ models/trained/ *.json *.csv 2>/dev/null\n",
    "\n",
    "print(\"\\nOK: Results zipped\")\n",
    "print(\"Downloading... (this may take a few minutes)\")\n",
    "\n",
    "files.download('/content/drive/MyDrive/CloudML/results_export.zip')\n",
    "\n",
    "print(\"\\nOK: Download complete!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
