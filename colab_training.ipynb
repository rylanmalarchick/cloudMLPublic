{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b176d0f6"
   },
   "source": [
    "# NASA Cloud-ML Training on Google Colab\n",
    "\n",
    "**Paper-Quality Baseline & Ablation Studies**\n",
    "\n",
    "This notebook is optimized for producing publication-ready results on Google Colab's T4 GPU (15GB VRAM).\n",
    "\n",
    "## üö® IMPORTANT: CUDA Graph Fix Applied\n",
    "\n",
    "**If you previously got \"RuntimeError: static input data pointer changed\":**\n",
    "- ‚úÖ **FIXED** - Configs updated to use `torch_compile_mode: \"default\"` (compatible with gradient checkpointing)\n",
    "- ‚úÖ **NEW** - Added `colab_full_stable.yaml` config (no torch.compile, maximum stability)\n",
    "- See troubleshooting section below for details\n",
    "\n",
    "## Config Quick Reference\n",
    "\n",
    "| Config | Model | torch.compile | Batch | Memory | Speed | Stability | TIER 1 |\n",
    "|--------|-------|---------------|-------|--------|-------|-----------|--------|\n",
    "| **colab_optimized_full_tuned.yaml** | 64/128/256 | ‚úÖ (default) | 20 | 10-12GB | ‚ö° Fast | ‚úÖ Good | ‚úÖ YES |\n",
    "| **colab_optimized_full.yaml** | 64/128/256 | ‚úÖ (default) | 20 | 9-10GB | ‚ö° Fast | ‚úÖ Good | ‚ùå No |\n",
    "| **colab_full_stable.yaml** | 64/128/256 | ‚ùå | 16 | 8-9GB | Normal | ‚úÖ‚úÖ Best | ‚ùå No |\n",
    "| **colab_optimized.yaml** | 32/64/128 | ‚ùå | 16 | 7-8GB | Normal | ‚úÖ Good | ‚ùå No |\n",
    "\n",
    "**üéØ TIER 1 READY:** Use **colab_optimized_full_tuned.yaml** (Option A-Tuned) for literature-backed improvements (+15-25% R¬≤ expected)!\n",
    "\n",
    "**Recommendation:** Start with **colab_optimized_full_tuned.yaml** (Option A-Tuned). If you get OOM errors, reduce `batch_size` to 16 or use **colab_optimized.yaml**.\n",
    "\n",
    "## What This Notebook Does\n",
    "\n",
    "1. **TIER 1 Training** (NEW!) - Literature-backed improvements: multi-scale attention + self-supervised pre-training\n",
    "2. **Strong Baseline Training** - Train a high-quality model with optimal hyperparameters\n",
    "3. **Comprehensive Ablation Studies** - Systematic evaluation of each component\n",
    "4. **GPU-Optimized** - Maximizes T4 GPU utilization (~10-12GB usage vs default 3.7GB)\n",
    "5. **Reproducible Results** - Fixed seeds, detailed logging, automatic checkpointing\n",
    "\n",
    "## Training Pipeline\n",
    "\n",
    "**TIER 1 Training (NEW - colab_optimized_full_tuned.yaml):**\n",
    "- **Self-Supervised Phase** (20 epochs): Encoder learns spatial features via image reconstruction\n",
    "- **Supervised Pre-training**: Model learns on primary flight (30Oct24)\n",
    "- **Final Training**: Fine-tunes on all flights with overweighting\n",
    "- **Validation**: Held-out flight (12Feb25) for unbiased evaluation\n",
    "\n",
    "**Baseline Training (colab_optimized_full.yaml):**\n",
    "- **Pre-training Phase**: Model learns on primary flight (30Oct24) - establishes feature representations\n",
    "- **Final Training Phase**: Fine-tunes on all flights with overweighting to retain learned features\n",
    "- **Validation**: Held-out flight (12Feb25) for unbiased evaluation\n",
    "- **LOO Cross-Validation** (Optional): Each flight held out once for robust generalization assessment\n",
    "\n",
    "## Expected Runtime\n",
    "\n",
    "- **TIER 1 Training**: ~3-4 hours (20 epochs pre-training + 50 epochs supervised)\n",
    "- **Baseline Training**: ~2-3 hours (50 epochs with early stopping)\n",
    "- **Full Ablation Suite**: ~6-8 hours (8 experiments √ó ~45 min each)\n",
    "- **With LOO CV**: ~8-12 hours (depending on flights)\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Google Drive with data uploaded to `/MyDrive/CloudML/data/`\n",
    "- Each flight folder must contain: `.h5` (IRAI), `.hdf5` (CPL), `.hdf` (navigation)\n",
    "- ~2GB Drive space for models and results\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71d849b2"
   },
   "source": [
    "## Setup (Run Once Per Session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fa7b2e4b"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 1: Mount Google Drive\n",
    "# ============================================================================\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create project directories\n",
    "!mkdir -p /content/drive/MyDrive/CloudML/data\n",
    "!mkdir -p /content/drive/MyDrive/CloudML/models\n",
    "!mkdir -p /content/drive/MyDrive/CloudML/plots\n",
    "!mkdir -p /content/drive/MyDrive/CloudML/logs\n",
    "\n",
    "print(\"‚úì Google Drive mounted successfully\")\n",
    "print(\"‚úì Project directories created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ef3e03b9"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 2: Clone/Update Repository\n",
    "# ============================================================================\n",
    "%cd /content\n",
    "\n",
    "if not os.path.exists('/content/repo'):\n",
    "    print('Cloning repository...')\n",
    "    !git clone https://github.com/rylanmalarchick/cloudMLPublic.git repo\n",
    "else:\n",
    "    print('Repository exists. Pulling latest changes...')\n",
    "    %cd /content/repo\n",
    "    !git pull origin main\n",
    "\n",
    "%cd /content/repo\n",
    "print(\"‚úì Repository ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "74070521"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 3: Install Dependencies\n",
    "# ============================================================================\n",
    "print(\"Installing dependencies (this may take 5-10 minutes)...\\n\")\n",
    "\n",
    "# Install PyTorch with CUDA 12.1 support\n",
    "!pip install --quiet torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# Clean up potential conflicts\n",
    "!pip uninstall -y -q mamba-ssm causal-conv1d 2>/dev/null\n",
    "\n",
    "# Install core dependencies\n",
    "!pip install --quiet h5py==3.14.0 netCDF4==1.7.2 pyhdf==0.11.6 scikit-learn matplotlib plotly pyyaml\n",
    "\n",
    "# Install advanced components\n",
    "!pip install --quiet torch_geometric==2.5.3\n",
    "!pip install --quiet causal-conv1d==1.4.0\n",
    "!pip install --quiet mamba-ssm==2.2.2\n",
    "\n",
    "print(\"\\n‚úì All dependencies installed successfully\")\n",
    "print(\"\\nVerifying installation...\")\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d69509ec"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 4: Verify Data\n",
    "# ============================================================================\n",
    "import os\n",
    "\n",
    "data_dir = '/content/drive/MyDrive/CloudML/data/'\n",
    "\n",
    "# Expected flights\n",
    "flights = ['10Feb25', '30Oct24', '04Nov24', '23Oct24', '18Feb25', '12Feb25']\n",
    "\n",
    "print(\"Checking data availability...\\n\")\n",
    "missing_data = []\n",
    "\n",
    "for flight in flights:\n",
    "    flight_path = os.path.join(data_dir, flight)\n",
    "    if os.path.exists(flight_path):\n",
    "        files = os.listdir(flight_path)\n",
    "        has_h5 = any(f.endswith('.h5') for f in files)\n",
    "        has_hdf5 = any(f.endswith('.hdf5') for f in files)\n",
    "        has_hdf = any(f.endswith('.hdf') for f in files)\n",
    "        \n",
    "        if has_h5 and has_hdf5 and has_hdf:\n",
    "            print(f\"‚úì {flight}: All files present\")\n",
    "        else:\n",
    "            print(f\"‚ö† {flight}: Missing files (h5={has_h5}, hdf5={has_hdf5}, hdf={has_hdf})\")\n",
    "            missing_data.append(flight)\n",
    "    else:\n",
    "        print(f\"‚úó {flight}: Folder not found\")\n",
    "        missing_data.append(flight)\n",
    "\n",
    "if missing_data:\n",
    "    print(f\"\\n‚ö† WARNING: {len(missing_data)} flight(s) missing data\")\n",
    "    print(\"Training will proceed with available flights only.\")\n",
    "else:\n",
    "    print(\"\\n‚úì All data verified successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d3e30745"
   },
   "source": [
    "---\n",
    "## Baseline Training - Choose Your Configuration\n",
    "\n",
    "**üéØ NEW: TIER 1 Implementation Ready!**\n",
    "\n",
    "**TIER 1 improvements** (based on validated literature):\n",
    "- ‚úÖ Multi-scale temporal attention (captures cross-view relationships at different scales)\n",
    "- ‚úÖ Self-supervised pre-training (encoder learns features before supervised training)\n",
    "- ‚úÖ Increased temporal frames (7 instead of 5 for better spatial coverage)\n",
    "- ‚úÖ Expected improvement: +15-25% R¬≤ (from -0.09 to 0.15-0.25)\n",
    "\n",
    "See `TIER1_READY.md` for full details.\n",
    "\n",
    "---\n",
    "\n",
    "**Five options available:**\n",
    "\n",
    "### Option A-Tuned: Full Model with TUNED Hyperparameters (RECOMMENDED) ‚≠ê‚≠ê\n",
    "- **NEW**: Tuned based on first run analysis (negative R¬≤, erratic val loss)\n",
    "- Model: **64/128/256 channels** (full capacity)\n",
    "- **Improvements:** Lower LR (0.0005), reduced warmup (500), gentler overweighting (2.0x), tighter early stopping\n",
    "- GPU Memory: ~9-10GB\n",
    "- **Use this if:** First run gave poor results (R¬≤ < 0)\n",
    "\n",
    "### Option A: Full Model with Optimizations\n",
    "- Model: **64/128/256 channels** (full capacity)\n",
    "- Optimizations: Gradient checkpointing + torch.compile (default mode)\n",
    "- GPU Memory: ~9-10GB\n",
    "- Batch size: 20\n",
    "- Speed: **15-25% faster** than no compile\n",
    "- **Use this for:** First attempt, original hyperparameters\n",
    "\n",
    "### Option B: Full Model - Maximum Stability\n",
    "- Model: **64/128/256 channels** (full capacity)\n",
    "- Optimizations: Gradient checkpointing only (NO torch.compile)\n",
    "- GPU Memory: ~8-9GB\n",
    "- Batch size: 16\n",
    "- **Use this if:** Option A gives \"static input data pointer changed\" errors\n",
    "\n",
    "### Option C: Memory-Optimized Model (SAFE FALLBACK)\n",
    "- Model: 32/64/128 channels (50% smaller)\n",
    "- GPU Memory: ~7-8GB\n",
    "- Batch size: 16\n",
    "- **Use this if:** Options A or B give OOM errors\n",
    "\n",
    "**Recommended:** Start with **Option A-Tuned** (improved based on first run analysis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "31aa68e9"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# OPTION A-TUNED: TIER 1 TRAINING (RECOMMENDED) ‚≠ê‚≠ê\n",
    "# ============================================================================\n",
    "\n",
    "# STEP 1: Pull latest Tier 1 code\n",
    "print(\"Pulling latest Tier 1 code...\")\n",
    "%cd /content/repo\n",
    "!git pull origin main\n",
    "print(\"‚úì Code updated\\n\")\n",
    "\n",
    "# STEP 2: Verify Tier 1 modules exist\n",
    "import os\n",
    "if os.path.exists('/content/repo/src/pretraining.py'):\n",
    "    print(\"‚úì Tier 1 self-supervised pretraining module found\")\n",
    "else:\n",
    "    print(\"‚ö† WARNING: Tier 1 module not found - running baseline only\")\n",
    "\n",
    "# STEP 3: Start training\n",
    "import datetime\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "baseline_name = f\"tier1_tuned_{timestamp}\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TIER 1 TRAINING: TUNED HYPERPARAMETERS + LITERATURE IMPROVEMENTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Experiment ID: {baseline_name}\")\n",
    "print(f\"Config: colab_optimized_full_tuned.yaml\")\n",
    "print(f\"Model: 64/128/256 channels (FULL)\")\n",
    "print(f\"\\nTIER 1 FEATURES ENABLED:\")\n",
    "print(f\"  ‚úÖ Self-supervised pre-training (20 epochs reconstruction)\")\n",
    "print(f\"  ‚úÖ Multi-scale temporal attention (4 heads)\")\n",
    "print(f\"  ‚úÖ Increased temporal frames (7 frames)\")\n",
    "print(f\"  ‚úÖ Expected R¬≤ improvement: +15-25%\")\n",
    "print(f\"\\nTuned Parameters:\")\n",
    "print(f\"  - Learning Rate: 0.0005 (reduced from 0.001)\")\n",
    "print(f\"  - Warmup Steps: 500 (reduced from 2000)\")\n",
    "print(f\"  - Overweight Factor: 2.0 (reduced from 3.5)\")\n",
    "print(f\"  - Early Stopping Patience: 10 (reduced from 15)\")\n",
    "print(f\"Expected Runtime: 3-4 hours (includes Tier 1 pre-training)\")\n",
    "print(f\"Expected GPU Usage: ~11-13GB (batch_size=20, 7 frames)\")\n",
    "print(f\"Target: R¬≤ > 0.15, MAE < 0.30 km\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nTraining started... Monitor GPU with: !nvidia-smi\\n\")\n",
    "print(\"Watch for: 'TIER 1: SELF-SUPERVISED PRE-TRAINING ENABLED' banner\\n\")\n",
    "\n",
    "%cd /content/repo\n",
    "!python main.py \\\n",
    "    --config configs/colab_optimized_full_tuned.yaml \\\n",
    "    --save_name {baseline_name} \\\n",
    "    --epochs 50\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TIER 1 TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Model saved to: /content/drive/MyDrive/CloudML/models/trained/{baseline_name}.pth\")\n",
    "print(f\"Pre-trained encoder: /content/drive/MyDrive/CloudML/models/pretrained/\")\n",
    "print(f\"Results saved to: /content/drive/MyDrive/CloudML/plots/\")\n",
    "print(f\"Logs saved to: /content/drive/MyDrive/CloudML/logs/\")\n",
    "print(f\"\\nCompare with baseline to see Tier 1 improvements!\")\n",
    "print(\"\\nCheck TensorBoard: %load_ext tensorboard\")\n",
    "print(\"                   %tensorboard --logdir /content/drive/MyDrive/CloudML/logs/tensorboard/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "13c7837b"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# OPTION A: FULL MODEL + OPTIMIZATIONS (ORIGINAL)\n",
    "# ============================================================================\n",
    "import datetime\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "baseline_name = f\"baseline_full_{timestamp}\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TRAINING FULL MODEL WITH OPTIMIZATIONS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Experiment ID: {baseline_name}\")\n",
    "print(f\"Config: colab_optimized_full.yaml\")\n",
    "print(f\"Model: 64/128/256 channels (FULL)\")\n",
    "print(f\"Optimizations: Gradient Checkpointing + torch.compile('default' mode)\")\n",
    "print(f\"Expected Runtime: 2-2.5 hours (faster with compile)\")\n",
    "print(f\"Expected GPU Usage: ~9-10GB (batch_size=20)\")\n",
    "print(f\"CUDA Graph Fix: Using 'default' compile mode (compatible with checkpointing)\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nTraining started... Monitor GPU with: !nvidia-smi\\n\")\n",
    "\n",
    "!python main.py \\\n",
    "    --config configs/colab_optimized_full.yaml \\\n",
    "    --save_name {baseline_name} \\\n",
    "    --epochs 50\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FULL MODEL TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Model saved to: /content/drive/MyDrive/CloudML/models/trained/{baseline_name}.pth\")\n",
    "print(f\"Results saved to: /content/drive/MyDrive/CloudML/logs/\")\n",
    "print(f\"Logs saved to: /content/drive/MyDrive/CloudML/plots/\")\n",
    "print(\"\\nCheck TensorBoard: %load_ext tensorboard\")\n",
    "print(\"                   %tensorboard --logdir /content/drive/MyDrive/CloudML/logs/tensorboard/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f8924f37"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# OPTION B: FULL MODEL - MAXIMUM STABILITY (NEW - NO torch.compile)\n",
    "# ============================================================================\n",
    "import datetime\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "baseline_name = f\"baseline_full_stable_{timestamp}\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TRAINING FULL MODEL - STABLE MODE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Experiment ID: {baseline_name}\")\n",
    "print(f\"Config: colab_full_stable.yaml\")\n",
    "print(f\"Model: 64/128/256 channels (FULL)\")\n",
    "print(f\"Optimizations: Gradient Checkpointing only (NO torch.compile)\")\n",
    "print(f\"Expected Runtime: ~3 hours (no compile speedup)\")\n",
    "print(f\"Expected GPU Usage: ~8-9GB (batch_size=16)\")\n",
    "print(f\"Stability: MAXIMUM (no CUDA graph issues)\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nTraining started... Monitor GPU with: !nvidia-smi\\n\")\n",
    "\n",
    "!python main.py \\\n",
    "    --config configs/colab_full_stable.yaml \\\n",
    "    --save_name {baseline_name} \\\n",
    "    --epochs 50\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FULL MODEL STABLE TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Model saved to: /content/drive/MyDrive/CloudML/models/trained/{baseline_name}.pth\")\n",
    "print(f\"Results saved to: /content/drive/MyDrive/CloudML/plots/\")\n",
    "print(f\"Logs saved to: /content/drive/MyDrive/CloudML/logs/\")\n",
    "print(\"\\nCheck TensorBoard: %load_ext tensorboard\")\n",
    "print(\"                   %tensorboard --logdir /content/drive/MyDrive/CloudML/logs/tensorboard/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5aef5561"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# OPTION C: MEMORY-OPTIMIZED MODEL (FALLBACK IF OOM)\n",
    "# ============================================================================\n",
    "import datetime\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "baseline_name = f\"baseline_memopt_{timestamp}\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TRAINING MEMORY-OPTIMIZED MODEL\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Experiment ID: {baseline_name}\")\n",
    "print(f\"Config: colab_optimized.yaml\")\n",
    "print(f\"Model: 32/64/128 channels (memory-optimized)\")\n",
    "print(f\"Expected Runtime: 2.5-3 hours\")\n",
    "print(f\"Expected GPU Usage: ~7-8GB (batch_size=16)\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nTraining started... Monitor GPU with: !nvidia-smi\\n\")\n",
    "\n",
    "!python main.py \\\n",
    "    --config configs/colab_optimized.yaml \\\n",
    "    --save_name {baseline_name} \\\n",
    "    --epochs 50\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MEMORY-OPTIMIZED TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Model saved to: /content/drive/MyDrive/CloudML/models/trained/{baseline_name}.pth\")\n",
    "print(f\"Results saved to: /content/drive/MyDrive/CloudML/plots/\")\n",
    "print(f\"Logs saved to: /content/drive/MyDrive/CloudML/logs/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d605b804"
   },
   "source": [
    "---\n",
    "## Troubleshooting Training Issues\n",
    "\n",
    "### Common Error: \"RuntimeError: static input data pointer changed\"\n",
    "\n",
    "**What it means:** CUDA graph incompatibility between torch.compile and gradient checkpointing.\n",
    "\n",
    "**When it happens:** Usually during epoch 2+ after compilation completes.\n",
    "\n",
    "**Solution:** Use Option B (Full Model - Stable) which disables torch.compile, OR the configs have been updated:\n",
    "- `colab_optimized_full.yaml` now uses `torch_compile_mode: \"default\"` (fixed)\n",
    "- `colab_full_stable.yaml` disables torch.compile entirely (most stable)\n",
    "\n",
    "**Quick fix if error occurs:**\n",
    "```python\n",
    "# Stop current run and use stable config:\n",
    "!python main.py --config configs/colab_full_stable.yaml --save_name baseline_stable --epochs 50\n",
    "```\n",
    "\n",
    "### Config Selection Decision Tree:\n",
    "\n",
    "1. **Start here:** Option A (`colab_optimized_full.yaml`)\n",
    "   - ‚úÖ If it works: Fastest training, best results\n",
    "   - ‚ùå If \"static input data pointer changed\": Go to step 2\n",
    "   - ‚ùå If OOM error: Go to step 3\n",
    "\n",
    "2. **CUDA graph error:** Use Option B (`colab_full_stable.yaml`)\n",
    "   - ‚úÖ Same model capacity, no compile issues\n",
    "   - ‚ùå If OOM error: Go to step 3\n",
    "\n",
    "3. **Out of Memory:** Use Option C (`colab_optimized.yaml`)\n",
    "   - ‚úÖ Guaranteed to fit on T4\n",
    "   - ‚ö†Ô∏è Smaller model = may need more training\n",
    "\n",
    "### Monitoring Tips:\n",
    "\n",
    "```python\n",
    "# Check GPU memory usage:\n",
    "!nvidia-smi\n",
    "\n",
    "# Watch training progress:\n",
    "!tail -f /content/drive/MyDrive/CloudML/logs/training.log\n",
    "\n",
    "# Kill training if needed:\n",
    "# Runtime ‚Üí Interrupt execution (or press stop button)\n",
    "```\n",
    "\n",
    "**See full documentation:** `docs/CUDA_GRAPH_FIX.md`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ad8da2c"
   },
   "source": [
    "---\n",
    "## Ablation Studies (Systematic Component Evaluation)\n",
    "\n",
    "These experiments isolate the contribution of each component by removing or modifying one aspect at a time.\n",
    "\n",
    "### Ablation Suite:\n",
    "\n",
    "1. **Angles Mode - Zenith Only**: Tests if solar azimuth angle adds value\n",
    "2. **No Spatial Attention**: Evaluates spatial attention contribution\n",
    "3. **No Temporal Attention**: Evaluates temporal attention contribution\n",
    "4. **No Attention (Both)**: Tests full attention mechanism value\n",
    "5. **No Augmentation**: Measures data augmentation impact\n",
    "6. **Simple MAE Loss**: Compares Huber vs MAE loss\n",
    "7. **Fewer Temporal Frames**: Tests temporal context importance (3 vs 5 frames)\n",
    "8. **CNN Architecture**: Compares Transformer vs simple CNN baseline\n",
    "\n",
    "**Total Runtime**: ~6-8 hours for all 8 ablations\n",
    "\n",
    "**Run this to execute all ablations sequentially:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "22114ba5"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ABLATION STUDIES - SYSTEMATIC EVALUATION\n",
    "# ============================================================================\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Define ablation experiments\n",
    "ablations = [\n",
    "    {\n",
    "        'name': 'ablation_angles_sza_only',\n",
    "        'description': 'Use only solar zenith angle (no azimuth)',\n",
    "        'args': '--angles_mode sza_only',\n",
    "        'expected_impact': 'Slight performance drop if SAA provides useful info'\n",
    "    },\n",
    "    {\n",
    "        'name': 'ablation_no_spatial_attention',\n",
    "        'description': 'Disable spatial attention mechanism',\n",
    "        'args': '--no-use_spatial_attention',\n",
    "        'expected_impact': 'Moderate drop - spatial attention focuses on clouds'\n",
    "    },\n",
    "    {\n",
    "        'name': 'ablation_no_temporal_attention',\n",
    "        'description': 'Disable temporal attention mechanism',\n",
    "        'args': '--no-use_temporal_attention',\n",
    "        'expected_impact': 'Moderate drop - temporal attention weighs informative frames'\n",
    "    },\n",
    "    {\n",
    "        'name': 'ablation_no_attention',\n",
    "        'description': 'Disable both attention mechanisms',\n",
    "        'args': '--no-use_spatial_attention --no-use_temporal_attention',\n",
    "        'expected_impact': 'Significant drop - demonstrates attention value'\n",
    "    },\n",
    "    {\n",
    "        'name': 'ablation_no_augmentation',\n",
    "        'description': 'Disable data augmentation',\n",
    "        'args': '--no-augment',\n",
    "        'expected_impact': 'Small drop - augmentation helps generalization'\n",
    "    },\n",
    "    {\n",
    "        'name': 'ablation_mae_loss',\n",
    "        'description': 'Use simple MAE loss instead of Huber',\n",
    "        'args': '--loss_type mae',\n",
    "        'expected_impact': 'Slight drop - Huber is robust to outliers'\n",
    "    },\n",
    "    {\n",
    "        'name': 'ablation_fewer_temporal',\n",
    "        'description': 'Reduce temporal frames from 5 to 3',\n",
    "        'args': '--temporal_frames 3',\n",
    "        'expected_impact': 'Moderate drop - less temporal context'\n",
    "    },\n",
    "    {\n",
    "        'name': 'ablation_cnn_baseline',\n",
    "        'description': 'Simple CNN without transformer',\n",
    "        'args': '--architecture_name cnn --batch_size 24',  # CNN can handle larger batches\n",
    "        'expected_impact': 'Significant drop - demonstrates transformer superiority'\n",
    "    },\n",
    "]\n",
    "\n",
    "# Save ablation plan\n",
    "ablation_log_path = '/content/drive/MyDrive/CloudML/ablation_plan.json'\n",
    "with open(ablation_log_path, 'w') as f:\n",
    "    json.dump(ablations, f, indent=2)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SYSTEMATIC ABLATION STUDIES\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total experiments: {len(ablations)}\")\n",
    "print(f\"Estimated total time: {len(ablations) * 45} minutes (~{len(ablations) * 45 / 60:.1f} hours)\")\n",
    "print(f\"Results will be saved to: /content/drive/MyDrive/CloudML/\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# Execute each ablation\n",
    "results_summary = []\n",
    "\n",
    "for i, ablation in enumerate(ablations, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ABLATION {i}/{len(ablations)}: {ablation['name']}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Description: {ablation['description']}\")\n",
    "    print(f\"Expected: {ablation['expected_impact']}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    save_name = f\"{ablation['name']}_{timestamp}\"\n",
    "    \n",
    "    # Run experiment\n",
    "    !python main.py \\\n",
    "        --config configs/colab_optimized.yaml \\\n",
    "        --save_name {save_name} \\\n",
    "        --epochs 40 \\\n",
    "        --batch_size 16 \\\n",
    "        --temporal_frames 5 \\\n",
    "        {ablation['args']}\n",
    "    \n",
    "    print(f\"\\n‚úì Completed: {ablation['name']}\\n\")\n",
    "    \n",
    "    results_summary.append({\n",
    "        'ablation': ablation['name'],\n",
    "        'description': ablation['description'],\n",
    "        'save_name': save_name\n",
    "    })\n",
    "\n",
    "# Save summary\n",
    "summary_path = '/content/drive/MyDrive/CloudML/ablation_summary.json'\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ALL ABLATIONS COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Summary saved to: {summary_path}\")\n",
    "print(f\"\\nNext steps:\")\n",
    "print(\"1. Run the aggregation cell below to compile results\")\n",
    "print(\"2. Check plots in: /content/drive/MyDrive/CloudML/plots/\")\n",
    "print(\"3. Review metrics for paper Table 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "42b63db1"
   },
   "source": [
    "---\n",
    "## Optional: Leave-One-Out Cross-Validation\n",
    "\n",
    "For the most rigorous evaluation, run LOO CV where each flight is held out once.\n",
    "\n",
    "**Warning**: This takes 8-12 hours for 6 flights!\n",
    "\n",
    "Only run this if:\n",
    "- You have Colab Pro (longer sessions)\n",
    "- You need LOO results for the paper\n",
    "- You can monitor the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dcc93d59"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LEAVE-ONE-OUT CROSS-VALIDATION (OPTIONAL)\n",
    "# ============================================================================\n",
    "import datetime\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "loo_name = f\"loo_cv_{timestamp}\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"LEAVE-ONE-OUT CROSS-VALIDATION\")\n",
    "print(\"=\"*80)\n",
    "print(\"This will train 6 models (one per flight held out)\")\n",
    "print(\"Expected Runtime: 8-12 hours\")\n",
    "print(\"‚ö† WARNING: Long training session - ensure you have Colab Pro or can monitor\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# Confirm before running\n",
    "confirm = input(\"Type 'yes' to proceed with LOO CV: \")\n",
    "\n",
    "if confirm.lower() == 'yes':\n",
    "    !python main.py \\\n",
    "        --config configs/colab_optimized.yaml \\\n",
    "        --save_name {loo_name} \\\n",
    "        --epochs 40 \\\n",
    "        --batch_size 16 \\\n",
    "        --temporal_frames 5 \\\n",
    "        --loo \\\n",
    "        --loo_epochs 40\n",
    "    \n",
    "    print(\"\\n‚úì LOO Cross-Validation Complete!\")\n",
    "    print(f\"Results saved to: /content/drive/MyDrive/CloudML/\")\n",
    "else:\n",
    "    print(\"LOO CV skipped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2a207594"
   },
   "source": [
    "---\n",
    "## Results Aggregation & Analysis\n",
    "\n",
    "Compile all results into a summary table for your paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ee6f7ee"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# AGGREGATE RESULTS FOR PAPER\n",
    "# ============================================================================\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "print(\"Aggregating results...\\n\")\n",
    "\n",
    "# Find all CSV result files\n",
    "results_dir = '/content/drive/MyDrive/CloudML/logs/csv/'\n",
    "csv_files = glob.glob(os.path.join(results_dir, '*.csv'))\n",
    "\n",
    "if csv_files:\n",
    "    all_results = []\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        exp_name = os.path.basename(csv_file).replace('.csv', '')\n",
    "        df['experiment'] = exp_name\n",
    "        all_results.append(df)\n",
    "    \n",
    "    # Combine all results\n",
    "    combined = pd.concat(all_results, ignore_index=True)\n",
    "    \n",
    "    # Save combined results\n",
    "    output_path = '/content/drive/MyDrive/CloudML/all_results_combined.csv'\n",
    "    combined.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"‚úì Combined {len(csv_files)} result files\")\n",
    "    print(f\"‚úì Saved to: {output_path}\")\n",
    "    print(\"\\nSummary Statistics by Experiment:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Group by experiment and show key metrics\n",
    "    summary = combined.groupby('experiment').agg({\n",
    "        'mae': 'mean',\n",
    "        'rmse': 'mean',\n",
    "        'r2': 'mean'\n",
    "    }).round(4)\n",
    "    \n",
    "    print(summary)\n",
    "    print(\"\\n‚úì Use this table for your paper!\")\n",
    "    \n",
    "else:\n",
    "    print(\"No result files found. Make sure experiments have completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e1b04bda"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CREATE PAPER-READY COMPARISON TABLE\n",
    "# ============================================================================\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load ablation summary\n",
    "summary_path = '/content/drive/MyDrive/CloudML/ablation_summary.json'\n",
    "\n",
    "if os.path.exists(summary_path):\n",
    "    with open(summary_path, 'r') as f:\n",
    "        ablations = json.load(f)\n",
    "    \n",
    "    print(\"Paper-Ready Ablation Table\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"{'Experiment':<40} | {'Description':<50}\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    for abl in ablations:\n",
    "        print(f\"{abl['ablation']:<40} | {abl['description']:<50}\")\n",
    "    \n",
    "    print(\"=\"*100)\n",
    "    print(\"\\nTo get metrics for each experiment, check the CSV files in:\")\n",
    "    print(\"/content/drive/MyDrive/CloudML/logs/csv/\")\n",
    "    print(\"\\nOr run the aggregation script:\")\n",
    "    print(\"!python scripts/aggregate_results.py\")\n",
    "else:\n",
    "    print(\"Ablation summary not found. Run ablations first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "506632c3"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# OPTION D: Custom Configuration\n",
    "# ============================================================================\n",
    "# Edit config file manually and run with custom settings\n",
    "\n",
    "%cd /content/repo\n",
    "# !python main.py --config configs/YOUR_CUSTOM_CONFIG.yaml --save_name custom_run --epochs 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2efacef6"
   },
   "source": [
    "---\n",
    "## Monitor Training Progress\n",
    "\n",
    "### üéØ TIER 1 Training Stages\n",
    "\n",
    "If you ran **Option A-Tuned**, you'll see two training phases:\n",
    "\n",
    "**Phase 1: Self-Supervised Pre-training (20 epochs, ~30-45 min)**\n",
    "- Watch reconstruction loss decrease (target: < 0.01)\n",
    "- Encoder learns spatial features from images\n",
    "- Should converge in first 10-15 epochs\n",
    "\n",
    "**Phase 2: Supervised Training (50 epochs, ~2-3 hours)**\n",
    "- Normal supervised training with pre-trained encoder\n",
    "- Watch R¬≤ improve (target: > 0.15)\n",
    "- Should see faster initial convergence vs baseline\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "59e69f2d"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# GPU MONITORING (Run in parallel with training)\n",
    "# ============================================================================\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def monitor_gpu(duration=300, interval=5):\n",
    "    \"\"\"Monitor GPU usage for specified duration\"\"\"\n",
    "    for i in range(duration // interval):\n",
    "        clear_output(wait=True)\n",
    "        print(f\"GPU Monitoring (updating every {interval}s, {i*interval}/{duration}s elapsed)\\n\")\n",
    "        !nvidia-smi --query-gpu=timestamp,memory.used,memory.total,utilization.gpu,temperature.gpu --format=csv\n",
    "        time.sleep(interval)\n",
    "\n",
    "# Run for 5 minutes\n",
    "monitor_gpu(duration=300, interval=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ed39b348"
   },
   "source": [
    "---\n",
    "## TensorBoard - View Training Curves\n",
    "\n",
    "**Before running TensorBoard, verify that log files exist:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "93112d00"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VERIFY TENSORBOARD LOGS EXIST\n",
    "# ============================================================================\n",
    "import os\n",
    "\n",
    "tb_dir = \"/content/drive/MyDrive/CloudML/logs/tensorboard/\"\n",
    "\n",
    "if os.path.exists(tb_dir):\n",
    "    runs = [d for d in os.listdir(tb_dir) if os.path.isdir(os.path.join(tb_dir, d))]\n",
    "    if runs:\n",
    "        print(f\"‚úì Found {len(runs)} TensorBoard run(s):\")\n",
    "        for run in sorted(runs):\n",
    "            run_path = os.path.join(tb_dir, run)\n",
    "            files = os.listdir(run_path)\n",
    "            event_files = [f for f in files if 'events.out.tfevents' in f]\n",
    "            print(f\"  - {run}: {len(event_files)} event file(s)\")\n",
    "        print(f\"\\n‚úì Ready to launch TensorBoard!\")\n",
    "    else:\n",
    "        print(f\"‚úó TensorBoard directory exists but is empty: {tb_dir}\")\n",
    "        print(\"  Run a training session first.\")\n",
    "else:\n",
    "    print(f\"‚úó TensorBoard directory not found: {tb_dir}\")\n",
    "    print(\"  Make sure:\")\n",
    "    print(\"  1. You've run a training session\")\n",
    "    print(\"  2. Files are saving to Google Drive (check config paths)\")\n",
    "    print(\"  3. Google Drive is mounted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5cc861ce"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TENSORBOARD (View training curves)\n",
    "# ============================================================================\n",
    "# Run this cell to launch TensorBoard in the notebook\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /content/drive/MyDrive/CloudML/logs/tensorboard/\n",
    "\n",
    "# If you see \"No dashboards are active\", it means:\n",
    "# 1. No training runs have been completed yet, OR\n",
    "# 2. Files are not saving to Drive (check verification cell above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "461a0e0e"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LIST ALL TRAINED MODELS\n",
    "# ============================================================================\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "models_dir = '/content/drive/MyDrive/CloudML/models/trained/'\n",
    "\n",
    "if os.path.exists(models_dir):\n",
    "    models = sorted(os.listdir(models_dir))\n",
    "    \n",
    "    print(f\"\\nTrained Models ({len(models)} total)\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"{'Model Name':<60} {'Size (MB)':<15} {'Modified':<20}\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    for model in models:\n",
    "        path = os.path.join(models_dir, model)\n",
    "        size_mb = os.path.getsize(path) / (1024 * 1024)\n",
    "        mtime = datetime.fromtimestamp(os.path.getmtime(path))\n",
    "        print(f\"{model:<60} {size_mb:>10.1f} MB   {mtime.strftime('%Y-%m-%d %H:%M')}\")\n",
    "    \n",
    "    print(\"=\"*100)\n",
    "else:\n",
    "    print(\"Models directory not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d6b90607"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DOWNLOAD RESULTS (Optional - already in Drive)\n",
    "# ============================================================================\n",
    "from google.colab import files\n",
    "\n",
    "# Zip and download results\n",
    "print(\"Zipping results for download...\")\n",
    "\n",
    "!cd /content/drive/MyDrive/CloudML && \\\n",
    "    zip -r results_export.zip plots/ logs/csv/ models/trained/ *.json *.csv 2>/dev/null\n",
    "\n",
    "print(\"\\n‚úì Results zipped\")\n",
    "print(\"Downloading... (this may take a few minutes)\")\n",
    "\n",
    "files.download('/content/drive/MyDrive/CloudML/results_export.zip')\n",
    "\n",
    "print(\"\\n‚úì Download complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1d964cd3"
   },
   "source": [
    "---\n",
    "## Quick Reference\n",
    "\n",
    "### File Locations\n",
    "- **Models**: `/content/drive/MyDrive/CloudML/models/trained/`\n",
    "- **Plots**: `/content/drive/MyDrive/CloudML/plots/`\n",
    "- **Logs**: `/content/drive/MyDrive/CloudML/logs/`\n",
    "- **Metrics (CSV)**: `/content/drive/MyDrive/CloudML/logs/csv/`\n",
    "- **TensorBoard**: `/content/drive/MyDrive/CloudML/logs/tensorboard/`\n",
    "\n",
    "### Key Metrics for Paper\n",
    "- **MAE** (Mean Absolute Error): Primary metric in km\n",
    "- **RMSE** (Root Mean Squared Error): Penalizes large errors\n",
    "- **R¬≤** (Coefficient of Determination): Model fit quality\n",
    "- **MAPE** (Mean Absolute Percentage Error): Relative error\n",
    "\n",
    "### Recommended Paper Structure\n",
    "1. **Baseline Results**: Report MAE, RMSE, R¬≤ from baseline training\n",
    "2. **Ablation Table**: Show Œî metrics for each ablation vs baseline\n",
    "3. **LOO Results**: If available, show per-flight generalization\n",
    "4. **Qualitative**: Include attention maps, error distributions\n",
    "\n",
    "### Troubleshooting\n",
    "- **OOM Error**: Reduce `batch_size` to 24 or 16\n",
    "- **Slow Training**: Check `!nvidia-smi` - GPU should be >80% utilized\n",
    "- **Session Timeout**: Enable Colab Pro or run shorter experiments\n",
    "- **Missing Data**: Check `/content/drive/MyDrive/CloudML/data/` structure\n",
    "\n",
    "### Support\n",
    "- **Documentation**: See `README.md`, `COLAB_SETUP.md`, `GPU_OPTIMIZATION.md`\n",
    "- **Issues**: https://github.com/rylanmalarchick/cloudMLPublic/issues\n",
    "- **Email**: rylan1012@gmail.com"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}