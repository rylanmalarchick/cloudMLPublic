{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NASA Cloud-ML Training on Google Colab\n",
    "\n",
    "**Paper-Quality Baseline & Ablation Studies**\n",
    "\n",
    "This notebook is optimized for producing publication-ready results on Google Colab's T4 GPU (15GB VRAM).\n",
    "\n",
    "## What This Notebook Does\n",
    "\n",
    "1. **Strong Baseline Training** - Train a high-quality model with optimal hyperparameters\n",
    "2. **Comprehensive Ablation Studies** - Systematic evaluation of each component\n",
    "3. **GPU-Optimized** - Maximizes T4 GPU utilization (~10-12GB usage vs default 3.7GB)\n",
    "4. **Reproducible Results** - Fixed seeds, detailed logging, automatic checkpointing\n",
    "\n",
    "## Training Pipeline\n",
    "\n",
    "- **Pre-training Phase**: Model learns on primary flight (30Oct24) - establishes feature representations\n",
    "- **Final Training Phase**: Fine-tunes on all flights with overweighting to retain learned features\n",
    "- **Validation**: Held-out flight (12Feb25) for unbiased evaluation\n",
    "- **LOO Cross-Validation** (Optional): Each flight held out once for robust generalization assessment\n",
    "\n",
    "## Expected Runtime\n",
    "\n",
    "- **Baseline Training**: ~2-3 hours (50 epochs with early stopping)\n",
    "- **Full Ablation Suite**: ~6-8 hours (8 experiments × ~45 min each)\n",
    "- **With LOO CV**: ~8-12 hours (depending on flights)\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Google Drive with data uploaded to `/MyDrive/CloudML/data/`\n",
    "- Each flight folder must contain: `.h5` (IRAI), `.hdf5` (CPL), `.hdf` (navigation)\n",
    "- ~2GB Drive space for models and results\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup (Run Once Per Session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 1: Mount Google Drive\n",
    "# ============================================================================\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create project directories\n",
    "!mkdir -p /content/drive/MyDrive/CloudML/data\n",
    "!mkdir -p /content/drive/MyDrive/CloudML/models\n",
    "!mkdir -p /content/drive/MyDrive/CloudML/plots\n",
    "!mkdir -p /content/drive/MyDrive/CloudML/logs\n",
    "\n",
    "print(\"✓ Google Drive mounted successfully\")\n",
    "print(\"✓ Project directories created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 2: Clone/Update Repository\n",
    "# ============================================================================\n",
    "%cd /content\n",
    "\n",
    "if not os.path.exists('/content/repo'):\n",
    "    print('Cloning repository...')\n",
    "    !git clone https://github.com/rylanmalarchick/cloudMLPublic.git repo\n",
    "else:\n",
    "    print('Repository exists. Pulling latest changes...')\n",
    "    %cd /content/repo\n",
    "    !git pull origin main\n",
    "\n",
    "%cd /content/repo\n",
    "print(\"✓ Repository ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 3: Install Dependencies\n",
    "# ============================================================================\n",
    "print(\"Installing dependencies (this may take 5-10 minutes)...\\n\")\n",
    "\n",
    "# Install PyTorch with CUDA 12.1 support\n",
    "!pip install --quiet torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# Clean up potential conflicts\n",
    "!pip uninstall -y -q mamba-ssm causal-conv1d 2>/dev/null\n",
    "\n",
    "# Install core dependencies\n",
    "!pip install --quiet h5py==3.14.0 netCDF4==1.7.2 pyhdf==0.11.6 scikit-learn matplotlib plotly pyyaml\n",
    "\n",
    "# Install advanced components\n",
    "!pip install --quiet torch_geometric==2.5.3\n",
    "!pip install --quiet causal-conv1d==1.4.0\n",
    "!pip install --quiet mamba-ssm==2.2.2\n",
    "\n",
    "print(\"\\n✓ All dependencies installed successfully\")\n",
    "print(\"\\nVerifying installation...\")\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 4: Verify Data\n",
    "# ============================================================================\n",
    "import os\n",
    "\n",
    "data_dir = '/content/drive/MyDrive/CloudML/data/'\n",
    "\n",
    "# Expected flights\n",
    "flights = ['10Feb25', '30Oct24', '04Nov24', '23Oct24', '18Feb25', '12Feb25']\n",
    "\n",
    "print(\"Checking data availability...\\n\")\n",
    "missing_data = []\n",
    "\n",
    "for flight in flights:\n",
    "    flight_path = os.path.join(data_dir, flight)\n",
    "    if os.path.exists(flight_path):\n",
    "        files = os.listdir(flight_path)\n",
    "        has_h5 = any(f.endswith('.h5') for f in files)\n",
    "        has_hdf5 = any(f.endswith('.hdf5') for f in files)\n",
    "        has_hdf = any(f.endswith('.hdf') for f in files)\n",
    "        \n",
    "        if has_h5 and has_hdf5 and has_hdf:\n",
    "            print(f\"✓ {flight}: All files present\")\n",
    "        else:\n",
    "            print(f\"⚠ {flight}: Missing files (h5={has_h5}, hdf5={has_hdf5}, hdf={has_hdf})\")\n",
    "            missing_data.append(flight)\n",
    "    else:\n",
    "        print(f\"✗ {flight}: Folder not found\")\n",
    "        missing_data.append(flight)\n",
    "\n",
    "if missing_data:\n",
    "    print(f\"\\n⚠ WARNING: {len(missing_data)} flight(s) missing data\")\n",
    "    print(\"Training will proceed with available flights only.\")\n",
    "else:\n",
    "    print(\"\\n✓ All data verified successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Baseline Training (High-Quality Model)\n",
    "\n",
    "This section trains the **strongest possible baseline** for your paper.\n",
    "\n",
    "### Configuration:\n",
    "- **Batch Size**: 32 (optimized for T4 GPU)\n",
    "- **Temporal Frames**: 5 (strong temporal context)\n",
    "- **Epochs**: 50 (with early stopping)\n",
    "- **Architecture**: Transformer with spatial & temporal attention\n",
    "- **Data Augmentation**: Enabled\n",
    "- **All Features**: Both SZA and SAA angles, full preprocessing pipeline\n",
    "\n",
    "### Expected Performance:\n",
    "- GPU Utilization: ~10-12GB (75-80%)\n",
    "- Training Time: ~2-3 hours\n",
    "- Results: State-of-the-art CBH prediction\n",
    "\n",
    "**Run this cell and come back in 2-3 hours!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# BASELINE TRAINING - THE STRONGEST MODEL\n",
    "# ============================================================================\n",
    "import datetime\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "baseline_name = f\"baseline_paper_{timestamp}\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TRAINING PAPER BASELINE MODEL\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Experiment ID: {baseline_name}\")\n",
    "print(f\"Config: colab_optimized.yaml\")\n",
    "print(f\"Expected Runtime: 2-3 hours\")\n",
    "print(f\"Expected GPU Usage: ~10-11GB (batch_size=24)\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nTraining started... Monitor GPU with: !nvidia-smi\\n\")\n",
    "\n",
    "!python main.py \\\n",
    "    --config configs/colab_optimized.yaml \\\n",
    "    --save_name {baseline_name} \\\n",
    "    --epochs 50 \\\n",
    "    --batch_size 24 \\\n",
    "    --temporal_frames 5 \\\n",
    "    --learning_rate 0.001 \\\n",
    "    --weight_decay 0.04 \\\n",
    "    --early_stopping_patience 15 \\\n",
    "    --use_spatial_attention \\\n",
    "    --use_temporal_attention \\\n",
    "    --augment \\\n",
    "    --angles_mode both \\\n",
    "    --loss_type huber \\\n",
    "    --architecture_name transformer\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BASELINE TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Model saved to: /content/drive/MyDrive/CloudML/models/trained/{baseline_name}.pth\")\n",
    "print(f\"Results saved to: /content/drive/MyDrive/CloudML/plots/\")\n",
    "print(f\"Logs saved to: /content/drive/MyDrive/CloudML/logs/\")\n",
    "print(\"\\nCheck TensorBoard: %load_ext tensorboard\")\n",
    "print(\"                   %tensorboard --logdir /content/drive/MyDrive/CloudML/logs/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Ablation Studies (Systematic Component Evaluation)\n",
    "\n",
    "These experiments isolate the contribution of each component by removing or modifying one aspect at a time.\n",
    "\n",
    "### Ablation Suite:\n",
    "\n",
    "1. **Angles Mode - Zenith Only**: Tests if solar azimuth angle adds value\n",
    "2. **No Spatial Attention**: Evaluates spatial attention contribution\n",
    "3. **No Temporal Attention**: Evaluates temporal attention contribution\n",
    "4. **No Attention (Both)**: Tests full attention mechanism value\n",
    "5. **No Augmentation**: Measures data augmentation impact\n",
    "6. **Simple MAE Loss**: Compares Huber vs MAE loss\n",
    "7. **Fewer Temporal Frames**: Tests temporal context importance (3 vs 5 frames)\n",
    "8. **CNN Architecture**: Compares Transformer vs simple CNN baseline\n",
    "\n",
    "**Total Runtime**: ~6-8 hours for all 8 ablations\n",
    "\n",
    "**Run this to execute all ablations sequentially:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ABLATION STUDIES - SYSTEMATIC EVALUATION\n",
    "# ============================================================================\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Define ablation experiments\n",
    "ablations = [\n",
    "    {\n",
    "        'name': 'ablation_angles_sza_only',\n",
    "        'description': 'Use only solar zenith angle (no azimuth)',\n",
    "        'args': '--angles_mode sza_only',\n",
    "        'expected_impact': 'Slight performance drop if SAA provides useful info'\n",
    "    },\n",
    "    {\n",
    "        'name': 'ablation_no_spatial_attention',\n",
    "        'description': 'Disable spatial attention mechanism',\n",
    "        'args': '--no-use_spatial_attention',\n",
    "        'expected_impact': 'Moderate drop - spatial attention focuses on clouds'\n",
    "    },\n",
    "    {\n",
    "        'name': 'ablation_no_temporal_attention',\n",
    "        'description': 'Disable temporal attention mechanism',\n",
    "        'args': '--no-use_temporal_attention',\n",
    "        'expected_impact': 'Moderate drop - temporal attention weighs informative frames'\n",
    "    },\n",
    "    {\n",
    "        'name': 'ablation_no_attention',\n",
    "        'description': 'Disable both attention mechanisms',\n",
    "        'args': '--no-use_spatial_attention --no-use_temporal_attention',\n",
    "        'expected_impact': 'Significant drop - demonstrates attention value'\n",
    "    },\n",
    "    {\n",
    "        'name': 'ablation_no_augmentation',\n",
    "        'description': 'Disable data augmentation',\n",
    "        'args': '--no-augment',\n",
    "        'expected_impact': 'Small drop - augmentation helps generalization'\n",
    "    },\n",
    "    {\n",
    "        'name': 'ablation_mae_loss',\n",
    "        'description': 'Use simple MAE loss instead of Huber',\n",
    "        'args': '--loss_type mae',\n",
    "        'expected_impact': 'Slight drop - Huber is robust to outliers'\n",
    "    },\n",
    "    {\n",
    "        'name': 'ablation_fewer_temporal',\n",
    "        'description': 'Reduce temporal frames from 5 to 3',\n",
    "        'args': '--temporal_frames 3',\n",
    "        'expected_impact': 'Moderate drop - less temporal context'\n",
    "    },\n",
    "    {\n",
    "        'name': 'ablation_cnn_baseline',\n",
    "        'description': 'Simple CNN without transformer',\n",
    "        'args': '--architecture_name cnn --batch_size 48',  # CNN can handle larger batches\n",
    "        'expected_impact': 'Significant drop - demonstrates transformer superiority'\n",
    "    },\n",
    "]\n",
    "\n",
    "# Save ablation plan\n",
    "ablation_log_path = '/content/drive/MyDrive/CloudML/ablation_plan.json'\n",
    "with open(ablation_log_path, 'w') as f:\n",
    "    json.dump(ablations, f, indent=2)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SYSTEMATIC ABLATION STUDIES\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total experiments: {len(ablations)}\")\n",
    "print(f\"Estimated total time: {len(ablations) * 45} minutes (~{len(ablations) * 45 / 60:.1f} hours)\")\n",
    "print(f\"Results will be saved to: /content/drive/MyDrive/CloudML/\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# Execute each ablation\n",
    "results_summary = []\n",
    "\n",
    "for i, ablation in enumerate(ablations, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ABLATION {i}/{len(ablations)}: {ablation['name']}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Description: {ablation['description']}\")\n",
    "    print(f\"Expected: {ablation['expected_impact']}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    save_name = f\"{ablation['name']}_{timestamp}\"\n",
    "    \n",
    "    # Run experiment\n",
    "    !python main.py \\\n",
    "        --config configs/colab_optimized.yaml \\\n",
    "        --save_name {save_name} \\\n",
    "        --epochs 40 \\\n",
    "        --batch_size 24 \\\n",
    "        --temporal_frames 5 \\\n",
    "        {ablation['args']}\n",
    "    \n",
    "    print(f\"\\n✓ Completed: {ablation['name']}\\n\")\n",
    "    \n",
    "    results_summary.append({\n",
    "        'ablation': ablation['name'],\n",
    "        'description': ablation['description'],\n",
    "        'save_name': save_name\n",
    "    })\n",
    "\n",
    "# Save summary\n",
    "summary_path = '/content/drive/MyDrive/CloudML/ablation_summary.json'\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ALL ABLATIONS COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Summary saved to: {summary_path}\")\n",
    "print(f\"\\nNext steps:\")\n",
    "print(\"1. Run the aggregation cell below to compile results\")\n",
    "print(\"2. Check plots in: /content/drive/MyDrive/CloudML/plots/\")\n",
    "print(\"3. Review metrics for paper Table 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Optional: Leave-One-Out Cross-Validation\n",
    "\n",
    "For the most rigorous evaluation, run LOO CV where each flight is held out once.\n",
    "\n",
    "**Warning**: This takes 8-12 hours for 6 flights!\n",
    "\n",
    "Only run this if:\n",
    "- You have Colab Pro (longer sessions)\n",
    "- You need LOO results for the paper\n",
    "- You can monitor the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LEAVE-ONE-OUT CROSS-VALIDATION (OPTIONAL)\n",
    "# ============================================================================\n",
    "import datetime\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "loo_name = f\"loo_cv_{timestamp}\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"LEAVE-ONE-OUT CROSS-VALIDATION\")\n",
    "print(\"=\"*80)\n",
    "print(\"This will train 6 models (one per flight held out)\")\n",
    "print(\"Expected Runtime: 8-12 hours\")\n",
    "print(\"⚠ WARNING: Long training session - ensure you have Colab Pro or can monitor\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# Confirm before running\n",
    "confirm = input(\"Type 'yes' to proceed with LOO CV: \")\n",
    "\n",
    "if confirm.lower() == 'yes':\n",
    "    !python main.py \\\n",
    "        --config configs/colab_optimized.yaml \\\n",
    "        --save_name {loo_name} \\\n",
    "        --epochs 40 \\\n",
    "        --batch_size 24 \\\n",
    "        --temporal_frames 5 \\\n",
    "        --loo \\\n",
    "        --loo_epochs 40\n",
    "    \n",
    "    print(\"\\n✓ LOO Cross-Validation Complete!\")\n",
    "    print(f\"Results saved to: /content/drive/MyDrive/CloudML/\")\n",
    "else:\n",
    "    print(\"LOO CV skipped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Results Aggregation & Analysis\n",
    "\n",
    "Compile all results into a summary table for your paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# AGGREGATE RESULTS FOR PAPER\n",
    "# ============================================================================\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "print(\"Aggregating results...\\n\")\n",
    "\n",
    "# Find all CSV result files\n",
    "results_dir = '/content/drive/MyDrive/CloudML/logs/csv/'\n",
    "csv_files = glob.glob(os.path.join(results_dir, '*.csv'))\n",
    "\n",
    "if csv_files:\n",
    "    all_results = []\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        exp_name = os.path.basename(csv_file).replace('.csv', '')\n",
    "        df['experiment'] = exp_name\n",
    "        all_results.append(df)\n",
    "    \n",
    "    # Combine all results\n",
    "    combined = pd.concat(all_results, ignore_index=True)\n",
    "    \n",
    "    # Save combined results\n",
    "    output_path = '/content/drive/MyDrive/CloudML/all_results_combined.csv'\n",
    "    combined.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"✓ Combined {len(csv_files)} result files\")\n",
    "    print(f\"✓ Saved to: {output_path}\")\n",
    "    print(\"\\nSummary Statistics by Experiment:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Group by experiment and show key metrics\n",
    "    summary = combined.groupby('experiment').agg({\n",
    "        'mae': 'mean',\n",
    "        'rmse': 'mean',\n",
    "        'r2': 'mean'\n",
    "    }).round(4)\n",
    "    \n",
    "    print(summary)\n",
    "    print(\"\\n✓ Use this table for your paper!\")\n",
    "    \n",
    "else:\n",
    "    print(\"No result files found. Make sure experiments have completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CREATE PAPER-READY COMPARISON TABLE\n",
    "# ============================================================================\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load ablation summary\n",
    "summary_path = '/content/drive/MyDrive/CloudML/ablation_summary.json'\n",
    "\n",
    "if os.path.exists(summary_path):\n",
    "    with open(summary_path, 'r') as f:\n",
    "        ablations = json.load(f)\n",
    "    \n",
    "    print(\"Paper-Ready Ablation Table\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"{'Experiment':<40} | {'Description':<50}\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    for abl in ablations:\n",
    "        print(f\"{abl['ablation']:<40} | {abl['description']:<50}\")\n",
    "    \n",
    "    print(\"=\"*100)\n",
    "    print(\"\\nTo get metrics for each experiment, check the CSV files in:\")\n",
    "    print(\"/content/drive/MyDrive/CloudML/logs/csv/\")\n",
    "    print(\"\\nOr run the aggregation script:\")\n",
    "    print(\"!python scripts/aggregate_results.py\")\n",
    "else:\n",
    "    print(\"Ablation summary not found. Run ablations first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Utilities & Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# GPU MONITORING (Run in parallel with training)\n",
    "# ============================================================================\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def monitor_gpu(duration=300, interval=5):\n",
    "    \"\"\"Monitor GPU usage for specified duration\"\"\"\n",
    "    for i in range(duration // interval):\n",
    "        clear_output(wait=True)\n",
    "        print(f\"GPU Monitoring (updating every {interval}s, {i*interval}/{duration}s elapsed)\\n\")\n",
    "        !nvidia-smi --query-gpu=timestamp,memory.used,memory.total,utilization.gpu,temperature.gpu --format=csv\n",
    "        time.sleep(interval)\n",
    "\n",
    "# Run for 5 minutes\n",
    "monitor_gpu(duration=300, interval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TENSORBOARD (View training curves)\n",
    "# ============================================================================\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /content/drive/MyDrive/CloudML/logs/tensorboard/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LIST ALL TRAINED MODELS\n",
    "# ============================================================================\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "models_dir = '/content/drive/MyDrive/CloudML/models/trained/'\n",
    "\n",
    "if os.path.exists(models_dir):\n",
    "    models = sorted(os.listdir(models_dir))\n",
    "    \n",
    "    print(f\"\\nTrained Models ({len(models)} total)\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"{'Model Name':<60} {'Size (MB)':<15} {'Modified':<20}\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    for model in models:\n",
    "        path = os.path.join(models_dir, model)\n",
    "        size_mb = os.path.getsize(path) / (1024 * 1024)\n",
    "        mtime = datetime.fromtimestamp(os.path.getmtime(path))\n",
    "        print(f\"{model:<60} {size_mb:>10.1f} MB   {mtime.strftime('%Y-%m-%d %H:%M')}\")\n",
    "    \n",
    "    print(\"=\"*100)\n",
    "else:\n",
    "    print(\"Models directory not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DOWNLOAD RESULTS (Optional - already in Drive)\n",
    "# ============================================================================\n",
    "from google.colab import files\n",
    "\n",
    "# Zip and download results\n",
    "print(\"Zipping results for download...\")\n",
    "\n",
    "!cd /content/drive/MyDrive/CloudML && \\\n",
    "    zip -r results_export.zip plots/ logs/csv/ models/trained/ *.json *.csv 2>/dev/null\n",
    "\n",
    "print(\"\\n✓ Results zipped\")\n",
    "print(\"Downloading... (this may take a few minutes)\")\n",
    "\n",
    "files.download('/content/drive/MyDrive/CloudML/results_export.zip')\n",
    "\n",
    "print(\"\\n✓ Download complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Quick Reference\n",
    "\n",
    "### File Locations\n",
    "- **Models**: `/content/drive/MyDrive/CloudML/models/trained/`\n",
    "- **Plots**: `/content/drive/MyDrive/CloudML/plots/`\n",
    "- **Logs**: `/content/drive/MyDrive/CloudML/logs/`\n",
    "- **Metrics (CSV)**: `/content/drive/MyDrive/CloudML/logs/csv/`\n",
    "- **TensorBoard**: `/content/drive/MyDrive/CloudML/logs/tensorboard/`\n",
    "\n",
    "### Key Metrics for Paper\n",
    "- **MAE** (Mean Absolute Error): Primary metric in km\n",
    "- **RMSE** (Root Mean Squared Error): Penalizes large errors\n",
    "- **R²** (Coefficient of Determination): Model fit quality\n",
    "- **MAPE** (Mean Absolute Percentage Error): Relative error\n",
    "\n",
    "### Recommended Paper Structure\n",
    "1. **Baseline Results**: Report MAE, RMSE, R² from baseline training\n",
    "2. **Ablation Table**: Show Δ metrics for each ablation vs baseline\n",
    "3. **LOO Results**: If available, show per-flight generalization\n",
    "4. **Qualitative**: Include attention maps, error distributions\n",
    "\n",
    "### Troubleshooting\n",
    "- **OOM Error**: Reduce `batch_size` to 24 or 16\n",
    "- **Slow Training**: Check `!nvidia-smi` - GPU should be >80% utilized\n",
    "- **Session Timeout**: Enable Colab Pro or run shorter experiments\n",
    "- **Missing Data**: Check `/content/drive/MyDrive/CloudML/data/` structure\n",
    "\n",
    "### Support\n",
    "- **Documentation**: See `README.md`, `COLAB_SETUP.md`, `GPU_OPTIMIZATION.md`\n",
    "- **Issues**: https://github.com/rylanmalarchick/cloudMLPublic/issues\n",
    "- **Email**: rylan1012@gmail.com"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
