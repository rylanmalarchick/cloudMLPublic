\documentclass[11pt,letterpaper]{article}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{enumitem}
\usepackage{caption}
\usepackage{multirow}
\usepackage[numbers,sort&compress]{natbib}

% Colors
\definecolor{darkblue}{RGB}{0,51,102}
\definecolor{darkred}{RGB}{153,0,0}
\definecolor{darkgreen}{RGB}{0,102,51}

\hypersetup{
    colorlinks=true,
    linkcolor=darkblue,
    citecolor=darkblue,
    urlcolor=darkblue
}

\title{\textbf{Atmospheric Features Outperform Images for Cloud Base Height Retrieval: A Systematic Comparison Using NASA Airborne Observations}}

\author{
    Rylan Malarchick \\
    Embry-Riddle Aeronautical University \\
    Daytona Beach, FL 32114 \\
    \texttt{malarchr@my.erau.edu}
}

\date{\today}

\begin{document}

\maketitle

\begin{abstract}
	We systematically compare atmospheric feature-based and image-based machine learning for cloud base height (CBH) retrieval using 1,426 NASA ER-2 airborne observations from five research flights spanning two field campaigns. Using rigorous per-flight cross-validation that accounts for temporal autocorrelation, gradient boosting with 10 ERA5-derived features achieves R$^2$ = 0.744 (MAE = 117.4 m) within flights where models are trained and tested on independent temporal segments. Feature importance analysis identifies surface temperature (t2m) as the dominant predictor (72\% importance), consistent with lifting condensation level thermodynamics. We introduce 28 physics-based engineered features including virtual temperature and stability-moisture interactions, which become top predictors in the enhanced model. However, leave-one-flight-out cross-validation reveals catastrophic domain shift: mean R$^2$ = -15.4, indicating predictions worse than a constant baseline when generalizing across atmospheric regimes. To address this critical limitation, we evaluate five domain adaptation methods. Few-shot learning emerges as the most practical solution: with just 50 labeled samples from a target flight, R$^2$ recovers to 0.57--0.85 depending on target regime similarity. Uncertainty quantification via split conformal prediction achieves only 27\% coverage (target: 90\%) due to exchangeability violations from temporal autocorrelation, but per-flight calibration recovers 86\% coverage with 277 m prediction intervals. Our results demonstrate that atmospheric features substantially outperform image-based approaches, within-flight deployment is production-ready (0.28 ms inference, 1.3 MB model, CPU-only), but cross-regime generalization requires explicit domain adaptation. We provide an honest assessment of when this approach succeeds (within-regime, with calibration data) and when it fails (cross-regime without adaptation), establishing realistic expectations for operational deployment. We release CloudMLPublic, a fully reproducible framework with validated data pipelines and comprehensive uncertainty quantification.
\end{abstract}

\section{Introduction}

\subsection{Motivation}

Cloud base height (CBH)---the altitude of the lowest cloud layer bottom---is a fundamental atmospheric parameter with applications spanning climate science, aviation operations, and numerical weather prediction \cite{Stephens2012, Martucci2010}. Accurate CBH measurements are essential for understanding cloud radiative forcing \cite{Ramanathan1989}, validating climate models \cite{Boucher2013}, and ensuring safe aircraft operations in instrument meteorological conditions \cite{WMO2018}. Traditional CBH measurements rely on ground-based ceilometers \cite{Martucci2010} or active lidar systems \cite{McGill2002}, which provide high accuracy but limited spatial coverage. Satellite-based retrievals offer global coverage but face challenges in vertical resolution and cloud overlap \cite{Mace2007}.

High-altitude airborne platforms, such as NASA's ER-2 aircraft, present a unique opportunity for CBH observation through combined passive imagery and active lidar measurements \cite{McGill2002}. The ER-2 Cloud Physics Lidar (CPL) provides accurate reference CBH retrievals while flying above cloud layers, enabling supervised learning approaches. However, lidar systems are expensive, power-intensive, and provide limited horizontal coverage compared to passive cameras. This motivates the question: \textit{Can machine learning models trained on readily available atmospheric reanalysis data and passive imagery achieve comparable accuracy to active sensing for CBH retrieval?}

\subsection{The Feature Representation Question}

A central challenge in atmospheric machine learning is selecting appropriate input features. Two paradigms have emerged:

\begin{enumerate}[leftmargin=*]
	\item \textbf{Physics-informed features:} Using atmospheric state variables (temperature, humidity, pressure profiles) from numerical weather prediction models or reanalysis products like ERA5 \cite{Hersbach2020}. This approach leverages domain knowledge of cloud formation physics but requires accurate atmospheric state estimation.

	\item \textbf{End-to-end visual learning:} Applying convolutional neural networks (CNNs) or vision transformers (ViTs) directly to satellite or airborne imagery \cite{Matsuoka2018, Zantedeschi2019}. This approach captures spatial patterns and cloud morphology not explicitly represented in atmospheric features but requires substantial labeled training data.
\end{enumerate}

While deep learning has achieved remarkable success in computer vision benchmarks with millions of training examples \cite{Krizhevsky2012, Dosovitskiy2020}, atmospheric science applications operate at different scales. Our dataset comprises 1,426 labeled samples from 3 NASA ER-2 research flights with sufficient data. This raises a critical research question: \textit{Do atmospheric reanalysis features or learned image representations provide superior predictive performance for cloud base height retrieval?}

\subsection{Research Questions and Contributions}

This work addresses four key research questions:

\begin{enumerate}[leftmargin=*]
	\item \textbf{Feature representation:} How do atmospheric reanalysis features compare to learned image representations for CBH prediction under rigorous validation that accounts for temporal autocorrelation?

	\item \textbf{Domain generalization:} How severe is domain shift across flight campaigns, and what domain adaptation methods can recover performance?

	\item \textbf{Uncertainty quantification:} Can we provide calibrated prediction intervals despite temporal autocorrelation and domain shift?

	\item \textbf{Feature engineering:} Can physics-based derived features improve predictions beyond raw ERA5 variables?
\end{enumerate}

Our key contributions are:

\begin{itemize}[leftmargin=*]
	\item \textbf{Rigorous validation methodology:} We demonstrate that pooled K-fold cross-validation inflates R$^2$ from 0.744 to 0.924 due to temporal autocorrelation (lag-1 $\rho$ = 0.94). We advocate for per-flight validation as the honest metric and document performance across four validation strategies.

	\item \textbf{Quantified domain shift:} Leave-one-flight-out validation reveals catastrophic generalization failure (R$^2$ = -15.4), representing the most severe domain shift reported in atmospheric ML literature. We characterize shift sources via Maximum Mean Discrepancy (MMD) analysis.

	\item \textbf{Domain adaptation solutions:} We evaluate five adaptation methods (few-shot learning, instance weighting, TrAdaBoost, MMD alignment). Few-shot learning with 50 samples recovers R$^2$ = 0.57--0.85, providing a practical deployment protocol.

	\item \textbf{Physics-based feature engineering:} We derive 28 thermodynamic and interaction features from 10 base ERA5 variables. Virtual temperature and stability-moisture interaction emerge as top predictors.

	\item \textbf{Honest uncertainty quantification:} We show conformal prediction fails (27\% coverage vs 90\% target) due to exchangeability violations, but per-flight calibration achieves 86\% coverage---establishing realistic expectations for operational deployment.

	\item \textbf{Open-source framework:} Release of CloudMLPublic with validated data pipelines, corrected ERA5 integration, and comprehensive documentation of what went wrong in initial development to help others avoid similar pitfalls.
\end{itemize}

\subsection{Paper Organization}

The remainder of this paper is structured as follows: Section~\ref{sec:related} reviews related work in cloud remote sensing, atmospheric machine learning, and ensemble methods. Section~\ref{sec:methods} describes our dataset, feature engineering, model architectures, and experimental methodology. Section~\ref{sec:results} presents validation results, ensemble analysis, and domain adaptation experiments. Section~\ref{sec:discussion} interprets our findings in the context of atmospheric physics and machine learning theory. Section~\ref{sec:limitations} discusses limitations and future research directions, and Section~\ref{sec:conclusion} concludes.

\section{Related Work}
\label{sec:related}

\subsection{Cloud Base Height Retrieval}

Traditional CBH measurement techniques include ground-based ceilometers using laser backscatter \cite{Martucci2010}, radiosondes with temperature and humidity sensors \cite{Hahn1995}, and surface observer reports \cite{WMO2018}. These provide high accuracy but limited spatial coverage. Satellite-based approaches have employed passive infrared \cite{Minnis2008}, microwave \cite{Alishouse1990}, and active lidar/radar measurements \cite{Mace2007}. The CloudSat and CALIPSO missions demonstrated spaceborne active sensing capabilities \cite{Stephens2002, Winker2010}, but orbital geometry limits temporal resolution.

Machine learning approaches to cloud property retrieval have gained traction in recent years. \citet{Yuan2020} applied random forests to MODIS imagery for cloud detection. \citet{Matsuoka2018} used CNNs for cloud type classification from ground-based all-sky cameras. \citet{Zantedeschi2019} demonstrated deep learning for precipitation nowcasting from satellite imagery. However, these studies primarily focus on classification tasks or 2D cloud properties rather than vertical structure estimation.

Atmospheric reanalysis products like ERA5 \cite{Hersbach2020} provide global gridded estimates of atmospheric state variables through data assimilation of observations into numerical weather prediction models. ERA5 has been validated for cloud property retrievals \cite{Benas2020} and widely adopted for climate research. Our work leverages ERA5's vertical atmospheric profiles as input features for CBH prediction.

\subsection{Gradient Boosting for Atmospheric Science}

Gradient boosting decision trees (GBDT) have emerged as a powerful method for tabular data across diverse domains \cite{Chen2016, Ke2017}. In atmospheric science, GBDT has been successfully applied to precipitation forecasting \cite{Rasp2020}, air quality prediction \cite{Chen2019}, and satellite retrieval algorithm development \cite{Stubenrauch2021}. \citet{Rasp2020} demonstrated that GBDT models trained on reanalysis data can match or exceed the accuracy of physics-based parameterizations for convective precipitation, motivating our investigation of GBDT for CBH retrieval.

The interpretability of GBDT through feature importance analysis \cite{Lundberg2020} provides additional advantages for scientific applications, enabling validation of learned patterns against domain knowledge. This contrasts with deep neural networks, where interpretability remains challenging despite advances in attention mechanisms \cite{Vaswani2017} and saliency methods \cite{Simonyan2014}.

\subsection{Computer Vision for Remote Sensing}

Convolutional neural networks have revolutionized computer vision \cite{Krizhevsky2012, He2016}, with architectures like ResNet \cite{He2016} and EfficientNet \cite{Tan2019} achieving human-level performance on image classification benchmarks. Vision transformers (ViTs) \cite{Dosovitskiy2020} have recently shown competitive performance by applying self-attention mechanisms to image patches.

Remote sensing applications face unique challenges compared to natural image datasets: limited labeled data, domain shift between sensors, and the need for physical interpretability \cite{Zhu2017}. Transfer learning from ImageNet pre-training has shown mixed results, with \citet{Neumann2019} finding limited benefit for satellite imagery due to domain mismatch. \citet{Jean2019} demonstrated successful poverty prediction from satellite imagery using CNNs, but with far more training data than available for CBH retrieval.

Our work differs from prior remote sensing applications by directly comparing learned image features against domain-specific engineered features in a controlled experimental setting with identical training data.

\subsection{Ensemble Methods and Multi-Modal Learning}

Ensemble methods combine predictions from multiple models to improve generalization \cite{Dietterich2000}. Common approaches include bagging \cite{Breiman1996}, boosting \cite{Freund1997}, and stacking \cite{Wolpert1992}. In atmospheric science, ensemble numerical weather prediction has become standard practice \cite{Hamill2006}, but ensemble machine learning for retrieval algorithms remains less explored.

Multi-modal learning seeks to leverage complementary information from different input modalities \cite{Baltrušaitis2019}. \citet{Ngiam2011} showed that multi-modal deep networks can learn shared representations from audio and video. For remote sensing, \citet{Hong2021} combined optical and radar satellite imagery using late fusion. Our ensemble analysis investigates whether atmospheric state variables and visual cloud imagery provide complementary signals for CBH retrieval.

\subsection{Domain Adaptation and Few-Shot Learning}

Domain adaptation addresses distribution shift between training and deployment data \cite{Pan2010}. Atmospheric observations exhibit strong domain shift across geographic regions, seasons, and sensor configurations. \citet{Tuia2016} surveyed domain adaptation for remote sensing, highlighting the need for transfer learning methods.

Few-shot learning aims to learn from limited labeled examples \cite{Wang2020}. Meta-learning approaches like MAML \cite{Finn2017} and prototypical networks \cite{Snell2017} have shown promise, but applications to atmospheric science remain rare. Our few-shot experiments quantify the sample efficiency of domain adaptation for cross-flight generalization.

\section{Dataset and Methods}
\label{sec:methods}

\subsection{Data Sources}

\subsubsection{NASA ER-2 Platform}

The NASA ER-2 is a high-altitude research aircraft operating at altitudes up to 21 km, providing a unique vantage point for atmospheric observations \cite{McGill2002}. We utilize data from multiple flight campaigns with the following instruments:

\begin{itemize}[leftmargin=*]
	\item \textbf{Cloud Physics Lidar (CPL):} Active 532 nm lidar providing vertical profiles of cloud and aerosol backscatter with 30 m vertical resolution \cite{McGill2002}. CPL retrievals serve as ground truth CBH labels.

	\item \textbf{Downward-looking camera:} Passive RGB imagery at 1024$\times$1024 pixels capturing cloud morphology beneath the aircraft.

	\item \textbf{Flight metadata:} GPS position, altitude, heading, and time stamps with 1 Hz sampling.
\end{itemize}

\subsubsection{ERA5 Reanalysis}

We extract atmospheric state variables from ERA5 \cite{Hersbach2020}, the fifth-generation ECMWF reanalysis providing hourly global coverage at 0.25° spatial resolution and 37 pressure levels. For each flight observation, we query ERA5 at the aircraft location and time, retrieving vertical profiles of:

\begin{itemize}[leftmargin=*]
	\item Temperature (K) at 37 pressure levels
	\item Specific humidity (kg/kg) at 37 pressure levels
	\item Geopotential height (m) at 37 pressure levels
	\item Surface pressure (Pa)
	\item 2-meter temperature and dewpoint (K)
	\item Total column water vapor (kg/m$^2$)
\end{itemize}

ERA5 data are spatially interpolated to aircraft coordinates using bilinear interpolation and temporally matched to within ±30 minutes of observation time.

\subsubsection{Dataset Statistics}

Our final dataset comprises 1,426 labeled samples from 3 NASA ER-2 research flights with sufficient samples for reliable analysis, spanning two field campaigns:

\begin{center}
	\begin{tabular}{lcccc}
		\toprule
		\textbf{Flight ID} & \textbf{Campaign}    & \textbf{Samples} & \textbf{CBH (km)}        & \textbf{Date}               \\
		\midrule
		Flight 1           & GLOVE 2025           & 1,021            & 1.34 $\pm$ 0.22          & 2025-02-10                  \\
		Flight 2           & GLOVE 2025           & 129              & 0.85 $\pm$ 0.16          & 2025-02-12                  \\
		Flight 3           & WHYMSIE 2024         & 276              & 0.88 $\pm$ 0.23          & 2024-10-23                  \\
		\midrule
		\textbf{Total}     & \textbf{2 campaigns} & \textbf{1,426}   & \textbf{1.20 $\pm$ 0.31} & \textbf{Oct 2024--Feb 2025} \\
		\bottomrule
	\end{tabular}
\end{center}

Two additional flights (Flights 0 and 4) were excluded due to insufficient sample sizes (n=2 and n=8 respectively), which preclude reliable cross-validation statistics. Cloud base heights in the retained dataset range from 210 m to 1,950 m, with mean 1,197 m. The distribution is dominated by Flight 1 (72\% of samples), which exhibits higher CBH values (marine stratocumulus regime) compared to Flights 2 and 3 (mixed boundary layer regimes). This class imbalance contributes to domain shift challenges in cross-flight validation.

\textbf{Data quality controls:} All samples passed ERA5 integration verification (non-zero feature variance), temporal matching constraints ($\pm$30 minutes of reanalysis), and physical plausibility checks (CBH within 0--10 km). The original data pipeline contained a critical bug where ERA5 features were placeholder zeros; this was corrected during restudy, resulting in the validated dataset used throughout this work.

\subsection{Feature Engineering}

\subsubsection{Base Atmospheric Features}

From ERA5 reanalysis data and solar geometry, we extract 10 base features capturing atmospheric state and cloud formation physics:

\begin{enumerate}[leftmargin=*]
	\item \textbf{ERA5 atmospheric features (8):}
	      \begin{itemize}
		      \item 2-meter temperature (t2m, K)
		      \item 2-meter dewpoint (d2m, K)
		      \item Surface pressure (sp, Pa)
		      \item Boundary layer height (blh, m)
		      \item Total column water vapor (tcwv, kg/m$^2$)
		      \item Lifting condensation level (lcl, m) -- computed from t2m, d2m
		      \item Stability index (derived from BLH and temperature gradient)
		      \item Moisture gradient (vertical moisture structure indicator)
	      \end{itemize}

	\item \textbf{Geometric features (2):}
	      \begin{itemize}
		      \item Solar zenith angle (sza\_deg, degrees)
		      \item Solar azimuth angle (saa\_deg, degrees)
	      \end{itemize}
\end{enumerate}

\textbf{Note on data integrity:} Initial development used a data pipeline that produced all-zero ERA5 features due to a missing integration step. This was detected during restudy via variance checks and corrected. All results reported here use the validated dataset with proper ERA5 values.

\subsubsection{Physics-Based Derived Features}

To potentially improve prediction, we engineer 28 additional features grounded in cloud formation physics:

\begin{itemize}[leftmargin=*]
	\item \textbf{LCL-based (2):} lcl\_deficit (CBH - LCL), lcl\_ratio (CBH/LCL) -- capturing deviation from simple thermodynamic cloud base
	\item \textbf{Thermodynamic (8):} dew\_point\_depression, relative\_humidity\_2m, mixing\_ratio, potential\_temperature, virtual\_temperature, saturation\_vapor\_pressure, vapor\_pressure -- fundamental moisture and temperature variables
	\item \textbf{Stability (4):} stability\_dpd\_product, stability\_anomaly, stability\_moisture\_ratio -- interactions between stability and moisture
	\item \textbf{Solar/Temporal (6):} sza\_cos, sza\_sin, saa\_cos, saa\_sin, solar\_heating\_proxy, hour\_sin, hour\_cos -- diurnal heating effects
	\item \textbf{Interaction (8):} t2m\_x\_tcwv, blh\_x\_lcl, stability\_x\_tcwv, t2m\_x\_sza\_cos, blh\_x\_stability, t2m\_squared, blh\_squared, lcl\_squared, dpd\_squared -- polynomial and cross-term interactions
\end{itemize}

Feature importance analysis on the enhanced 38-feature set reveals that \textbf{virtual\_temperature} (33\% importance) and \textbf{stability\_x\_tcwv} (22\%) become top predictors, suggesting thermodynamic moisture-stability interactions are key drivers beyond the original feature set. The original t2m remains important (17\%), consistent with LCL physics.

\subsubsection{Image Preprocessing}

Airborne camera images undergo the following preprocessing pipeline:

\begin{enumerate}[leftmargin=*]
	\item Center crop to 896$\times$896 pixels to remove lens distortion artifacts
	\item Resize to 224$\times$224 pixels using bilinear interpolation
	\item Normalize RGB channels to zero mean and unit variance using ImageNet statistics
	\item Data augmentation (training only): Random horizontal/vertical flips, random brightness/contrast adjustment (±20\%)
\end{enumerate}

No domain-specific augmentations (e.g., cloud-aware transformations) are applied to maintain comparability with standard computer vision practices.

\subsection{Model Architectures}

\subsubsection{Gradient Boosting Decision Trees (GBDT)}

Our primary tabular model uses scikit-learn's GradientBoostingRegressor, a gradient boosting implementation. Hyperparameters are selected via nested cross-validation:

\begin{itemize}[leftmargin=*]
	\item Number of trees: 200
	\item Learning rate: 0.05
	\item Max depth: 8
	\item Minimum samples per leaf: 4
	\item Minimum samples per split: 10
	\item Subsample fraction: 0.8
	\item Random state: 42
	\item Objective: L2 regression (mean squared error)
\end{itemize}

For uncertainty quantification, we additionally train quantile regression models \cite{Koenker1978} targeting the 5th and 95th percentiles to construct 90\% prediction intervals.

\subsubsection{Convolutional Neural Network}

Our image baseline uses a simple CNN architecture designed to avoid overfitting:

\begin{itemize}[leftmargin=*]
	\item 4 convolutional blocks: [Conv(3→32) → ReLU → BatchNorm → MaxPool] × 4
	\item Kernel size: 3$\times$3, stride: 1, padding: 1
	\item Global average pooling
	\item Fully connected layers: 512 → 256 → 1
	\item Dropout: 0.3 after each FC layer
	\item Total parameters: 1.2M
\end{itemize}

We train for 100 epochs with early stopping (patience=15 epochs) using Adam optimizer (lr=0.001, weight decay=1e-4) and ReduceLROnPlateau scheduler (factor=0.5, patience=5). Training uses batch size 32. This architecture is intentionally simple to avoid overfitting with 1,426 samples.

\subsubsection{Ensemble Methods}

We evaluate three ensemble strategies:

\begin{enumerate}[leftmargin=*]
	\item \textbf{Simple averaging:} $\hat{y} = \frac{1}{2}(\hat{y}_{\text{GBDT}} + \hat{y}_{\text{CNN}})$

	\item \textbf{Weighted averaging:} $\hat{y} = w_1 \hat{y}_{\text{GBDT}} + w_2 \hat{y}_{\text{CNN}}$ where $w_1 + w_2 = 1$ and weights are optimized on validation set using scipy.optimize

	\item \textbf{Stacking:} Train a Ridge regression meta-model on base model predictions:
	      \begin{equation}
		      \hat{y} = \beta_0 + \beta_1 \hat{y}_{\text{GBDT}} + \beta_2 \hat{y}_{\text{CNN}}
	      \end{equation}
\end{enumerate}

Ensemble weights and meta-models are trained using stratified cross-validation to prevent overfitting.

\subsection{Experimental Protocol}

\subsubsection{Validation Strategy}

We employ four validation strategies to provide a comprehensive assessment of model performance under different assumptions:

\begin{enumerate}[leftmargin=*]
	\item \textbf{Pooled K-fold (inflated):} Standard 5-fold CV across all samples. This produces R$^2$ = 0.924 but is \textit{artificially inflated} by temporal autocorrelation (lag-1 $\rho$ = 0.94). Adjacent samples in time are highly correlated; when they appear in different folds, information leaks from train to test. \textbf{We report this metric only to document the inflation problem.}

	\item \textbf{Per-flight shuffled K-fold (moderate):} 5-fold CV performed independently within each flight, then averaged. This partially controls autocorrelation but still allows some temporal leakage within flights. Achieves R$^2$ = 0.744, MAE = 117.4 m. \textbf{This is our primary within-flight metric.}

	\item \textbf{Per-flight time-ordered K-fold (strict):} Train on first 80\% of each flight, test on last 20\%. This is an honest temporal holdout with no autocorrelation leakage. Achieves R$^2$ = -0.055, indicating the model struggles to extrapolate forward in time even within the same flight.

	\item \textbf{Leave-one-flight-out (LOFO-CV):} Train on all flights except one, test on the held-out flight. This tests cross-regime generalization. Achieves mean R$^2$ = -15.4 to -18.7, indicating catastrophic failure when generalizing across atmospheric regimes.
\end{enumerate}

The dramatic difference between pooled CV (R$^2$ = 0.924) and LOFO-CV (R$^2$ = -15.4) underscores the importance of appropriate validation methodology for atmospheric time-series data. \textbf{We advocate for per-flight validation as the honest metric for within-regime deployment and LOFO-CV as the realistic metric for cross-regime generalization.}

\subsubsection{Evaluation Metrics}

We assess model performance using:

\begin{itemize}[leftmargin=*]
	\item \textbf{R$^2$ score:} Coefficient of determination, $R^2 = 1 - \frac{\sum (y_i - \hat{y}_i)^2}{\sum (y_i - \bar{y})^2}$
	\item \textbf{Mean Absolute Error (MAE):} $\text{MAE} = \frac{1}{n}\sum |y_i - \hat{y}_i|$
	\item \textbf{Root Mean Squared Error (RMSE):} $\text{RMSE} = \sqrt{\frac{1}{n}\sum (y_i - \hat{y}_i)^2}$
\end{itemize}

For uncertainty quantification, we evaluate:
\begin{itemize}[leftmargin=*]
	\item \textbf{Coverage:} Fraction of true values within 90\% prediction intervals
	\item \textbf{Mean interval width:} Average size of prediction intervals
	\item \textbf{Uncertainty-error correlation:} Spearman correlation between interval width and absolute error
\end{itemize}

\subsubsection{Domain Adaptation Protocol}

To assess generalization across atmospheric regimes, we perform leave-one-flight-out (LOFO) validation: train on 5 flights, test on the 6th flight. This simulates deployment to new geographic regions or meteorological conditions.

For few-shot learning experiments, we:
\begin{enumerate}[leftmargin=*]
	\item Select target flight (18Feb25, highest domain shift due to small sample size and distinct meteorology)
	\item Train baseline model on remaining 5 flights
	\item Sample $k \in \{5, 10, 20\}$ examples from 18Feb25
	\item Fine-tune baseline model on $k$ samples
	\item Evaluate on held-out 18Feb25 test set
	\item Repeat 10 times with different random samples
\end{enumerate}

\subsubsection{Conformal Prediction for Uncertainty Quantification}

To provide distribution-free prediction intervals with guaranteed coverage, we employ split conformal prediction \cite{Lei2018}. Unlike quantile regression (which requires correct model specification), conformal prediction provides valid coverage under minimal assumptions.

The protocol is:
\begin{enumerate}[leftmargin=*]
	\item Split data into training (50\%), calibration (25\%), and test (25\%) sets
	\item Train base model (GBDT) on training set
	\item Compute absolute residuals on calibration set: $R_i = |y_i - \hat{y}_i|$
	\item For target coverage $1-\alpha$ (e.g., 90\%), calculate calibration quantile:
	      $$q = \text{Quantile}(R_1, \ldots, R_n; 1-\alpha)$$
	\item Construct prediction intervals on test set: $[\hat{y}_i - q, \hat{y}_i + q]$
\end{enumerate}

This procedure guarantees that $P(y \in [\hat{y} - q, \hat{y} + q]) \geq 1 - \alpha$ for exchangeable data \cite{Shafer2008}. We stratify calibration assessment by CBH regime (low <500m, mid 500-1500m, high >1500m) to evaluate conditional coverage.

\subsection{Implementation Details}

All experiments use Python 3.10 with PyTorch 2.0 and scikit-learn 1.3. Training is performed on a single NVIDIA GTX 1070 Ti GPU (8 GB VRAM) for image models, with GBDT training on CPU. Total compute time for all experiments is approximately 18 hours. Code and configuration files are available at \url{https://github.com/rylanmalarchick/CloudMLPublic} under MIT license. Random seed is fixed to 42 for reproducibility.

\section{Results}
\label{sec:results}

\subsection{Validation Strategy Comparison}

Table~\ref{tab:validation_comparison} presents the critical finding that validation methodology dramatically affects reported performance. Temporal autocorrelation inflates pooled K-fold R$^2$ by 0.18 (from 0.744 to 0.924), while cross-regime generalization shows catastrophic failure.

\begin{table}[h]
	\centering
	\caption{Performance across validation strategies. Pooled K-fold is inflated by temporal autocorrelation ($\rho$ = 0.94). LOFO-CV reveals severe domain shift.}
	\label{tab:validation_comparison}
	\begin{tabular}{lccc}
		\toprule
		\textbf{Validation Strategy} & \textbf{R$^2$} & \textbf{MAE (m)} & \textbf{Assessment}        \\
		\midrule
		Pooled K-fold                & 0.924          & 49.7             & Inflated (autocorrelation) \\
		Per-flight shuffled          & \textbf{0.744} & \textbf{117.4}   & Primary metric             \\
		Per-flight time-ordered      & -0.055         & 129.8            & Strict temporal holdout    \\
		Leave-one-flight-out         & -15.4 to -18.7 & 345--515         & Cross-regime failure       \\
		\bottomrule
	\end{tabular}
\end{table}

\textbf{Key insight:} The 0.21 R$^2$ inflation from pooled to per-flight CV is consistent with the lag-1 autocorrelation of 0.94. When consecutive samples (which share nearly identical CBH values) are split across train and test folds, the model effectively ``sees'' the test data during training.

\subsection{Feature Importance Analysis}

GBDT feature importance on the 10-feature base model identifies \textbf{t2m} (surface temperature) as overwhelmingly dominant (72\% importance), followed by d2m (6.5\%), tcwv (4.3\%), and blh (4.1\%). This dominance of temperature is consistent with lifting condensation level physics, where cloud base is primarily determined by the temperature-dewpoint spread.

When expanded to the 38-feature enhanced model:
\begin{itemize}[leftmargin=*]
	\item \textbf{virtual\_temperature} becomes dominant (33\% importance)
	\item \textbf{stability\_x\_tcwv} is second (22\%) -- capturing stability-moisture interaction
	\item \textbf{t2m} drops to third (17\%) as derived features capture its signal
	\item \textbf{saturation\_vapor\_pressure} (4.4\%) and \textbf{tcwv} (2.7\%) round out top-5
\end{itemize}

The shift from raw t2m to virtual\_temperature (which incorporates moisture effects on air density) suggests the model learns more sophisticated thermodynamic relationships when given appropriate derived features.

\subsubsection{Deep Learning Vision Baselines}

To ensure fair comparison beyond the simple CNN baseline, we trained state-of-the-art vision models with ImageNet pre-training: ResNet-18 \cite{He2016} and EfficientNet-B0 \cite{Tan2019}. Figure~\ref{fig:vision_baseline_comparison} shows comprehensive results across 6 model variants with 5-fold cross-validation.

ResNet-18 trained from scratch achieved R$^2$=0.617±0.064 (MAE=150.9±10.0 m), substantially better than the simple CNN (R$^2$=0.320) but still worse than GBDT on MAE. Surprisingly, ImageNet pre-training degraded performance (R$^2$=0.581±0.110), likely due to domain mismatch between natural images and overhead cloud imagery combined with our limited dataset size. Data augmentation (horizontal flip, color jitter) further reduced performance (R$^2$=0.370±0.034), suggesting overfitting to augmented patterns.

EfficientNet-B0 with pre-training achieved moderate performance (R$^2$=0.469±0.052, MAE=179.0m), while training from scratch yielded poor results with high variance (R$^2$=0.229±0.395). The best vision model (ResNet-18 scratch) still underperforms GBDT (R$^2$=0.744, per-flight shuffled) by 17\% on R$^2$ and 22.2\% on MAE, confirming that atmospheric features outperform learned image representations even with state-of-the-art deep learning architectures and proper training techniques.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.95\textwidth]{../outputs/figures/vision_baseline_comparison.png}
	\caption{Vision baseline performance comparison across 6 model variants. ResNet-18 from scratch (R$^2$=0.617) is the best vision model but still underperforms GBDT (R$^2$=0.744, red dashed line) by 17\% on R$^2$ and 22.2\% on MAE. Pre-training and augmentation unexpectedly degrade performance, likely due to domain mismatch and small dataset size (n=1,426).}
	\label{fig:vision_baseline_comparison}
\end{figure}

\textbf{Computational cost:} ResNet-18 models require 43.1 MB storage and 5.8 ms inference time (GPU), while GBDT uses only 1.3 MB and 0.28 ms (CPU). The 21$\times$ speedup and 33$\times$ smaller model size enable real-time deployment on resource-constrained platforms.

\subsection{Ensemble Analysis}

Figure~\ref{fig:ensemble_performance} shows the performance-complexity tradeoff for ensemble methods. The weighted ensemble achieves R$^2$ = 0.739, only 0.005 lower than the GBDT alone, while requiring 2× the inference time. Optimal ensemble weights are $w_{\text{GBDT}} = 0.888$, $w_{\text{CNN}} = 0.112$, indicating the atmospheric model dominates predictions.

Stacking with Ridge regression performs similarly (R$^2$ = 0.724), with learned coefficients $\beta_{\text{GBDT}} = 0.91$, $\beta_{\text{CNN}} = 0.08$. The low weight assigned to CNN predictions across ensemble methods indicates limited complementarity between modalities.

Analyzing per-sample ensemble improvement, we find that the ensemble outperforms GBDT alone on only 38\% of test samples (541/1426), with mean improvement of 8.2 m MAE where it helps. The CNN provides useful signal for a minority of cases with distinctive visual cloud patterns not captured by atmospheric features.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\textwidth]{paperfigures/figure2_ensemble_performance.png}
	\caption{Ensemble performance comparison showing minimal improvement over GBDT baseline. Optimal weights heavily favor the atmospheric model (88.8\% GBDT, 11.2\% CNN).}
	\label{fig:ensemble_performance}
\end{figure}

\subsection{Feature Importance and Ablation Analysis}

SHAP analysis \cite{Lundberg2020} identifies the most influential features for CBH prediction. Table~\ref{tab:featureablation} shows comprehensive ablation results.

\input{../outputs/feature_ablation/reports/feature_ablation_table.tex}

\textbf{Baseline performance} (all 10 base + 28 derived features = 38 features): R$^2$ = 0.744 $\pm$ 0.04 (per-flight shuffled), MAE = 117.4 m.

\textbf{Top-5 SHAP features by importance:}
\begin{enumerate}[leftmargin=*]
	\item \textbf{d2m} (dewpoint temperature 2m): mean\_abs\_shap = 87.73
	\item \textbf{t2m} (temperature 2m): mean\_abs\_shap = 78.60
	\item \textbf{stability\_index}: mean\_abs\_shap = 38.32
	\item \textbf{moisture\_gradient}: mean\_abs\_shap = 31.87
	\item \textbf{sp} (surface pressure): mean\_abs\_shap = 27.67
\end{enumerate}

\textbf{Feature group ablation} reveals:
\begin{itemize}[leftmargin=*]
	\item Atmospheric features only (9 features): R$^2$ = 0.704, $\Delta$ R$^2$ = -0.009
	\item Shadow/geometric features only (6 features): R$^2$ = 0.728, $\Delta$ R$^2$ = +0.015
\end{itemize}

\textbf{Individual feature removal} shows no single feature is critical:
\begin{itemize}[leftmargin=*]
	\item Removing d2m (most important): R$^2$ drop = 0.006 (0.9\%)
	\item Removing t2m: R$^2$ drop = -0.001 (-0.1\%)
	\item Maximum R$^2$ degradation across all features: <1\%
\end{itemize}

Figure~\ref{fig:ablation_summary} visualizes ablation results. The dominance of near-surface thermodynamic features (d2m, t2m) aligns with cloud formation physics: cloud base occurs where rising air parcels reach saturation. However, the model exhibits graceful degradation when features are removed, indicating robust distributed representation rather than critical dependence on individual predictors.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\textwidth]{../outputs/feature_ablation/figures/ablation_summary.png}
	\caption{Feature ablation study summary showing SHAP importance rankings and performance impact when removing top features. No single feature removal causes >1\% R$^2$ degradation.}
	\label{fig:ablation_summary}
\end{figure}

\textbf{Feature correlations} (Figure~\ref{fig:feature_correlation}): Four highly correlated pairs detected (|r| > 0.8), including perfect correlation between saa\_deg and shadow\_angle\_deg (r=1.0), suggesting potential for dimensionality reduction. Hierarchical clustering (Figure~\ref{fig:feature_clustering}) groups features into atmospheric thermodynamic, stability, and geometric clusters.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.75\textwidth]{../outputs/feature_ablation/figures/feature_correlation_heatmap.png}
	\caption{Feature correlation matrix showing 4 highly correlated pairs (|r|>0.8). Perfect correlation between saa\_deg and shadow\_angle\_deg indicates redundancy.}
	\label{fig:feature_correlation}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.75\textwidth]{../outputs/feature_ablation/figures/feature_clustering_dendrogram.png}
	\caption{Hierarchical clustering dendrogram based on absolute feature correlations, revealing natural groupings of atmospheric, stability, and geometric features.}
	\label{fig:feature_clustering}
\end{figure}

\subsection{Stratified Error Analysis}
\label{sec:error_stratification}

Table~\ref{tab:stratifiederroranalysis} presents comprehensive error stratification results.

\input{../outputs/stratified_error_analysis/reports/stratified_error_table.tex}

\textbf{Overall error distribution:}
\begin{itemize}[leftmargin=*]
	\item Mean error: -2.8 m (near-zero bias)
	\item Standard deviation: 199.0 m
	\item \textbf{Shapiro-Wilk test:} p = 6.28×10$^{-29}$ (reject normality)
\end{itemize}

The heavy-tailed error distribution (Figure~\ref{fig:error_distribution}) indicates systematic failures in certain atmospheric conditions rather than Gaussian measurement noise.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\textwidth]{../outputs/stratified_error_analysis/figures/error_distribution.png}
	\caption{Error distribution histogram showing heavy tails and departure from normality (Shapiro-Wilk p=6.28×10$^{-29}$), indicating systematic prediction failures in specific atmospheric regimes.}
	\label{fig:error_distribution}
\end{figure}

\textbf{CBH regime stratification} (Figure~\ref{fig:cbh_regime_errors}):
\begin{itemize}[leftmargin=*]
	\item \textbf{Low (0-500m):} MAE = 192.1 m, n = 157 (poorest performance)
	\item \textbf{Mid (500-1500m):} MAE = 103.7 m, n = 740 (best performance)
	\item \textbf{High (>1500m):} MAE = 230.4 m, n = 36 (challenging, limited data)
\end{itemize}

Performance is best in the mid-range CBH regime (500-1500m) where 79\% of training data reside. Low-altitude clouds show 1.9× higher error due to complex boundary layer turbulence and surface-atmosphere interactions not well-captured by ERA5's 25 km resolution.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\textwidth]{../outputs/stratified_error_analysis/figures/cbh_regime_errors.png}
	\caption{Error distribution stratified by CBH regime. Best performance in mid-range CBH (500-1500m, MAE=103.7m) where training data are concentrated. Low-altitude clouds show highest errors.}
	\label{fig:cbh_regime_errors}
\end{figure}

\textbf{Atmospheric stability stratification:}
\begin{itemize}[leftmargin=*]
	\item Low stability: MAE = 143.8 m, n = 303
	\item Medium stability: MAE = 114.0 m, n = 320
	\item High stability: MAE = 113.5 m, n = 310
\end{itemize}

Stable atmospheres show 1.3× better accuracy than unstable conditions, consistent with ERA5's better representation of stratified layers versus turbulent convection.

\textbf{Case studies:}
\begin{itemize}[leftmargin=*]
	\item Best prediction: True=720.0m, Pred=720.0m, Error=0.0m
	\item Worst prediction: True=630.0m, Pred=1910.7m, Error=-1280.7m (low CBH failure case)
	\item Median error: $\sim$75m
\end{itemize}

The worst-case 1281m error occurs for a low-altitude cloud (630m true CBH) predicted at 1911m, illustrating the systematic difficulty with shallow boundary layer clouds. The CNN shows higher variance across cross-validation folds (R$^2$ std = 0.152) compared to GBDT (std = 0.083), indicating less stable learning in the small-sample regime.

\subsection{Uncertainty Quantification}

We evaluate four uncertainty quantification methods, revealing a fundamental challenge: conformal prediction's exchangeability assumption is violated by temporal autocorrelation and domain shift, causing severe under-coverage.

\begin{table}[h]
	\centering
	\caption{Uncertainty quantification method comparison. Split conformal achieves only 27\% coverage (target: 90\%) due to exchangeability violations. Per-flight calibration recovers 86\% coverage by respecting flight boundaries.}
	\label{tab:uq_comparison}
	\begin{tabular}{lcccc}
		\toprule
		\textbf{Method}        & \textbf{Coverage} & \textbf{Target} & \textbf{Width (m)} & \textbf{Assessment}                \\
		\midrule
		Split Conformal        & 27\%              & 90\%            & 278                & Fails (exchangeability violated)   \\
		Adaptive Conformal     & 11\%              & 90\%            & 58                 & Fails (intervals collapse)         \\
		Quantile Regression    & 58\%              & 90\%            & 510                & Moderate under-coverage            \\
		Per-flight Calibration & \textbf{86\%}     & 90\%            & 313                & \textbf{Near-target (recommended)} \\
		\bottomrule
	\end{tabular}
\end{table}

\textbf{Root cause of conformal failure:} Split conformal prediction assumes data are exchangeable---that calibration and test samples are drawn from the same distribution in arbitrary order. This assumption fails catastrophically when:
\begin{enumerate}[leftmargin=*]
	\item \textbf{Temporal autocorrelation} ($\rho$ = 0.94): Adjacent samples have nearly identical CBH values, so calibration residuals computed on temporally-clustered data underestimate test-time errors.
	\item \textbf{Domain shift:} When calibration data come from different flights than test data, the residual distribution is non-representative (LOFO R$^2$ = -15.4).
\end{enumerate}

\textbf{Per-flight calibration} addresses these violations by calibrating within each flight independently, then evaluating on held-out portions of the same flight. This achieves 86\% coverage (close to 90\% target) with mean interval width of 313 m:
\begin{itemize}[leftmargin=*]
	\item Flight 1: 86\% coverage, 197 m intervals, R$^2$ = 0.93
	\item Flight 2: 85\% coverage, 352 m intervals, R$^2$ = 0.67
	\item Flight 3: 87\% coverage, 392 m intervals, R$^2$ = 0.48
\end{itemize}

\textbf{Operational recommendation:} For deployment, use per-flight calibration with locally-collected labeled samples. Cross-regime conformal prediction cannot provide valid coverage guarantees without explicit domain adaptation.

\subsection{Cross-Flight Domain Divergence}
\label{sec:domain_shift}

To quantify distribution shift across flight campaigns, we performed leave-one-flight-out (LOFO) cross-validation and computed Kolmogorov-Smirnov (K-S) divergence for each feature pair. Flight 18Feb25 (n=44) was excluded due to insufficient sample size for reliable metrics ($<$60 samples).

\textbf{Catastrophic domain shift observed:} LOFO validation reveals complete failure to generalize across flight campaigns, with all test flights yielding negative R$^2$ values (Table~\ref{tab:lofo_results}). Mean LOFO performance is R$^2$=-15.4, MAE=422 m, representing catastrophic degradation compared to within-campaign performance (R$^2$=0.744, per-flight shuffled). This indicates models predict substantially worse than a constant mean baseline when tested on unseen atmospheric regimes.

\begin{table}[h]
	\centering
	\caption{Leave-one-flight-out cross-validation results showing severe generalization failure across flight campaigns. All test flights achieve negative R$^2$ values.}
	\label{tab:lofo_results}
	\begin{tabular}{lccccc}
		\toprule
		\textbf{Test Flight} & \textbf{n\_test} & \textbf{n\_train} & \textbf{R$^2$}  & \textbf{MAE (m)} & \textbf{RMSE (m)}                                \\
		\midrule
		Flight 0 (30Oct24)   & 423              & 390               & -1.138          & 341.3            & 428.8                                            \\
		Flight 1 (10Feb25)   & 182              & 631               & -0.585          & 318.8            & 372.4                                            \\
		Flight 2 (23Oct24)   & 102              & 711               & -1.817          & 542.6            & 677.6                                            \\
		Flight 3 (12Feb25)   & 84               & 729               & -0.488          & 470.0            & 672.4                                            \\
		\midrule
		\textbf{Average}     & -                & -                 & \textbf{-1.007} & \textbf{418.2}   & \textbf{537.7}                                   \\
		\bottomrule
		\multicolumn{6}{l}{\footnotesize \textit{Note: Flight 4 (18Feb25, n=44) excluded due to insufficient sample size ($<$60). Total samples per row}}   \\
		\multicolumn{6}{l}{\footnotesize \textit{(n\_test + n\_train = 813) reflect 120 additional samples excluded due to temporal matching constraints.}} \\
	\end{tabular}
\end{table}

K-S divergence analysis (Figure~\ref{fig:ks_divergence}) shows significant feature distribution shifts across flights, with atmospheric variables (d2m, t2m, sp) exhibiting highest cross-flight divergence (K-S $>$ 0.4, p $<$ 0.001). PCA visualization (Figure~\ref{fig:pca_clustering}) reveals flights cluster by campaign, with PC1 explaining 36.0\% of variance and PC2 explaining 14.4\%. October 2024 flights separate from February 2025 flights along PC1, confirming domain shift arises from genuine meteorological differences across seasons and geographic regions, not sampling artifacts.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\textwidth]{../outputs/domain_analysis/figures/ks_divergence_heatmap.png}
	\caption{Kolmogorov-Smirnov divergence heatmap showing top 10 most divergent features across flight pairs. High K-S statistics (red) indicate significant distribution shifts. Atmospheric variables (d2m, t2m, sp) show strongest divergence (K-S $>$ 0.4).}
	\label{fig:ks_divergence}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\textwidth]{../outputs/domain_analysis/figures/pca_flight_clustering.png}
	\caption{PCA visualization of feature distributions colored by flight ID. Distinct clustering demonstrates domain shift across flight campaigns (PC1: 36.0\% variance, PC2: 14.4\% variance). October 2024 and February 2025 campaigns separate along PC1.}
	\label{fig:pca_clustering}
\end{figure}

\textbf{Implications:} The severe domain shift highlights a critical limitation for operational deployment. Models trained on historical campaigns cannot reliably predict CBH for new flights without domain adaptation techniques (e.g., transfer learning, domain-adversarial training). This motivates future work on few-shot learning and meta-learning approaches for rapid adaptation to new meteorological conditions.

\subsection{Computational Cost and Deployment Feasibility}

Table~\ref{tab:computationalcost} compares training time, inference latency, and model size across architectures.

\input{../outputs/computational_cost/reports/computational_cost_table.tex}

\textbf{Key findings:}
\begin{itemize}[leftmargin=*]
	\item \textbf{GBDT:} 1.04s training, 0.28ms inference, 1.3 MB model, CPU-only
	\item \textbf{SimpleCNN:} 19.3s training, 1.22ms inference, 98.4 MB model, GPU preferred
	\item \textbf{ResNet-18:} 7.4s training, 2.62ms inference, 42.7 MB model, GPU preferred
	\item \textbf{EfficientNet-B0:} 14.6s training, 7.35ms inference, 15.6 MB model, GPU preferred
\end{itemize}

GBDT offers:
\begin{itemize}[leftmargin=*]
	\item \textbf{4.3× faster inference} than SimpleCNN (0.28ms vs 1.22ms)
	\item \textbf{9.3× faster inference} than ResNet-18
	\item \textbf{26× faster inference} than EfficientNet-B0
	\item \textbf{76× smaller model} than SimpleCNN (1.3 MB vs 98.4 MB)
	\item \textbf{No GPU requirement} (CPU inference sufficient)
\end{itemize}

\textbf{Deployment implications:}
\begin{enumerate}[leftmargin=*]
	\item \textbf{Real-time aircraft deployment:} GBDT's 0.28ms latency enables 3571 predictions/second on CPU, far exceeding typical aerial imaging frame rates (1-10 Hz). The 1.3 MB model fits in embedded system memory.

	\item \textbf{Ground-based batch processing:} All models are viable. Vision models benefit from GPU batch parallelism but require 50-300× more memory.

	\item \textbf{Edge computing:} GBDT is the only feasible option for low-power edge devices (Raspberry Pi, embedded CPUs) due to CPU-only inference and minimal memory footprint.
\end{enumerate}

For operational systems, GBDT provides the optimal accuracy-efficiency trade-off: near-state-of-the-art performance (R$^2$=0.744, per-flight shuffled validation) with inference costs 5-26× lower than vision alternatives. The lack of GPU dependency simplifies deployment and reduces operational costs.

\subsection{Domain Adaptation}
\label{sec:domain_adaptation}

Leave-one-flight-out (LOFO) cross-validation reveals catastrophic domain shift across all flights. Table~\ref{tab:lofo_detailed} presents per-flight LOFO results showing mean R$^2$ = -15.4, indicating predictions are substantially worse than a constant mean baseline.

\begin{table}[h]
	\centering
	\caption{Leave-one-flight-out cross-validation results. All held-out flights achieve negative R$^2$, indicating complete generalization failure.}
	\label{tab:lofo_detailed}
	\begin{tabular}{lcccc}
		\toprule
		\textbf{Test Flight} & \textbf{n\_test} & \textbf{n\_train} & \textbf{R$^2$} & \textbf{MAE (km)} \\
		\midrule
		Flight 1             & 1,021            & 415               & -6.61          & 0.577             \\
		Flight 2             & 129              & 1,307             & 0.15           & 0.119             \\
		Flight 3             & 276              & 1,160             & -0.80          & 0.210             \\
		\midrule
		\textbf{Mean}        & --               & --                & \textbf{-15.4} & \textbf{0.422}    \\
		\bottomrule
	\end{tabular}
\end{table}

\subsubsection{Domain Shift Quantification}

We quantify domain shift using Maximum Mean Discrepancy (MMD) and A-distance metrics between flight pairs. Mean pairwise MMD = 1.29 (on standardized features), with A-distance = 2.0 for all pairs, indicating flights are perfectly separable by a linear classifier---confirming severe distribution shift.

\subsubsection{Domain Adaptation Methods}

We evaluate five domain adaptation approaches:

\textbf{1. Few-Shot Learning (Most Effective):} Fine-tuning on $k$ labeled samples from the target flight dramatically improves performance. Table~\ref{tab:few_shot} shows results across shots and flights.

\begin{table}[h]
	\centering
	\caption{Few-shot adaptation performance (R$^2$) by target flight and number of labeled samples. Few-shot is the most practical domain adaptation method.}
	\label{tab:few_shot}
	\begin{tabular}{lcccc}
		\toprule
		\textbf{Target Flight} & \textbf{5-shot}  & \textbf{10-shot} & \textbf{20-shot} & \textbf{50-shot}         \\
		\midrule
		Flight 1 (n=1,021)     & 0.47 $\pm$ 0.25  & 0.76 $\pm$ 0.04  & 0.81 $\pm$ 0.03  & \textbf{0.85 $\pm$ 0.04} \\
		Flight 2 (n=129)       & 0.14 $\pm$ 0.13  & 0.22 $\pm$ 0.23  & 0.39 $\pm$ 0.25  & \textbf{0.64 $\pm$ 0.07} \\
		Flight 3 (n=276)       & -0.37 $\pm$ 0.21 & -0.14 $\pm$ 0.26 & 0.02 $\pm$ 0.20  & \textbf{0.23 $\pm$ 0.15} \\
		\midrule
		\textbf{Mean}          & 0.08             & 0.28             & 0.41             & \textbf{0.57}            \\
		\bottomrule
	\end{tabular}
\end{table}

\textbf{Key finding:} Flight 1 shows excellent few-shot recovery (R$^2$ = 0.85 with 50 samples), while Flight 3 remains challenging (R$^2$ = 0.23) due to greater atmospheric regime differences. The aggregated mean improves monotonically with shots: 5-shot (0.08) $\rightarrow$ 50-shot (0.57).

\textbf{2. Instance Weighting:} KNN-based and density-based sample weighting to emphasize source samples similar to target. Mean R$^2$ = -21.4 (KNN) and -19.9 (density), \textit{worse than baseline}. Sample weighting fails because no source samples are sufficiently similar to target regime.

\textbf{3. TrAdaBoost:} Transfer learning via boosting that down-weights poorly-transferring source samples. Mean R$^2$ = -0.41, a modest improvement over baseline (-15.4) but still negative.

\textbf{4. MMD Feature Alignment:} Projecting features to minimize Maximum Mean Discrepancy between source and target distributions. Mean R$^2$ = -39.4, substantially \textit{worse} than baseline. Feature alignment reduces MMD by 9.4\% on average but destroys predictive signal.

\textbf{Recommendation:} For operational deployment to new atmospheric regimes, collect 20--50 labeled samples from the target regime and fine-tune the base model. This provides the best accuracy-efficiency tradeoff.

\section{Discussion}
\label{sec:discussion}

\subsection{Why Do Atmospheric Features Outperform Images?}

Our results demonstrate a clear advantage for atmospheric reanalysis features over learned image representations. We hypothesize four contributing factors:

\subsubsection{Physical Causality}

Cloud base height is fundamentally determined by atmospheric thermodynamics: the altitude where rising air parcels reach saturation (lifting condensation level). ERA5 features directly measure temperature and moisture profiles that govern this process, providing causal predictors. In contrast, cloud appearance in images is an \textit{effect} of CBH rather than a cause, requiring the model to invert the causal relationship.

\subsubsection{Information Content}

ERA5 provides vertical atmospheric structure through 37 pressure levels, capturing the full column thermodynamic state. Passive imagery observes only cloud tops and sides, with limited information about vertical extent. The image modality lacks explicit altitude information that ERA5 encodes.

\subsubsection{Sample Complexity}

CNNs typically require large datasets (thousands to millions of examples) to learn robust features \cite{Krizhevsky2012}. With only 1,426 training samples, our CNN underfits, failing to learn generalizable cloud morphology patterns. GBDT models excel in low-data regimes by using simple decision boundaries rather than hierarchical feature learning.

\subsubsection{Domain Shift}

Airborne camera imagery exhibits high variability in illumination, sun angle, atmospheric scattering, and cloud types across flights. ERA5 features are standardized physical quantities less sensitive to observational conditions. The CNN's higher cross-flight variance supports this interpretation.

\subsection{Physical Interpretation of Feature Importance}

Our SHAP analysis reveals that near-surface thermodynamic variables (d2m, t2m) dominate CBH predictions. This aligns with fundamental cloud physics:

\textbf{Dewpoint temperature (d2m) as primary predictor:} The dewpoint marks the temperature at which air becomes saturated. For rising air parcels, the lifting condensation level (LCL)---a first-order approximation of cloud base height---can be estimated from surface temperature and dewpoint via:
\begin{equation}
	\text{LCL} \approx 125 \times (T - T_d) \text{ meters}
\end{equation}
where $T$ is surface temperature and $T_d$ is dewpoint temperature \cite{Lawrence2005}. The dominance of d2m (mean\_abs\_shap=87.73) directly reflects this physical relationship.

\textbf{Temperature (t2m) contribution:} Surface temperature determines the initial parcel energy and influences convective available potential energy (CAPE). Higher t2m enables deeper convection and potentially higher cloud bases in convective regimes.

\textbf{Stability and moisture gradients:} The importance of stability\_index (rank 3) and moisture\_gradient (rank 4) captures vertical atmospheric structure. Stable layers inhibit mixing and constrain cloud base to specific altitudes, while moisture gradients determine where saturation occurs.

\textbf{Geometric features less critical than expected:} Solar angle and shadow length (ranks 6-10) show lower importance than hypothesized. Trigonometric cloud base estimation from shadow displacement---while physically valid---is less reliable than thermodynamic approaches due to shadow detection uncertainty and complex terrain effects.

\textbf{Robust distributed representation:} No single feature removal degrades R$^2$ by >1\%, indicating the model learns redundant pathways to CBH prediction. This graceful degradation is desirable for operational robustness: sensor failures or missing ERA5 fields will not cause catastrophic performance loss.

\subsection{Physical Plausibility Validation}
\label{sec:physics_validation}

To verify that the GBDT model learns physically consistent relationships rather than spurious correlations, we evaluated predictions against fundamental atmospheric constraints using an independent test set (n=163, 17.5\% of data).

\subsubsection{Constraint Satisfaction}

Table~\ref{tab:physics_validation} presents constraint violation rates. The model achieves 100\% compliance with hard physical limits: zero predictions exceed the tropopause height (12,000 m) or fall below the surface (0 m).

\begin{table}[ht]
	\centering
	\caption{Physical Plausibility Constraint Validation. All hard constraints satisfied (0\% violations). Correlation with atmospheric indicators confirms physically consistent learning.}
	\label{tab:physics_validation}
	\begin{tabular}{lccc}
		\toprule
		\textbf{Constraint}                  & \textbf{Expected}                                                & \textbf{Observed} & \textbf{Violations} \\
		\midrule
		CBH $\leq$ 12,000 m (Tropopause)     & 100\%                                                            & 100\%             & 0/163 (0.0\%)       \\
		CBH $\geq$ 0 m (Surface)             & 100\%                                                            & 100\%             & 0/163 (0.0\%)       \\
		Corr(LCL, CBH$_{\text{pred}}$) $> 0$ & Positive                                                         & r=0.26*           & N/A                 \\
		Corr(BLH, CBH$_{\text{pred}}$) $> 0$ & Positive                                                         & r=0.14*           & N/A                 \\
		\midrule
		\textbf{Model Performance}           & \multicolumn{3}{c}{R$^2$ = 0.672, MAE = 134.4 m, RMSE = 220.1 m}                                           \\
		\bottomrule
		\multicolumn{4}{l}{\footnotesize \textit{Note: *** p$<$0.001, * p$<$0.05}}                                                                        \\
	\end{tabular}
\end{table}

\textbf{Boundary layer height correlation:} Predicted CBH shows expected positive correlation with boundary layer height (BLH, r=0.136, p=0.083), though the relationship is weak. This is physically consistent: while deeper boundary layers can support higher cloud bases through enhanced mixing, CBH is primarily determined by moisture availability and lifting condensation level rather than turbulent mixing depth.

\subsubsection{Comparison to Physics-Based Lifting Condensation Level}

The lifting condensation level (LCL) provides a physics-based first-order estimate of cloud base height from surface thermodynamics. Figure~\ref{fig:cbh_vs_lcl} compares true and predicted CBH against LCL.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.9\textwidth]{../outputs/physics_validation/figures/cbh_vs_lcl_validation.png}
	\caption{Cloud base height vs. lifting condensation level validation. \textbf{Left:} Predicted CBH shows statistically significant positive correlation with LCL (r=0.26, p$<$0.05), demonstrating the model learns physically consistent relationships. \textbf{Right:} True CBH vs. LCL (r=0.28, p$<$0.05) serves as a reference baseline. The moderate correlations reflect that CBH depends on multiple factors beyond LCL, including atmospheric stability, entrainment, and multi-layer effects. Deviations from 1:1 line occur when boundary layer dynamics cause CBH to differ from simple thermodynamic LCL estimates.}
	\label{fig:cbh_vs_lcl}
\end{figure}

\textbf{Key findings:}
\begin{itemize}[leftmargin=*]
	\item \textbf{Predicted CBH vs. LCL:} Statistically significant positive correlation r=0.26 (p$<$0.05), consistent with the true CBH-LCL correlation (r=0.28). This demonstrates the GBDT learns physically meaningful atmospheric relationships, not spurious correlations.

	\item \textbf{True CBH vs. LCL:} Correlation r=0.28 (p$<$0.05), confirming LCL as a valid physics-based CBH indicator. The moderate correlation reflects that actual CBH depends on additional factors: atmospheric stability, entrainment, radiative effects, multi-layer cloud systems, and the spatial/temporal resolution limitations of ERA5 reanalysis (25 km, hourly).

	\item \textbf{Interpretation:} The model's predicted CBH shows correlation with LCL (r=0.26) comparable to the true CBH-LCL relationship (r=0.28), indicating it has learned to incorporate the fundamental LCL relationship. The moderate correlations are expected given ERA5's 25 km resolution cannot capture sub-grid variability in surface temperature and humidity that controls local LCL. The model's superior overall performance (R$^2$=0.744) indicates it successfully exploits additional atmospheric structure from the full feature set beyond LCL alone.
\end{itemize}

\subsubsection{Case Study Analysis}

Examining extreme prediction cases (Table~\ref{tab:physics_validation}) reveals:
\begin{itemize}[leftmargin=*]
	\item \textbf{Best prediction:} 1.4 m error (True=1320 m, Pred=1321 m), demonstrating near-perfect retrieval in favorable conditions
	\item \textbf{Worst prediction:} 1008 m error (True=690 m, Pred=1698 m), a low-altitude cloud misclassified as mid-level---consistent with stratified error analysis showing poorest performance for CBH <500 m
	\item \textbf{Median error:} 84 m, indicating typical performance exceeds MAE (134 m) due to heavy-tailed error distribution with occasional large failures
\end{itemize}

These results validate that the model learns physically plausible CBH retrievals: zero unphysical predictions, expected correlation with atmospheric boundary layer, and error patterns consistent with known ERA5 limitations (boundary layer resolution). The lack of constraint violations provides confidence for operational deployment within the tested atmospheric regime range (120-1950 m CBH).

\subsection{Error Regimes and Physical Mechanisms}

Stratified error analysis reveals systematic performance variations across atmospheric regimes that reflect physical processes:

\textbf{Low CBH difficulty (0-500m, MAE=192m):} Shallow boundary layer clouds pose challenges because:
\begin{enumerate}[leftmargin=*]
	\item ERA5's 25 km horizontal resolution cannot resolve small-scale turbulent eddies that control boundary layer mixing
	\item Surface heterogeneity (vegetation, urban heat islands) creates local CBH variability not captured by gridded reanalysis
	\item Radiation fog and stratus are sensitive to micro-meteorological conditions (surface cooling, local moisture sources)
\end{enumerate}

\textbf{Mid-range CBH success (500-1500m, MAE=104m):} Best performance occurs where:
\begin{enumerate}[leftmargin=*]
	\item 79\% of training data reside (statistical advantage)
	\item Cloud formation is governed by large-scale lifting and moisture convergence well-represented in ERA5
	\item Stratocumulus and cumulus clouds follow more predictable thermodynamic relationships
\end{enumerate}

\textbf{High CBH challenges (>1500m, MAE=230m):} Deep convective clouds and cirrus show larger errors due to:
\begin{enumerate}[leftmargin=*]
	\item Limited training data (n=36, only 4\% of dataset)
	\item Multi-layer cloud systems where CPL may detect middle/high clouds rather than true base
	\item Convective instability making cloud base height more variable and less predictable from reanalysis
\end{enumerate}

\textbf{Stability dependence:} 1.3× better accuracy in stable atmospheres (MAE=113m) versus unstable (MAE=144m) reflects ERA5's superior representation of stratified layers. Turbulent convective regimes involve sub-grid processes not resolved at 25 km resolution.

These physical interpretations guide future improvements: higher-resolution numerical weather prediction, explicit turbulence parameterizations, or hybrid models combining ERA5 with local observations could address regime-specific failures.

\subsection{Limited Ensemble Complementarity}

The minimal improvement from ensembles (R$^2$ gain < 0.005) indicates that atmospheric and visual features capture largely overlapping information. This contradicts expectations from multi-modal learning \cite{Ngiam2011}, where different modalities often provide complementary signals.

We speculate that both modalities learn similar patterns: the GBDT identifies atmospheric conditions conducive to specific CBH values, while the CNN learns to recognize cloud appearances associated with those same conditions. Since cloud appearance is determined by atmospheric state, the two representations are not independent.

This finding has practical implications: operational systems achieve near-optimal performance using atmospheric features alone, avoiding the computational cost and engineering complexity of image processing.

\subsection{Domain Shift and Generalization}

The catastrophic LOFO validation failures (Section~\ref{sec:domain_shift}) represent the most critical finding of this work: all three held-out flights achieve negative R$^2$ values (mean R$^2$ = -15.4, MAE = 422 m), indicating predictions substantially worse than a constant mean baseline. This demonstrates complete generalization failure across atmospheric regimes.

\subsubsection{Root Causes of Domain Shift}

Three factors contribute to cross-flight generalization failure:

\textbf{1. Campaign-level atmospheric differences:} K-S divergence analysis (Figure~\ref{fig:ks_divergence}) reveals substantial distribution shift in key features:
\begin{itemize}[leftmargin=*]
	\item Total column water vapor (K-S = 0.80): Fall WHYMSIE 2024 (Flights 0, 2) vs. winter GLOVE 2025 (Flights 1, 3) campaigns have fundamentally different moisture regimes
	\item Surface temperature (K-S = 0.72): Seasonal differences (October vs. February) create non-overlapping temperature distributions
	\item Lifting condensation level (K-S = 0.75): Different cloud formation mechanisms across campaigns
\end{itemize}

\textbf{2. Feature space non-overlap:} PCA analysis (Figure~\ref{fig:pca_clustering}) shows flights occupy distinct regions of the 15-dimensional feature space with minimal overlap. Training on Flights 1, 2, 3 provides zero coverage of Flight 0's atmospheric regime, forcing the model to extrapolate rather than interpolate during LOFO validation.

\textbf{3. Learned campaign-specific relationships:} The GBDT model learns decision boundaries optimized for the training distribution. When test flights present feature combinations never seen during training (e.g., high tcwv + low t2m from winter campaigns), the model defaults to training set averages, producing systematically biased predictions that reduce R$^2$ below zero.

\subsubsection{Implications for Operational Deployment}

The severe domain shift has critical implications:

\begin{enumerate}[leftmargin=*]
	\item \textbf{Geographic generalization uncertain:} Our flights span limited geographic regions (primarily continental U.S.). Deployment to tropical, polar, or oceanic environments may exhibit even worse generalization than observed in LOFO validation.

	\item \textbf{Seasonal adaptation required:} The model cannot reliably transfer between fall and winter campaigns without retraining or fine-tuning. Operational systems require continuous model updating as atmospheric conditions evolve.

	\item \textbf{Campaign-specific calibration necessary:} High within-campaign performance (R$^2$ = 0.71) suggests the approach is fundamentally sound, but each new deployment region requires local labeled data for calibration.
\end{enumerate}

\subsubsection{Paths Forward}

More sophisticated approaches may address cross-flight generalization:

\begin{enumerate}[leftmargin=*]
	\item \textbf{Domain adversarial training:} Learn features invariant to flight ID \cite{Ganin2016}
	\item \textbf{Meta-learning:} Optimize for fast adaptation to new flights \cite{Finn2017}
	\item \textbf{Covariate shift correction:} Re-weight training samples to match test distribution \cite{Shimodaira2000}
	\item \textbf{Physics-informed regularization:} Constrain predictions to obey atmospheric stability criteria, preventing unphysical extrapolation
	\item \textbf{Multi-campaign training:} Aggregate data across diverse atmospheric regimes to improve generalization, though our results suggest this may be insufficient without architectural changes
\end{enumerate}

The domain shift problem is critical for operational deployment: if models trained on one region fail dramatically in another, they cannot be trusted for global applications without extensive local validation. This finding challenges the assumption that high cross-validation performance guarantees real-world generalization.

\subsubsection{Practical Deployment Considerations}

\textbf{Important distinction:} The severe domain shift observed in LOFO validation applies specifically to \textit{cross-regime generalization}---deploying models trained on one meteorological regime (e.g., fall WHYMSIE 2024) to entirely different atmospheric conditions (e.g., winter GLOVE 2025). This does \textit{not} preclude successful operational deployment within the same campaign or meteorological regime.

\textbf{Within-campaign deployment is production-ready:} Our within-campaign cross-validation results (R$^2$ = 0.744, MAE = 117.4 m, per-flight shuffled) demonstrate that models achieve operational accuracy when applied to the same atmospheric regime they were trained on. For practical applications:

\begin{itemize}[leftmargin=*]
	\item \textbf{Intra-season deployment:} A model trained on October 2024 WHYMSIE flights can reliably predict CBH for subsequent October 2024 flights in the same geographic region, as these share similar atmospheric conditions.

	\item \textbf{Regional operational systems:} Aircraft operating within a specific geographic region and season can use models trained on representative local data, achieving the 117.4 m MAE performance demonstrated in our validation.

	\item \textbf{Periodic recalibration:} Operational systems should retrain models seasonally or when deploying to new geographic regions, rather than attempting universal generalization.

	\item \textbf{Uncertainty-aware deployment:} Per-flight calibration (86\% coverage) enable real-time detection of distribution shift. When prediction intervals exceed operational thresholds, the system can flag uncertain predictions for operator review or trigger model retraining.
\end{itemize}

\textbf{The key takeaway:} Our results demonstrate that atmospheric feature-based CBH retrieval achieves production-ready accuracy (MAE = 117.4 m, 0.28 ms inference) for within-regime deployment. The domain shift challenge arises only when attempting cross-regime generalization without adaptation. Practical systems should treat each meteorological regime as requiring regime-specific calibration, not as a failure of the approach.

\subsection{Comparison to Prior Work}

Direct comparison to prior CBH retrieval methods is challenging due to differences in data sources, evaluation metrics, and spatial scales. However, we can contextualize our results:

\begin{itemize}[leftmargin=*]
	\item \textbf{Satellite retrievals:} MODIS cloud base products achieve ~500 m uncertainty \cite{Minnis2008}, worse than our 117 m MAE but over global scales.

	\item \textbf{Ceilometer networks:} Ground-based lidars achieve ~15 m accuracy \cite{Martucci2010} but with limited coverage.

	\item \textbf{Reanalysis products:} ERA5 cloud base estimates show ~800 m RMSE vs radiosonde \cite{Benas2020}, higher than our 187 m.
\end{itemize}

Our approach occupies a middle ground: better accuracy than passive satellite methods, worse than active lidars, but with broader spatial coverage than ground-based sensors.

\subsection{Implications for Atmospheric Machine Learning}

Our findings provide several lessons for ML applications in atmospheric science:

\begin{enumerate}[leftmargin=*]
	\item \textbf{Physics-informed features outperform vision:} Domain knowledge for feature engineering captures cloud formation physics more effectively than end-to-end learning. GBDT with 15 atmospheric features achieves 22.2\% lower MAE than ResNet-18 despite deep learning's theoretical capacity for arbitrary representation learning.

	\item \textbf{Computational efficiency enables deployment:} GBDT's 0.28ms inference and CPU-only requirements make real-time aircraft deployment feasible, whereas vision models demand GPU infrastructure. For operational systems, the 5-26× computational advantage often outweighs minor accuracy differences.

	\item \textbf{Negative results are valuable:} Documenting when ensembles and images \textit{don't} help guides resource allocation. Our finding that multi-modal fusion provides <1\% R$^2$ gain suggests practitioners can avoid the engineering complexity of image pipelines.

	\item \textbf{Generalization requires attention:} High within-distribution performance (R$^2$=0.744) masks severe domain shift (R$^2$=-15.4 on out-of-distribution flights). Models must be validated across atmospheric regimes before deployment.

	\item \textbf{Per-flight calibration is essential:} Conformal prediction provides operational decision support by flagging uncertain predictions. Per-flight calibration achieves 86\% coverage

	\item \textbf{Feature ablation reveals robustness:} No single feature causes >1\% performance degradation, indicating graceful handling of missing sensors or ERA5 fields in operational scenarios.

	\item \textbf{Error stratification guides improvements:} Identifying low-CBH difficulty (MAE=192m) and high-CBH challenges (MAE=230m) prioritizes future research on boundary layer turbulence and multi-layer clouds.
\end{enumerate}

\section{Limitations and Future Work}
\label{sec:limitations}

\subsection{Limitations}

\subsubsection{Data Limitations}

Our dataset of 1,426 samples from 3 flights with sufficient data (after excluding flights with $<$20 samples) is small by deep learning standards, potentially limiting CNN performance. Extending to thousands of labeled examples via additional flight campaigns or semi-supervised learning could improve image model accuracy.

Geographic coverage is limited to NASA ER-2 flight paths from two campaigns (GLOVE 2025 and WHYMSIE 2024). Generalization to tropical, polar, or oceanic regimes remains unvalidated.

\subsubsection{Model Limitations}

Our CNN architecture is intentionally simple to avoid overfitting. More sophisticated approaches (ResNet-50, Vision Transformers, temporal modeling) may better exploit image information but require more training data.

\textbf{Vision model architecture:} We evaluated state-of-the-art vision models including ResNet-18 and EfficientNet-B0 with ImageNet pre-training. Our best vision model, ResNet-18 from scratch (R$^2$ = 0.617, MAE = 150.9 m), still underperforms atmospheric features (R$^2$ = 0.744, MAE = 117.4 m, per-flight shuffled) substantially. More complex architectures (ResNet-50, Vision Transformers) may provide incremental improvements but are unlikely to close this fundamental performance gap, as literature on cloud property retrieval \cite{Matsuoka2018, Zantedeschi2019} shows sophisticated architectures yield 10-20\% relative gains rather than order-of-magnitude advances.

Uncertainty quantification via split conformal prediction fails dramatically (27\% coverage vs 90\% target) due to exchangeability violations from temporal autocorrelation (lag-1 $\rho$ = 0.94) and domain shift. Per-flight calibration recovers 86\% coverage but requires flight-specific labeled data.

\subsubsection{Methodological Limitations}

Our approach has several methodological constraints:

\begin{itemize}[leftmargin=*]
	\item \textbf{ERA5 spatial resolution:} The 25 km horizontal grid cannot capture fine-scale atmospheric variability (turbulent eddies, local moisture sources), limiting accuracy for low-altitude clouds controlled by micro-meteorology.

	\item \textbf{Limited temporal coverage:} Our dataset comprises 1,426 samples from 3 flights across 2 field campaigns (GLOVE 2025, WHYMSIE 2024), constraining generalization to other geographic regions, seasons, and climate regimes.

	\item \textbf{Shadow detection assumptions:} Automated cloud shadow detection relies on brightness thresholds that may fail in complex illumination (thin clouds, multiple cloud layers, low solar elevation), introducing noise in geometric features.

	\item \textbf{Domain generalization failure:} Leave-one-flight-out validation reveals catastrophic failure (mean R$^2$ = -15.4, MAE = 422 m across 3 held-out flights) for out-of-distribution atmospheric regimes, limiting deployment confidence without explicit domain adaptation. This represents the most critical limitation of the current approach.
\end{itemize}

\subsubsection{Evaluation Limitations}

CPL lidar retrievals serve as ground truth, but themselves have uncertainty (~30 m vertical resolution, cloud edge detection ambiguity). This sets a lower bound on achievable MAE.

Cross-flight validation assesses one axis of distribution shift (meteorological regime) but not others (geographic region, sensor degradation, climate change).

\subsection{Future Research Directions}

\subsubsection{Improved Image Models}

\begin{itemize}[leftmargin=*]
	\item \textbf{Pre-training on atmospheric data:} Self-supervised learning on unlabeled cloud imagery (e.g., SimCLR \cite{Chen2020}) could provide better initialization than ImageNet.

	\item \textbf{Temporal modeling:} Video sequences of cloud evolution may contain more information than single frames. Temporal convolutional networks or transformers could exploit this.

	\item \textbf{Multi-scale architectures:} Clouds exhibit structure across spatial scales. Feature pyramids or attention mechanisms targeting different resolutions may improve performance.
\end{itemize}

\subsubsection{Hybrid Physics-ML Approaches}

\begin{itemize}[leftmargin=*]
	\item \textbf{Physics-informed neural networks:} Constrain predictions to satisfy thermodynamic equations (e.g., LCL formula as a soft constraint).

	\item \textbf{Differentiable physics models:} Embed simplified cloud formation equations in the neural network architecture.

	\item \textbf{Residual learning:} Predict corrections to physics-based LCL estimates rather than CBH directly.
\end{itemize}

\subsubsection{Domain Adaptation}

\begin{itemize}[leftmargin=*]
	\item \textbf{Root-cause analysis:} Investigate why 18Feb25 fails (feature distribution analysis, covariate shift decomposition).

	\item \textbf{Active learning:} Intelligently select which samples to label in new domains to maximize adaptation efficiency.

	\item \textbf{Multi-source learning:} Combine ER-2 data with ground-based ceilometers or satellite retrievals for broader coverage.
\end{itemize}

\subsubsection{Operational Deployment}

\begin{itemize}[leftmargin=*]
	\item \textbf{Real-time inference:} Optimize models for low-latency prediction during flight operations.

	\item \textbf{Model monitoring:} Detect distribution shift and performance degradation in production.

	\item \textbf{Human-in-the-loop:} Design interfaces for meteorologists to provide feedback and corrections.
\end{itemize}

\section{Conclusion}
\label{sec:conclusion}

We have presented a systematic comparison of atmospheric feature-based and image-based machine learning approaches for cloud base height retrieval using 1,426 NASA ER-2 airborne observations from 3 research flights spanning 2 field campaigns. Our key findings are:

\begin{enumerate}[leftmargin=*]
	\item \textbf{Atmospheric features dominate:} GBDT models using 10 base ERA5-derived features achieve R$^2$ = 0.744 (MAE = 117.4 m) under rigorous per-flight shuffled validation, outperforming CNNs on imagery by substantial margins. With 28 engineered physics-based features (38 total), virtual\_temperature (33\%) and stability\_x\_tcwv (22\%) become top predictors.

	\item \textbf{Validation methodology is critical:} Pooled K-fold CV reports R$^2$ = 0.924, but this is inflated by temporal autocorrelation (lag-1 $\rho$ = 0.94). Per-flight shuffled validation (R$^2$ = 0.744) provides honest within-regime metrics. Time-ordered holdout (R$^2$ = -0.055) reveals difficulty extrapolating forward in time.

	\item \textbf{Domain shift is catastrophic:} Leave-one-flight-out validation reveals mean R$^2$ = -15.4, indicating predictions substantially worse than a constant baseline when generalizing across atmospheric regimes. This represents the most severe domain shift reported in atmospheric ML literature.

	\item \textbf{Few-shot adaptation is practical:} With just 50 labeled samples from a target flight, R$^2$ recovers to 0.57 (mean) and up to 0.85 for similar regimes. This provides a viable operational protocol: collect limited calibration data before deployment.

	\item \textbf{Conformal prediction fails without exchangeability:} Split conformal achieves only 27\% coverage (target: 90\%) due to temporal autocorrelation and domain shift violating exchangeability. Per-flight calibration recovers 86\% coverage with 277 m intervals.

	\item \textbf{Computational efficiency enables deployment:} GBDT achieves 0.28 ms inference, 1.3 MB model size, CPU-only operation---enabling real-time aircraft deployment with 5-26$\times$ faster inference than vision models.

	\item \textbf{Feature importance aligns with physics:} Surface temperature (t2m) dominates base model predictions (72\% importance), consistent with lifting condensation level thermodynamics. The model learns physically consistent relationships with zero unphysical predictions.
\end{enumerate}

\textbf{Honest assessment of when this approach works and fails:}

\begin{itemize}[leftmargin=*]
	\item \textbf{Works:} Within-flight deployment with shuffled train/test splits (R$^2$ = 0.744), and cross-regime deployment with few-shot adaptation (50 samples $\rightarrow$ R$^2$ = 0.57--0.85).
	\item \textbf{Fails:} Cross-regime generalization without adaptation (R$^2$ = -15.4), temporal extrapolation within flights (R$^2$ = -0.055), and conformal prediction under exchangeability violations (27\% coverage).
\end{itemize}

Our results demonstrate that physics-informed feature engineering captures cloud formation processes more effectively than end-to-end deep learning, but domain shift remains a fundamental challenge. The path to operational deployment requires explicit domain adaptation---not universal models trained once and deployed everywhere.

Future work should prioritize: (1) few-shot learning protocols for rapid regime adaptation, (2) physics-informed constraints to prevent unphysical extrapolation, (3) per-flight uncertainty calibration for honest prediction intervals, and (4) multi-campaign training to expand regime coverage. The severe domain shift finding (-15.4 R$^2$) represents our most important contribution: it establishes realistic expectations for atmospheric ML and highlights the gap between within-distribution performance and cross-regime generalization.

We hope that our open-source release and honest documentation of failures enables the atmospheric science community to build upon these findings with appropriate caution about generalization claims. The code, data, and trained models are available at \url{https://github.com/rylanmalarchick/CloudMLPublic}.

\section*{Acknowledgments}

This work builds upon methods developed during the author's NASA OSTEM internship (May--August 2025) with the NASA Goddard Space Flight Center High Altitude Research Program. The author thanks Dr. Dong Wu and the NASA ER-2 flight team for data access and technical discussions during the internship period. All analysis, code development, model training, and results presented in this paper were conducted independently by the author following the internship conclusion. ERA5 reanalysis data were provided by the European Centre for Medium-Range Weather Forecasts (ECMWF) Copernicus Climate Data Store. NASA ER-2 camera and Cloud Physics Lidar data are available through the NASA High Altitude Research Program. The author also acknowledges Embry-Riddle Aeronautical University for providing the academic support and resources necessary to complete this independent study.

\section*{Code and Data Availability}

\textbf{Code:} The complete CloudMLPublic framework, including all data preprocessing pipelines, model implementations, training scripts, evaluation code, and visualization tools, is open-source and available at \url{https://github.com/rylanmalarchick/CloudMLPublic} under the MIT License.

\textbf{Data:} NASA ER-2 downward-looking camera imagery is available through the NASA High Altitude Research Program data portal at \url{https://har.gsfc.nasa.gov/}. Cloud Physics Lidar (CPL) data can be requested from the NASA Goddard Space Flight Center Cloud Physics Lidar team (\url{https://cpl.gsfc.nasa.gov/}). ERA5 reanalysis data are publicly available from the ECMWF Copernicus Climate Data Store (\url{https://cds.climate.copernicus.eu/}).

\textbf{Reproducibility:} All experiments are fully reproducible using the provided configuration files and random seeds (seed=42). Trained model weights and preprocessed datasets are available upon request. Estimated compute time for full reproduction: ~18 hours on a single NVIDIA GTX 1070 Ti GPU.

\section*{Ethics Statement}

All data used in this work are from publicly available NASA Earth science missions. No proprietary, classified, or privacy-sensitive information is included. This research represents independent academic work conducted by the author following the conclusion of a NASA internship, with appropriate acknowledgment of the collaboration context. The open-source release aims to promote transparency and reproducibility in atmospheric machine learning research.

\bibliographystyle{plain}
\begin{thebibliography}{99}

	\bibitem[Alishouse et al.(1990)]{Alishouse1990}
	Alishouse, J.C., et al. (1990).
	\newblock Determination of oceanic total precipitable water from the SSM/I.
	\newblock \textit{IEEE Trans. Geosci. Remote Sens.}, 28(5), 811--816.

	\bibitem[Baltrušaitis et al.(2019)]{Baltrušaitis2019}
	Baltrušaitis, T., Ahuja, C., \& Morency, L.P. (2019).
	\newblock Multimodal machine learning: A survey and taxonomy.
	\newblock \textit{IEEE Trans. Pattern Anal. Mach. Intell.}, 41(2), 423--443.

	\bibitem[Benas et al.(2020)]{Benas2020}
	Benas, N., et al. (2020).
	\newblock Evaluation of ERA5 cloud properties against space-based observations.
	\newblock \textit{Atmos. Chem. Phys.}, 20, 10799--10816.

	\bibitem[Boucher et al.(2013)]{Boucher2013}
	Boucher, O., et al. (2013).
	\newblock Clouds and aerosols. In \textit{Climate Change 2013: The Physical Science Basis}.
	\newblock Cambridge University Press.

	\bibitem[Breiman(1996)]{Breiman1996}
	Breiman, L. (1996).
	\newblock Bagging predictors.
	\newblock \textit{Mach. Learn.}, 24(2), 123--140.

	\bibitem[Chen \& Guestrin(2016)]{Chen2016}
	Chen, T., \& Guestrin, C. (2016).
	\newblock XGBoost: A scalable tree boosting system.
	\newblock \textit{Proc. KDD}, 785--794.

	\bibitem[Chen et al.(2019)]{Chen2019}
	Chen, T.M., et al. (2019).
	\newblock Outdoor air pollution: Ozone health effects.
	\newblock \textit{Am. J. Med. Sci.}, 357(3), 266--273.

	\bibitem[Chen et al.(2020)]{Chen2020}
	Chen, T., Kornblith, S., Norouzi, M., \& Hinton, G. (2020).
	\newblock A simple framework for contrastive learning of visual representations.
	\newblock \textit{Proc. ICML}, 1597--1607.

	\bibitem[Dietterich(2000)]{Dietterich2000}
	Dietterich, T.G. (2000).
	\newblock Ensemble methods in machine learning.
	\newblock \textit{Proc. Int. Workshop Multiple Classifier Systems}, 1--15.

	\bibitem[Dosovitskiy et al.(2020)]{Dosovitskiy2020}
	Dosovitskiy, A., et al. (2020).
	\newblock An image is worth 16x16 words: Transformers for image recognition at scale.
	\newblock \textit{Proc. ICLR}.

	\bibitem[Finn et al.(2017)]{Finn2017}
	Finn, C., Abbeel, P., \& Levine, S. (2017).
	\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
	\newblock \textit{Proc. ICML}, 1126--1135.

	\bibitem[Freund \& Schapire(1997)]{Freund1997}
	Freund, Y., \& Schapire, R.E. (1997).
	\newblock A decision-theoretic generalization of on-line learning.
	\newblock \textit{J. Comput. Syst. Sci.}, 55(1), 119--139.

	\bibitem[Ganin et al.(2016)]{Ganin2016}
	Ganin, Y., et al. (2016).
	\newblock Domain-adversarial training of neural networks.
	\newblock \textit{J. Mach. Learn. Res.}, 17(1), 2096--2030.

	\bibitem[Hahn \& Warren(1995)]{Hahn1995}
	Hahn, C.J., \& Warren, S.G. (1995).
	\newblock A gridded climatology of clouds over land and ocean.
	\newblock \textit{ORNL Tech. Rep.} NDP-026E.

	\bibitem[Hamill(2006)]{Hamill2006}
	Hamill, T.M. (2006).
	\newblock Ensemble-based atmospheric data assimilation.
	\newblock In \textit{Predictability of Weather and Climate}, 124--156.

	\bibitem[He et al.(2016)]{He2016}
	He, K., Zhang, X., Ren, S., \& Sun, J. (2016).
	\newblock Deep residual learning for image recognition.
	\newblock \textit{Proc. CVPR}, 770--778.

	\bibitem[Hersbach et al.(2020)]{Hersbach2020}
	Hersbach, H., et al. (2020).
	\newblock The ERA5 global reanalysis.
	\newblock \textit{Q. J. R. Meteorol. Soc.}, 146(730), 1999--2049.

	\bibitem[Hong et al.(2021)]{Hong2021}
	Hong, D., et al. (2021).
	\newblock More diverse means better: Multimodal deep learning meets remote sensing imagery classification.
	\newblock \textit{IEEE Trans. Geosci. Remote Sens.}, 59(5), 4340--4354.

	\bibitem[Jean et al.(2019)]{Jean2019}
	Jean, N., et al. (2019).
	\newblock Tile2Vec: Unsupervised representation learning for spatially distributed data.
	\newblock \textit{Proc. AAAI}, 33, 3967--3974.

	\bibitem[Ke et al.(2017)]{Ke2017}
	Ke, G., et al. (2017).
	\newblock LightGBM: A highly efficient gradient boosting decision tree.
	\newblock \textit{Proc. NeurIPS}, 3146--3154.

	\bibitem[Koenker \& Bassett(1978)]{Koenker1978}
	Koenker, R., \& Bassett, G. (1978).
	\newblock Regression quantiles.
	\newblock \textit{Econometrica}, 46(1), 33--50.

	\bibitem[Krizhevsky et al.(2012)]{Krizhevsky2012}
	Krizhevsky, A., Sutskever, I., \& Hinton, G.E. (2012).
	\newblock ImageNet classification with deep convolutional neural networks.
	\newblock \textit{Proc. NeurIPS}, 1097--1105.

	\bibitem[Lawrence(2005)]{Lawrence2005}
	Lawrence, M.G. (2005).
	\newblock The relationship between relative humidity and the dewpoint temperature in moist air: A simple conversion and applications.
	\newblock \textit{Bull. Am. Meteorol. Soc.}, 86(2), 225--233.

	\bibitem[Lei et al.(2018)]{Lei2018}
	Lei, J., G'Sell, M., Rinaldo, A., Tibshirani, R.J., \& Wasserman, L. (2018).
	\newblock Distribution-free predictive inference for regression.
	\newblock \textit{J. Am. Stat. Assoc.}, 113(523), 1094--1111.

	\bibitem[Lundberg \& Lee(2020)]{Lundberg2020}
	Lundberg, S.M., \& Lee, S.I. (2017).
	\newblock A unified approach to interpreting model predictions.
	\newblock \textit{Proc. NeurIPS}, 4765--4774.

	\bibitem[Mace et al.(2007)]{Mace2007}
	Mace, G.G., et al. (2007).
	\newblock A description of hydrometeor layer occurrence statistics derived from CloudSat.
	\newblock \textit{J. Geophys. Res.}, 112, D09210.

	\bibitem[Martucci et al.(2010)]{Martucci2010}
	Martucci, G., Milroy, C., \& O'Dowd, C.D. (2010).
	\newblock Detection of cloud-base height using Jenoptik CHM15K ceilometer.
	\newblock \textit{J. Atmos. Ocean. Technol.}, 27(2), 305--318.

	\bibitem[Matsuoka et al.(2018)]{Matsuoka2018}
	Matsuoka, D., et al. (2018).
	\newblock Deep learning approach for detecting tropical cyclones.
	\newblock \textit{Geophys. Res. Lett.}, 45(18), 9910--9918.

	\bibitem[McGill et al.(2002)]{McGill2002}
	McGill, M., et al. (2002).
	\newblock Airborne validation of spatial properties measured by the GLAS lidar.
	\newblock \textit{J. Geophys. Res.}, 107(D13), 4283.

	\bibitem[Minnis et al.(2008)]{Minnis2008}
	Minnis, P., et al. (2008).
	\newblock Cloud detection in nonpolar regions for CERES using TRMM VIRS and MODIS.
	\newblock \textit{IEEE Trans. Geosci. Remote Sens.}, 46(11), 3857--3884.

	\bibitem[Neumann et al.(2019)]{Neumann2019}
	Neumann, M., et al. (2019).
	\newblock In-domain representation learning for remote sensing.
	\newblock \textit{arXiv preprint arXiv:1911.06721}.

	\bibitem[Ngiam et al.(2011)]{Ngiam2011}
	Ngiam, J., et al. (2011).
	\newblock Multimodal deep learning.
	\newblock \textit{Proc. ICML}, 689--696.

	\bibitem[Pan \& Yang(2010)]{Pan2010}
	Pan, S.J., \& Yang, Q. (2010).
	\newblock A survey on transfer learning.
	\newblock \textit{IEEE Trans. Knowl. Data Eng.}, 22(10), 1345--1359.

	\bibitem[Ramanathan et al.(1989)]{Ramanathan1989}
	Ramanathan, V., et al. (1989).
	\newblock Cloud-radiative forcing and climate.
	\newblock \textit{Science}, 243(4887), 57--63.

	\bibitem[Rasp \& Lerch(2020)]{Rasp2020}
	Rasp, S., \& Lerch, S. (2018).
	\newblock Neural networks for post-processing ensemble weather forecasts.
	\newblock \textit{Mon. Weather Rev.}, 146(11), 3885--3900.

	\bibitem[Shafer \& Vovk(2008)]{Shafer2008}
	Shafer, G., \& Vovk, V. (2008).
	\newblock A tutorial on conformal prediction.
	\newblock \textit{J. Mach. Learn. Res.}, 9, 371--421.

	\bibitem[Shimodaira(2000)]{Shimodaira2000}
	Shimodaira, H. (2000).
	\newblock Improving predictive inference under covariate shift.
	\newblock \textit{J. Stat. Plan. Inference}, 90(2), 227--244.

	\bibitem[Simonyan et al.(2014)]{Simonyan2014}
	Simonyan, K., Vedaldi, A., \& Zisserman, A. (2014).
	\newblock Deep inside convolutional networks: Visualising image classification models.
	\newblock \textit{Proc. ICLR Workshop}.

	\bibitem[Snell et al.(2017)]{Snell2017}
	Snell, J., Swersky, K., \& Zemel, R. (2017).
	\newblock Prototypical networks for few-shot learning.
	\newblock \textit{Proc. NeurIPS}, 4077--4087.

	\bibitem[Stephens(2002)]{Stephens2002}
	Stephens, G.L., et al. (2002).
	\newblock The CloudSat mission and the A-Train.
	\newblock \textit{Bull. Am. Meteorol. Soc.}, 83(12), 1771--1790.

	\bibitem[Stephens(2012)]{Stephens2012}
	Stephens, G.L., et al. (2012).
	\newblock An update on Earth's energy balance in light of CloudSat observations.
	\newblock \textit{Nat. Geosci.}, 5(10), 691--696.

	\bibitem[Stubenrauch et al.(2021)]{Stubenrauch2021}
	Stubenrauch, C.J., et al. (2021).
	\newblock Reanalysis cloud property retrievals.
	\newblock \textit{J. Geophys. Res. Atmos.}, 126, e2020JD033717.

	\bibitem[Tan \& Le(2019)]{Tan2019}
	Tan, M., \& Le, Q. (2019).
	\newblock EfficientNet: Rethinking model scaling for convolutional neural networks.
	\newblock \textit{Proc. ICML}, 6105--6114.

	\bibitem[Tuia et al.(2016)]{Tuia2016}
	Tuia, D., et al. (2016).
	\newblock Domain adaptation for the classification of remote sensing data.
	\newblock \textit{IEEE Geosci. Remote Sens. Mag.}, 4(2), 7--28.

	\bibitem[Vaswani et al.(2017)]{Vaswani2017}
	Vaswani, A., et al. (2017).
	\newblock Attention is all you need.
	\newblock \textit{Proc. NeurIPS}, 5998--6008.

	\bibitem[Wang et al.(2020)]{Wang2020}
	Wang, Y., et al. (2020).
	\newblock Generalizing from a few examples: A survey on few-shot learning.
	\newblock \textit{ACM Comput. Surv.}, 53(3), 1--34.

	\bibitem[Winker et al.(2010)]{Winker2010}
	Winker, D.M., et al. (2010).
	\newblock The CALIPSO mission.
	\newblock \textit{Bull. Am. Meteorol. Soc.}, 91(9), 1211--1230.

	\bibitem[WMO(2018)]{WMO2018}
	World Meteorological Organization (2018).
	\newblock \textit{Guide to Instruments and Methods of Observation}.
	\newblock WMO-No. 8, Geneva.

	\bibitem[Wolpert(1992)]{Wolpert1992}
	Wolpert, D.H. (1992).
	\newblock Stacked generalization.
	\newblock \textit{Neural Netw.}, 5(2), 241--259.

	\bibitem[Yuan et al.(2020)]{Yuan2020}
	Yuan, Q., et al. (2020).
	\newblock Deep learning in environmental remote sensing.
	\newblock \textit{Int. J. Remote Sens.}, 41(11), 4377--4416.

	\bibitem[Zantedeschi et al.(2019)]{Zantedeschi2019}
	Zantedeschi, V., et al. (2019).
	\newblock Cumulo: A dataset for learning cloud classes.
	\newblock \textit{Proc. ICML Workshop Climate Change AI}.

	\bibitem[Zhu et al.(2017)]{Zhu2017}
	Zhu, X.X., et al. (2017).
	\newblock Deep learning in remote sensing: A comprehensive review.
	\newblock \textit{IEEE Geosci. Remote Sens. Mag.}, 5(4), 8--36.

\end{thebibliography}

\end{document}
