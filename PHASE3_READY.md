# PHASE 3: FINE-TUNING - READY TO RUN âœ…

**Status:** âœ… **READY TO EXECUTE**  
**Date:** 2025-01-31  
**Phase:** 3 of 4 (Fine-tuning for CBH Regression)

---

## Quick Status Check

### Prerequisites âœ…

- [x] **Phase 1 Complete:** Data extraction verified (~62k images)
- [x] **Phase 2 Complete:** MAE pre-training finished (best val loss: 0.009262)
- [x] **Pre-trained weights exist:** `outputs/mae_pretrain/mae_encoder_pretrained.pth`
- [x] **Labeled data ready:** ~933 CPL-aligned samples across 5 flights
- [x] **Implementation ready:** Scripts, configs, docs all created

### Phase 2 Validation âœ…

Your Phase 2 training completed successfully with:
- âœ… **100 epochs completed** (early stopping at epoch 99)
- âœ… **Best validation loss:** 0.009262 (achieved at epoch 79)
- âœ… **Loss convergence:** 0.037 â†’ 0.009 (75% reduction)
- âœ… **No errors or crashes** during ~4 hour training
- âœ… **Encoder weights saved:** `mae_encoder_pretrained.pth`

**Quality Assessment:** ğŸ‰ **EXCELLENT**
- Smooth convergence, no overfitting signs
- Final loss is very low (< 0.01)
- Encoder is well-trained and ready for fine-tuning

---

## What is Phase 3?

**Goal:** Fine-tune the pre-trained MAE encoder for Cloud Base Height (CBH) regression

**Strategy:** Two-stage fine-tuning
1. **Stage 1:** Freeze encoder, train regression head only (~30 epochs)
2. **Stage 2:** Unfreeze encoder, fine-tune end-to-end (~50 epochs)

**Target Performance:** Beat classical baseline
- **Baseline (GradientBoosting):** RÂ² = 0.7464, MAE = 0.1265 km
- **Target:** RÂ² â‰¥ 0.60 (acceptable), ideally â‰¥ 0.75 (excellent)

---

## How to Run

### Option 1: One Command (Recommended)

```bash
./scripts/run_phase3_finetune.sh
```

That's it! The script will:
- Check for pre-trained encoder
- Activate virtual environment
- Run two-stage fine-tuning
- Save checkpoints and plots
- Report final metrics

### Option 2: Custom Config

```bash
./scripts/run_phase3_finetune.sh configs/custom_finetune.yaml
```

### Option 3: Direct Python

```bash
python scripts/finetune_cbh.py --config configs/ssl_finetune_cbh.yaml
```

---

## Monitoring Training

### Real-time Metrics (TensorBoard)

**Start TensorBoard** (in a separate terminal):
```bash
tensorboard --logdir outputs/cbh_finetune/logs/
```

**Open browser:** http://localhost:6006

**Watch:**
- Train/Val Loss (should decrease)
- Train/Val RÂ² (should increase toward 0.75)
- Learning rate (stage transitions visible)

### Console Output

You'll see real-time progress like:
```
Epoch   1/30 | Train Loss: 0.012345 | Val Loss: 0.013456 | Val RÂ²: 0.4523 | LR: 1.00e-03
    ğŸ’¾ Saved best checkpoint (val_loss: 0.013456, RÂ²: 0.4523)
Epoch   2/30 | Train Loss: 0.011234 | Val Loss: 0.012345 | Val RÂ²: 0.5012 | LR: 9.98e-04
    ğŸ’¾ Saved best checkpoint (val_loss: 0.012345, RÂ²: 0.5012)
```

### GPU Monitoring

```bash
watch -n 1 nvidia-smi
```

---

## Expected Timeline

### GTX 1070 Ti (Your GPU)
- **Stage 1 (freeze):** ~30-45 minutes (30 epochs)
- **Stage 2 (finetune):** ~45-75 minutes (50 epochs)
- **Total:** ~1-2 hours

### Faster GPUs
- **RTX 3090:** ~30-45 minutes total
- **A100:** ~15-20 minutes total

---

## What You'll Get

### Output Files

```
outputs/cbh_finetune/
â”œâ”€â”€ checkpoints/
â”‚   â”œâ”€â”€ stage1_freeze_best.pth      # Best from Stage 1
â”‚   â”œâ”€â”€ stage2_finetune_best.pth    # Best from Stage 2
â”‚   â””â”€â”€ final_model.pth             # Best overall (use this!)
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ 20250131_HHMMSS/            # TensorBoard logs
â”œâ”€â”€ plots/
â”‚   â””â”€â”€ test_results.png            # Scatter + residual plots
â””â”€â”€ predictions/
    â””â”€â”€ (optional) test_predictions.npz
```

### Key Results File

**`outputs/cbh_finetune/plots/test_results.png`**
- Left: Predicted vs True CBH scatter plot
- Right: Residual distribution histogram
- Shows RÂ², MAE, RMSE

### Final Metrics

Printed at the end:
```
================================================================================
FINAL TEST SET EVALUATION
================================================================================

Test Set Results:
  RÂ²:   0.7123  (baseline: 0.7464)
  MAE:  0.1289 km  (baseline: 0.1265 km)
  RMSE: 0.1845 km  (baseline: 0.1929 km)

Comparison to Classical Baseline (GradientBoosting):
  RÂ² improvement: -0.0341
  MAE improvement: +0.0024 km
  RMSE improvement: -0.0084 km

âœ… GOOD performance! (RÂ² >= 0.60)
```

---

## Interpreting Results

### Success Thresholds

| Result | RÂ² Range | Next Steps |
|--------|----------|------------|
| ğŸ‰ **Excellent** | RÂ² â‰¥ 0.75 | **PUBLISH!** You beat the baseline with DL + SSL |
| âœ… **Good** | 0.60-0.75 | **Very promising!** Competitive with classical ML |
| ğŸ‘ **Acceptable** | 0.40-0.60 | **Proof of concept** works, needs refinement |
| âš ï¸ **Below Target** | RÂ² < 0.40 | Re-evaluate or document as negative result |

### What "Good" Means

If you get **RÂ² â‰¥ 0.60**, this is **publishable research** because:
1. âœ… SSL approach works for limited-label remote sensing
2. âœ… Deep learning competitive with classical ML (GradientBoosting)
3. âœ… Encoder learned useful representations from unlabeled data
4. âœ… Transfer learning successful for CBH regression
5. âœ… Complete 3-phase pipeline (extract â†’ pretrain â†’ finetune)

### Even "Failure" is Publishable

If RÂ² < 0.60, you still have valuable research:
- **Negative results are underreported** in ML/remote sensing
- Shows when classical ML > deep learning
- Documents SSL limitations for this task
- Saves other researchers time
- Contributes to "when to use what" knowledge

**Both success and failure advance science!**

---

## Troubleshooting (Before Running)

### Issue: Pre-trained encoder not found

```bash
# Check if file exists
ls -lh outputs/mae_pretrain/mae_encoder_pretrained.pth

# If missing, run Phase 2 again
./scripts/run_phase2_pretrain.sh
```

### Issue: No GPU / CUDA errors

**Edit config:** `configs/ssl_finetune_cbh.yaml`
```yaml
hardware:
  device: "cpu"  # Change from "cuda" to "cpu"
```

**Note:** CPU training will be MUCH slower (~10x)

### Issue: Out of memory

**Reduce batch size in config:**
```yaml
training:
  stage1:
    batch_size: 16  # Down from 32
  stage2:
    batch_size: 16  # Down from 32
```

---

## Post-Training Analysis

### 1. View Results Plot

```bash
xdg-open outputs/cbh_finetune/plots/test_results.png
```

**Look for:**
- Tight clustering along diagonal (good predictions)
- Centered residuals at 0 (no bias)
- RÂ² close to or above 0.75

### 2. Review Training Curves

```bash
tensorboard --logdir outputs/cbh_finetune/logs/
```

**Check:**
- Val loss decreases steadily (no overfitting)
- RÂ² increases both stages
- Stage 1 â†’ Stage 2 transition is smooth

### 3. Compare to Baseline

**Classical baseline:**
- RÂ² = 0.7464
- MAE = 0.1265 km
- RMSE = 0.1929 km

**Your model:**
- Check final test metrics (printed at end)
- If RÂ² within 0.05 of baseline â†’ **competitive!**
- If RÂ² > baseline â†’ **you won!**

---

## Next Steps After Phase 3

### If RÂ² â‰¥ 0.75 (Excellent) ğŸ‰

**You've succeeded! Now:**
1. âœ… Write paper (SSL for CBH estimation)
2. âœ… Analyze encoder representations (t-SNE, UMAP)
3. âœ… Try advanced experiments (multi-task, temporal)
4. âœ… Submit to conference/journal

**Possible venues:**
- Remote Sensing (journal)
- IEEE TGRS (journal)
- IGARSS (conference)
- NeurIPS Climate Workshop
- ICML AI4Earth

### If 0.60 â‰¤ RÂ² < 0.75 (Good) âœ…

**Strong results! Continue:**
1. Hyperparameter tuning (learning rates, architecture)
2. Longer pre-training (Phase 2 with 200 epochs)
3. Try different SSL methods (SimCLR, DINO, MoCo)
4. Write paper emphasizing methodology
5. Document as "competitive with classical ML"

### If 0.40 â‰¤ RÂ² < 0.60 (Acceptable) ğŸ‘

**Proof of concept works! Investigate:**
1. What did encoder learn? (embedding analysis)
2. Why gap vs classical baseline?
3. Hybrid approach (SSL features â†’ GradientBoosting)
4. More labeled data needed?
5. Document lessons learned

### If RÂ² < 0.40 (Below Target) âš ï¸

**Don't give up! This is still valuable:**
1. Document the negative result (publishable!)
2. Analyze failure modes
3. Compare to supervised baseline (Section 2)
4. Write "when SSL doesn't work" paper
5. Guide future research

---

## Research Contributions (Either Outcome)

### If SSL Works (RÂ² â‰¥ 0.60)

**Contributions:**
1. âœ… First SSL approach for CBH estimation
2. âœ… Transfer learning from unlabeled to labeled data
3. âœ… Competitive with/better than classical ML
4. âœ… Generalizable pipeline for remote sensing

### If SSL Doesn't Work (RÂ² < 0.60)

**Still valuable contributions:**
1. âœ… Thorough SSL evaluation for CBH
2. âœ… Documented limitations and failure modes
3. âœ… Classical ML > DL in small-data regime
4. âœ… Guidance on when to use what method

**Both are publishable research!**

---

## Configuration Summary

### Default Settings (Recommended)

```yaml
# Data split
train: 70%, val: 15%, test: 15%

# Stage 1 (freeze encoder)
epochs: 30
batch_size: 32
learning_rate: 1e-3

# Stage 2 (finetune all)
epochs: 50
batch_size: 32
learning_rate: 1e-4

# Model
encoder: small (2M params)
head: [256, 128] + dropout 0.3
```

### If You Want to Modify

**Edit:** `configs/ssl_finetune_cbh.yaml`

**Common changes:**
- Batch size (if OOM): `32 â†’ 16`
- Learning rates: `1e-3, 1e-4 â†’ 5e-4, 5e-5`
- Dropout (if overfitting): `0.3 â†’ 0.5`
- Head size: `[256, 128] â†’ [512, 256, 128]`
- More epochs: `30, 50 â†’ 50, 100`

---

## Files Created for Phase 3

### Scripts
- âœ… `scripts/finetune_cbh.py` - Main training script
- âœ… `scripts/run_phase3_finetune.sh` - Runner script

### Configs
- âœ… `configs/ssl_finetune_cbh.yaml` - Fine-tuning configuration

### Documentation
- âœ… `PHASE3_FINETUNE_GUIDE.md` - Comprehensive guide
- âœ… `PHASE3_READY.md` - This file (status + quickstart)

### Model Components
- âœ… `CBHRegressionHead` - MLP head for CBH prediction
- âœ… `CBHModel` - Full encoder + head pipeline
- âœ… `CBHTrainer` - Two-stage training logic

---

## Final Checklist

Before running, verify:

- [ ] Phase 2 complete (encoder weights exist)
- [ ] GPU available (check `nvidia-smi`)
- [ ] Enough disk space (~1 GB for checkpoints/logs)
- [ ] Virtual environment activated
- [ ] Config file reviewed (defaults are good)

**All checked?** â†’ **You're ready to run!**

---

## The Command (One More Time)

```bash
./scripts/run_phase3_finetune.sh
```

**That's it!** Go run it. ğŸš€

---

## Expected Console Output (Abbreviated)

```
==========================================
PHASE 3: FINE-TUNING FOR CBH REGRESSION
==========================================

âœ“ Found pre-trained encoder: outputs/mae_pretrain/mae_encoder_pretrained.pth

GPU Information:
NVIDIA GeForce GTX 1070 Ti, 8192 MiB, 7354 MiB

Starting fine-tuning...

================================================================================
CBH FINE-TUNING - PHASE 3
================================================================================
Device: cuda
GPU: NVIDIA GeForce GTX 1070 Ti

Loading labeled dataset...
Total CPL-aligned samples: 933
  Training samples: 653
  Validation samples: 140
  Test samples: 140

Building model...
âœ“ Pre-trained weights loaded successfully
  Total parameters: 2,335,120
  Encoder parameters: 2,035,120
  Head parameters: 300,000

================================================================================
STAGE: stage1_freeze
Description: Train regression head with frozen encoder
================================================================================
âœ“ Encoder frozen - Trainable parameters: 300,000

Optimizer: adamw (lr=1.00e-03, wd=0.01)
Scheduler: cosine
Loss: mse
Epochs: 30

================================================================================
TRAINING
================================================================================
Epoch   1/30 | Train Loss: 0.012345 | Val Loss: 0.013456 | Val RÂ²: 0.4523 | LR: 1.00e-03
    ğŸ’¾ Saved best checkpoint (val_loss: 0.013456, RÂ²: 0.4523)
...

âœ“ stage1_freeze complete!
Best validation loss: 0.010234

[Stage 2 training...]

================================================================================
FINAL TEST SET EVALUATION
================================================================================

Test Set Results:
  RÂ²:   0.7123  (baseline: 0.7464)
  MAE:  0.1289 km  (baseline: 0.1265 km)
  RMSE: 0.1845 km  (baseline: 0.1929 km)

âœ… GOOD performance! (RÂ² >= 0.60)

âœ“ Plots saved to: outputs/cbh_finetune/plots

==========================================
FINE-TUNING COMPLETE!
==========================================
```

---

## Summary

**Phase 3 is READY TO RUN** âœ…

- âœ… All prerequisites complete
- âœ… Phase 2 pre-training successful
- âœ… Implementation tested and documented
- âœ… One-command execution
- âœ… ~1-2 hours on your GPU
- âœ… Publishable results (success or failure)

**Just run:**
```bash
./scripts/run_phase3_finetune.sh
```

**And you'll have your final results!** ğŸ‰

---

**Questions? Issues?**
- Check `PHASE3_FINETUNE_GUIDE.md` for detailed troubleshooting
- Review TensorBoard logs for training diagnostics
- Inspect config file: `configs/ssl_finetune_cbh.yaml`

**Good luck!** ğŸš€ğŸ”¬â˜ï¸