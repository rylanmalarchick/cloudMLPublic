\documentclass[11pt,letterpaper]{article}

% Packages
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{fancyhdr}

% Header/Footer
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{Cloud Base Height Retrieval: Sprints 3-5}
\fancyhead[R]{November 2025}
\fancyfoot[C]{\thepage}

% Title
\title{\textbf{Cloud Base Height Retrieval:\\
Sprints 3 to 5 Status Report}}
\author{Rylan Malarchick}
\date{November 2025}

\begin{document}

\maketitle

\begin{abstract}
This report documents the completion of Feature Engineering \& Integration, along with Hybrid Model Development, as well as Part 5 (Advanced Deep Learning) for the Cloud Base Height (CBH) retrieval project. It presents a comprehensive evaluation spanning classical machine learning baselines, hybrid CNN architectures, and state-of-the-art deep learning models with temporal modeling and advanced fusion techniques. The key finding is that \textbf{temporal Vision Transformer (ViT) models with physics-informed consistency losses are the first deep learning architectures to outperform the physical baseline}, achieving $R^2 = 0.728$ with 126-meter mean absolute error. All results reported use real operational data from NASA ER-2 flights with validated ERA5 atmospheric reanalysis.
\end{abstract}

\tableofcontents
\newpage

\section{Summary}

\subsection{Overview}

\begin{itemize}
    \item \textbf{Section 3}: Integration of geometric features (WP1) with atmospheric features (WP2), establishing physical baseline and validation framework
    \item \textbf{Section 4}: Development and evaluation of hybrid deep learning models combining image features with physical constraints using attention-based fusion
    \item \textbf{Section 5}: Advanced deep learning with pre-trained backbones, temporal modeling, physics-informed losses, and multimodal fusion (FiLM)
\end{itemize}

\subsection{Key Results Summary}

Performance Evolution Across Sections:

\begin{table}[h]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{Section} & \textbf{$R^2$} & \textbf{MAE (m)} \\
\midrule
Physical GBDT (baseline) & 3 & 0.668 & 137 \\
Image-only CNN & 4 & 0.279 & 233 \\
Attention Fusion CNN & 4 & 0.326 & 221 \\
ResNet-50 Baseline & 5 & 0.524 & 171 \\
ViT-Tiny Baseline & 5 & 0.577 & 166 \\
Temporal ViT & 5 & 0.727 & 126 \\
Temporal ViT + Consistency ($\lambda=0.1$) & 5 & \textbf{0.728} & \textbf{126} \\
FiLM Fusion & 5 & 0.542 & 166 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Breakthrough Achievement}

\textbf{First Deep Learning Model to Beat Physical Baseline:}

The Temporal ViT model with physics-informed temporal consistency loss represents a breakthrough:

\begin{itemize}
    \item \textbf{Performance}: $R^2 = 0.728$ (vs. 0.668 baseline) = +9\% improvement
    \item \textbf{Accuracy}: MAE = 126 m (vs. 137 m baseline) = 11 m improvement (8\%)
    \item \textbf{Architecture}: ViT-Tiny encoder with 5-frame temporal attention
    \item \textbf{Innovation}: Physics-informed temporal consistency regularization
\end{itemize}

\subsection{Critical Findings}

\begin{enumerate}
    \item \textbf{Temporal information is essential}: Single-frame models ($R^2 \approx 0.28$--0.58) cannot compete with temporal models ($R^2 = 0.73$)
    \item \textbf{Pre-trained backbones improve over CNNs from scratch}: ViT-Tiny (ImageNet-21k pre-trained) achieves $R^2 = 0.577$ vs. custom CNN $R^2 = 0.279$
    \item \textbf{Attention mechanisms outperform CNNs}: Vision Transformers systematically outperform ResNet architectures
    \item \textbf{Physics-informed losses provide marginal gains}: Temporal consistency loss ($\lambda = 0.1$) improves $R^2$ from 0.727 to 0.728
    \item \textbf{ERA5 fusion remains challenging}: FiLM fusion ($R^2 = 0.542$) underperforms image-only ViT, suggesting atmospheric features require better integration strategies
\end{enumerate}

\newpage

\section{Section 3: Feature Engineering \& Integration}

\subsection{Part 1: Geometric Features}

\subsubsection{Shadow-Based CBH Derivation}

Building on solar geometry analysis, I implemented shadow-length-based cloud base height estimation following the physical principle:

\begin{equation}
H_{\text{cloud}} = L_{\text{shadow}} \cdot \tan(\theta_{\text{SZA}})
\end{equation}

where $L_{\text{shadow}}$ is the detected shadow length (meters) and $\theta_{\text{SZA}}$ is the solar zenith angle.

\textbf{Implementation:}
\begin{itemize}
    \item Edge detection on $440 \times 640$ grayscale images using Canny algorithm
    \item Shadow region identification via brightness thresholding (adaptive percentile-based)
    \item Length measurement in pixel coordinates with camera geometry correction
    \item Conversion to physical units using aircraft altitude and viewing angle
    \item Confidence scoring based on edge sharpness and detection consistency
\end{itemize}

\textbf{Feature set (10 features):}
\begin{enumerate}
    \item \texttt{derived\_geometric\_H}: Shadow-derived cloud base height estimate
    \item \texttt{shadow\_length\_pixels}: Raw detected shadow length
    \item \texttt{shadow\_detection\_confidence}: Quality score [0, 1]
    \item \texttt{sza\_rad}: Solar zenith angle (radians)
    \item \texttt{saa\_rad}: Solar azimuth angle (radians)
    \item \texttt{cloud\_top\_edge\_y}: Vertical position of cloud top edge
    \item \texttt{cloud\_bottom\_edge\_y}: Vertical position of cloud bottom edge
    \item \texttt{edge\_sharpness}: Mean gradient magnitude along edges
    \item \texttt{altitude\_m}: Aircraft altitude (from GPS)
    \item \texttt{geometric\_consistency}: Multi-frame consistency metric
\end{enumerate}

\textbf{Data quality:}
\begin{itemize}
    \item 87.1\% of samples have valid shadow detections (confidence $> 0.5$)
    \item 12.9\% missing values handled via median imputation
    \item Failures occur in: (1) optically thin cirrus, (2) broken cloud fields, (3) SZA $> 70°$ (this is a marginal part of our dataset so not a huge concern - and also does not really correspond to any of our CPL picks)
\end{itemize}

\subsection{Part 2: Atmospheric Features}

\subsubsection{ERA5 Reanalysis Integration}

\textbf{Data Source:}

ERA5 atmospheric reanalysis data was processed from NASA archives:

\begin{itemize}
    \item \textbf{Surface-level data}: 119 daily files (Oct 23, 2024 -- Feb 19, 2025)
    \item \textbf{Pressure-level data}: 37 vertical levels, hourly temporal resolution
    \item \textbf{Spatial resolution}: $0.25° \times 0.25°$ (approximately 25 km)
    \item \textbf{Coverage}: All 5 flight dates fully covered
    \item \textbf{Processing success}: 933/933 samples (100\%)
\end{itemize}

\textbf{Spatiotemporal Matching:}
\begin{itemize}
    \item \textbf{Spatial}: Nearest neighbor interpolation to flight track GPS coordinates
    \item \textbf{Temporal}: Nearest hourly ERA5 timestamp to image acquisition time
    \item \textbf{Vertical}: Boundary layer features extracted from surface and pressure levels
\end{itemize}

\textbf{Derived atmospheric features (9 features):}
\begin{enumerate}
    \item \texttt{blh\_m}: Boundary layer height (from ERA5 BLH field)
    \item \texttt{lcl\_m}: Lifting condensation level (computed from T, Td)
    \item \texttt{inversion\_height\_m}: Temperature inversion base height
    \item \texttt{moisture\_gradient}: Vertical moisture gradient (kg/kg/m)
    \item \texttt{stability\_index}: Atmospheric stability (lapse rate, K/km)
    \item \texttt{surface\_temp\_k}: 2-meter temperature
    \item \texttt{surface\_dewpoint\_k}: 2-meter dewpoint temperature
    \item \texttt{surface\_pressure\_pa}: Surface pressure
    \item \texttt{profile\_confidence}: ERA5 data quality indicator
\end{enumerate}

\textbf{Feature Statistics (Real ERA5):}

\begin{table}[h]
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Variable} & \textbf{Mean} & \textbf{Std Dev} \\
\midrule
Boundary Layer Height & 658 m & 485 m \\
Lifting Condensation Level & 839 m & 589 m \\
Inversion Height & 875 m & 688 m \\
Moisture Gradient & $-1.07 \times 10^{-6}$ kg/kg/m & -- \\
Stability Index & 3.81 K/km & 0.93 K/km \\
Surface Temperature & 284.4 K & 9.1 K \\
Surface Dewpoint & 277.7 K & 9.5 K \\
Surface Pressure & 96,928 Pa & 7,535 Pa \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Physical validation:}
\begin{itemize}
    \item BLH values ($658 \pm 485$ m) consistent with marine boundary layer
    \item Stability index (3.81 K/km) indicates stable atmosphere (less than standard 6.5 K/km)
    \item LCL correlates with observed low cloud base heights (mean CBH = 830 m)
    \item All values physically realistic for coastal/oceanic flight conditions
\end{itemize}

\subsection{Part 3: Physical Baseline Model}

\subsubsection{Model Architecture}

\textbf{Algorithm:} XGBoost Gradient Boosted Decision Trees (GBDT)

\textbf{Input features:} 19 total
\begin{itemize}
    \item 10 geometric features (WP1)
    \item 9 atmospheric features (WP2)
\end{itemize}

\textbf{Hyperparameters:}
\begin{itemize}
    \item \texttt{max\_depth}: 6 (prevents overfitting on small dataset)
    \item \texttt{learning\_rate}: 0.1 (conservative step size)
    \item \texttt{n\_estimators}: 100 (ensemble size)
    \item \texttt{subsample}: 0.8 (row sampling for regularization)
    \item \texttt{colsample\_bytree}: 0.8 (column sampling)
    \item \texttt{min\_child\_weight}: 3 (minimum samples per leaf)
    \item \texttt{gamma}: 0.1 (minimum loss reduction for split)
    \item \texttt{reg\_alpha}: 0.01 (L1 regularization)
    \item \texttt{reg\_lambda}: 1.0 (L2 regularization)
\end{itemize}

\subsubsection{Validation Protocol}

\textbf{Method:} Stratified 5-Fold Cross-Validation

\textbf{Rationale:}
\begin{itemize}
    \item Ensures balanced CBH distribution across folds (10 quantile bins)
    \item Prevents extreme domain shift (Flight F4 has mean CBH = 0.697 km vs. 0.917 km for F0)
    \item Provides stable performance estimates for hyperparameter tuning
    \item Leave-One-Flight-Out CV explicitly avoided after catastrophic failure ($R^2 = -3.13$ on F4)
\end{itemize}

\subsubsection{Results}

\textbf{Aggregate Performance:}
\begin{itemize}
    \item Mean $R^2$: $0.6759 \pm 0.0442$
    \item Mean MAE: $0.1356 \pm 0.0068$ km (136 meters)
    \item Mean RMSE: $0.2105 \pm 0.0123$ km (211 meters)
\end{itemize}

\textbf{Per-Fold Results:}

\begin{table}[h]
\centering
\begin{tabular}{cccc}
\toprule
\textbf{Fold} & \textbf{$N_{\text{test}}$} & \textbf{$R^2$} & \textbf{MAE (km)} \\
\midrule
0 & 187 & 0.629 & 0.139 \\
1 & 187 & 0.753 & 0.126 \\
2 & 187 & 0.663 & 0.144 \\
3 & 186 & 0.641 & 0.140 \\
4 & 186 & 0.693 & 0.129 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Interpretation:}
\begin{itemize}
    \item Low cross-fold variance indicates robust generalization
    \item Best fold (Fold 1): $R^2 = 0.753$ suggests upper performance bound
    \item Worst fold (Fold 0): $R^2 = 0.629$ still exceeds earlier hybrid models
    \item This model establishes the baseline to beat: $R^2 = 0.668$
\end{itemize}

\subsection{Integrated Feature Store}

\textbf{File:} \texttt{sow\_outputs/integrated\_features/Integrated\_Features.hdf5}

\textbf{Structure:}
\begin{verbatim}
Integrated_Features.hdf5
|-- geometric_features/ [933 x 10]
|   |-- derived_geometric_H
|   |-- shadow_length_pixels
|   +-- ... (8 more)
|-- atmospheric_features/ [933 x 9]
|   |-- blh_m
|   |-- lcl_m
|   +-- ... (7 more)
|-- metadata/
|   |-- sample_id [933]
|   |-- flight_id [933]
|   |-- cbh_km (target) [933]
|   |-- latitude [933]
|   |-- longitude [933]
|   +-- timestamp [933]
+-- image_features/ [reserved for CNN embeddings]
\end{verbatim}

\textbf{Dataset Statistics:}
\begin{itemize}
    \item \textbf{Total samples}: 933 (post-filtering for quality)
    \item \textbf{CBH range}: [0.120, 1.950] km
    \item \textbf{CBH mean}: $0.830 \pm 0.371$ km
    \item \textbf{Flight distribution}:
    \begin{itemize}
        \item F0 (30Oct24): 501 samples (mean CBH = 0.917 km)
        \item F1 (10Feb25): 191 samples (mean CBH = 0.695 km)
        \item F2 (23Oct24): 105 samples (mean CBH = 0.503 km)
        \item F3 (12Feb25): 92 samples (mean CBH = 1.081 km)
        \item F4 (18Feb25): 44 samples (mean CBH = 0.697 km)
    \end{itemize}
\end{itemize}

\subsection{Feature Importance Analysis}

\textbf{Top 10 Features (GBDT Gain Importance):}

\begin{table}[h]
\centering
\begin{tabular}{clcc}
\toprule
\textbf{Rank} & \textbf{Feature} & \textbf{Type} & \textbf{Importance} \\
\midrule
1 & \texttt{blh\_m} & Atmospheric & 0.243 \\
2 & \texttt{derived\_geometric\_H} & Geometric & 0.187 \\
3 & \texttt{sza\_rad} & Geometric & 0.124 \\
4 & \texttt{lcl\_m} & Atmospheric & 0.098 \\
5 & \texttt{shadow\_length\_pixels} & Geometric & 0.076 \\
6 & \texttt{surface\_temp\_k} & Atmospheric & 0.061 \\
7 & \texttt{edge\_sharpness} & Geometric & 0.054 \\
8 & \texttt{stability\_index} & Atmospheric & 0.047 \\
9 & \texttt{cloud\_bottom\_edge\_y} & Geometric & 0.039 \\
10 & \texttt{inversion\_height\_m} & Atmospheric & 0.031 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key insights:}
\begin{itemize}
    \item BLH (boundary layer height) is the strongest predictor, validating atmospheric state importance
    \item Shadow-derived geometric height is second-most important
    \item Solar zenith angle critical (affects shadow geometry)
    \item Mix of geometric and atmospheric features in top 10 validates multi-modal approach
\end{itemize}

\newpage

\section{Section 4: Hybrid Model Development}

\subsection{Motivation}

Section 3 demonstrated that physical features (geometric + atmospheric) achieve strong performance ($R^2 = 0.676$). However, the question remained: \textit{Can deep learning on raw images match or exceed this performance?}

Section 4 addresses this by developing hybrid CNN architectures that combine:
\begin{itemize}
    \item \textbf{Image features}: Learned representations from $440 \times 640$ grayscale images
    \item \textbf{Physical features}: Hand-engineered geometric and atmospheric features
\end{itemize}

\subsection{Part 1: CNN Architecture Development}

\subsubsection{Model 1: Image-Only Baseline CNN}

\textbf{Architecture:}
\begin{itemize}
    \item \textbf{Input}: $1 \times 440 \times 640$ grayscale images
    \item \textbf{Encoder}: 4-stage 2D CNN
    \begin{itemize}
        \item Stage 1: Conv2d(1→64, k=3), BatchNorm, ReLU, MaxPool(2)
        \item Stage 2: Conv2d(64→128, k=3), BatchNorm, ReLU, MaxPool(2)
        \item Stage 3: Conv2d(128→256, k=3), BatchNorm, ReLU, MaxPool(2)
        \item Stage 4: Conv2d(256→256, k=3), BatchNorm, ReLU, AdaptiveAvgPool(1, 1)
    \end{itemize}
    \item \textbf{Embedding}: 256-dimensional image representation
    \item \textbf{Regressor}: Linear(256→128), ReLU, Dropout(0.3), Linear(128→1)
    \item \textbf{Output}: CBH prediction (scalar, km)
\end{itemize}

\textbf{Training:}
\begin{itemize}
    \item Optimizer: Adam (lr=1e-4)
    \item Loss: MSE
    \item Batch size: 16
    \item Early stopping: patience=10 epochs
    \item Data augmentation: Random horizontal flip, brightness jitter ($\pm 10\%$)
\end{itemize}

\textbf{Results:}
\begin{itemize}
    \item $R^2$: $0.279 \pm 0.031$
    \item MAE: $0.233 \pm 0.019$ km (233 meters)
    \item RMSE: $0.315 \pm 0.017$ km
\end{itemize}

\textbf{Analysis:}
\begin{itemize}
    \item Significantly underperforms physical baseline ($R^2$ gap = 0.40)
    \item CNN struggles to learn geometric relationships from scratch
    \item Limited training data (933 samples) insufficient for CNN generalization
    \item No pre-training or transfer learning applied
\end{itemize}

\subsubsection{Model 2: Concatenation Fusion}

\textbf{Architecture:}
\begin{itemize}
    \item Same CNN encoder as Model 1 (256-dim image embedding)
    \item Physical features: 19-dim vector (geometric + atmospheric)
    \item \textbf{Fusion}: Simple concatenation: $[\text{img\_emb}; \text{phys\_feat}] \rightarrow 275$-dim
    \item Regressor: Linear(275→128), ReLU, Dropout(0.3), Linear(128→1)
\end{itemize}

\textbf{Results:}
\begin{itemize}
    \item $R^2$: $0.180 \pm 0.028$
    \item MAE: $0.246 \pm 0.016$ km
    \item RMSE: $0.336 \pm 0.013$ km
\end{itemize}

\textbf{Critical finding:} Performance \textit{degraded} compared to image-only ($\Delta R^2 = -0.099$)!

\textbf{Interpretation:}
\begin{itemize}
    \item CNN features are noisy and interfere with physical features
    \item Naive concatenation cannot properly balance modalities
    \item Regression head overfits to noisy CNN features
    \item Demonstrates need for learned fusion mechanisms
\end{itemize}

\subsubsection{Model 3: Attention Fusion}

\textbf{Architecture:}
\begin{itemize}
    \item Same CNN encoder (256-dim image embedding)
    \item Physical features: 19-dim vector
    \item \textbf{Cross-attention mechanism}:
    \begin{itemize}
        \item Query: Linear(256→64) from image embedding
        \item Key/Value: Linear(19→64) from physical features
        \item Attention: $\alpha = \text{softmax}(QK^T / \sqrt{d_k})$
        \item Output: $\text{Attn}(Q, K, V) = \alpha V$
    \end{itemize}
    \item \textbf{Gated fusion}:
    \begin{itemize}
        \item Gate: $g = \sigma(\text{Linear}([\text{img}; \text{attn}]))$
        \item Fused: $f = g \odot \text{img} + (1 - g) \odot \text{attn}$
    \end{itemize}
    \item Regressor: Linear(256→128), ReLU, Dropout(0.3), Linear(128→1)
\end{itemize}

\textbf{Results:}
\begin{itemize}
    \item $R^2$: $0.326 \pm 0.047$
    \item MAE: $0.221 \pm 0.014$ km (221 meters)
    \item RMSE: $0.304 \pm 0.019$ km
\end{itemize}

\textbf{Analysis:}
\begin{itemize}
    \item Improves over concatenation by $\Delta R^2 = +0.146$ (81\% relative improvement)
    \item Attention learns to downweight noisy CNN features
    \item Still underperforms physical baseline by $\Delta R^2 = 0.35$
    \item Validates hypothesis that learned fusion helps, but CNN features remain weak
\end{itemize}

\subsection{Ablation Study}

\textbf{Comprehensive Model Comparison:}

\begin{table}[h]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{$R^2$} & \textbf{MAE (km)} & \textbf{RMSE (km)} \\
\midrule
Physical-only GBDT & 0.676 & 0.136 & 0.211 \\
Attention Fusion CNN & 0.326 & 0.221 & 0.304 \\
Image-only CNN & 0.279 & 0.233 & 0.315 \\
Concatenation Fusion CNN & 0.180 & 0.246 & 0.336 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key ablation insights:}
\begin{enumerate}
    \item \textbf{Physical vs. Image}: Physical features are $2.4\times$ stronger ($R^2$ gap = 0.40)
    \item \textbf{Image vs. Concat}: Adding physical features \textit{hurts} naive fusion ($\Delta R^2 = -0.10$)
    \item \textbf{Concat vs. Attention}: Attention recovers 81\% of lost performance ($\Delta R^2 = +0.15$)
    \item \textbf{Physical vs. Best Hybrid}: Physical still wins by $2.1\times$ ($\Delta R^2 = 0.35$)
\end{enumerate}

\textbf{Section 4 Conclusions:}
\begin{itemize}
    \item No CNN architecture beats the physical baseline
    \item Root causes identified:
    \begin{itemize}
        \item CNN trained from scratch on small dataset (933 samples)
        \item No pre-training or transfer learning
        \item Single-frame input (no temporal context)
        \item Simple architecture (4-layer CNN insufficient)
    \end{itemize}
    \item \textbf{Motivation for Part 5}: Pre-trained backbones + temporal modeling
\end{itemize}

\newpage

\section{Section 5: Advanced Deep Learning}

\subsection{Overview}

Part 5 addresses the limitations identified in Part 4 by introducing:

\begin{enumerate}
    \item \textbf{Pre-trained backbones}: Transfer learning from ImageNet
    \item \textbf{Modern architectures}: ResNet-50, Vision Transformers (ViT)
    \item \textbf{Temporal modeling}: Multi-frame sequences with attention
    \item \textbf{Physics-informed losses}: Temporal consistency regularization
    \item \textbf{Advanced fusion}: FiLM (Feature-wise Linear Modulation) for ERA5 integration
\end{enumerate}

\subsection{Part 1: Pre-Trained Backbones}

\subsubsection{Model: ResNet-50 Baseline}

\textbf{Architecture:}
\begin{itemize}
    \item \textbf{Backbone}: ResNet-50 pre-trained on ImageNet-1k (torchvision)
    \item \textbf{Input handling}: Grayscale images duplicated $3\times$ to RGB channels
    \item \textbf{Fine-tuning strategy}:
    \begin{itemize}
        \item Freeze: conv1, bn1, layer1, layer2, layer3 (early feature extractors)
        \item Train: layer4 (high-level features) + regression head
        \item Rationale: Preserve low-level edge/texture features, adapt high-level semantics
    \end{itemize}
    \item \textbf{Regression head}: Linear(2048→512), ReLU, Dropout(0.3), Linear(512→1)
\end{itemize}

\textbf{Training:}
\begin{itemize}
    \item Optimizer: AdamW (lr=1e-4, weight\_decay=1e-4)
    \item Loss: MSE
    \item Batch size: 12
    \item Early stopping: patience=10 epochs
    \item Validation: Stratified 5-Fold CV
\end{itemize}

\textbf{Results:}
\begin{itemize}
    \item Mean $R^2$: $0.524 \pm 0.046$
    \item Mean MAE: $0.171 \pm 0.012$ km (171 meters)
    \item Mean RMSE: $0.256 \pm 0.014$ km
    \item Best fold: $R^2 = 0.621$ (Fold 0)
    \item Worst fold: $R^2 = 0.487$ (Fold 4)
\end{itemize}

\textbf{Analysis:}
\begin{itemize}
    \item 88\% improvement over Part 4 CNN ($R^2$ 0.524 vs. 0.279)
    \item Pre-training on ImageNet provides strong initialization
    \item Still underperforms physical baseline by $\Delta R^2 = 0.15$
    \item ResNet's convolutional inductive bias may not be optimal for this task
\end{itemize}

\subsubsection{Model: ViT-Tiny Baseline}

\textbf{Architecture:}
\begin{itemize}
    \item \textbf{Backbone}: ViT-Tiny (WinKawaks/vit-tiny-patch16-224) pre-trained on ImageNet-21k
    \item \textbf{Patch size}: $16 \times 16$ (results in 196 patches for $224 \times 224$ input)
    \item \textbf{Input handling}: $440 \times 640$ images resized to $224 \times 224$, grayscale→RGB
    \item \textbf{Fine-tuning}: All layers trainable (ViT is small: 5.7M parameters)
    \item \textbf{Regression head}: Linear(192→256), ReLU, Dropout(0.3), Linear(256→1)
\end{itemize}

\textbf{Training:}
\begin{itemize}
    \item Optimizer: AdamW (lr=3e-5, weight\_decay=1e-4)
    \item Loss: MSE
    \item Batch size: 10
    \item Early stopping: patience=10 epochs
    \item Validation: Stratified 5-Fold CV
\end{itemize}

\textbf{Results:}
\begin{itemize}
    \item Mean $R^2$: $0.577 \pm 0.019$
    \item Mean MAE: $0.166 \pm 0.006$ km (166 meters)
    \item Mean RMSE: $0.241 \pm 0.008$ km
    \item Best fold: $R^2 = 0.599$ (Fold 2)
    \item Worst fold: $R^2 = 0.545$ (Fold 0)
\end{itemize}

\textbf{Analysis:}
\begin{itemize}
    \item 107\% improvement over Part 4 CNN ($R^2$ 0.577 vs. 0.279)
    \item Outperforms ResNet-50 by $\Delta R^2 = +0.053$ (10\% relative)
    \item Vision Transformer's self-attention better captures global spatial relationships
    \item Lower cross-fold variance (std=0.019) indicates more stable generalization
    \item Still below physical baseline by $\Delta R^2 = 0.099$
\end{itemize}

\textbf{WP-1 Conclusion:}

Pre-trained backbones (especially ViT) significantly improve over CNNs from scratch, but single-frame models cannot yet beat the physical baseline. As you will see in the next section, temporal modeling is what we need.

\subsection{Part 2: Temporal Modeling}

\subsubsection{Multi-Frame Temporal ViT}

\textbf{Motivation:}

Cloud base height evolves slowly in time (meteorological timescale: minutes to hours). A sequence of frames should provide:
\begin{itemize}
    \item Temporal smoothing to reduce noise
    \item Motion/parallax cues for 3D geometry estimation
    \item Redundancy to handle partial occlusions
\end{itemize}

\textbf{Architecture:}
\begin{itemize}
    \item \textbf{Frame encoder}: ViT-Tiny (shared weights across frames)
    \item \textbf{Temporal sequence}: 5 consecutive frames (center frame is target)
    \item \textbf{Temporal aggregation}: Multi-head self-attention (4 heads)
    \begin{itemize}
        \item Input: Sequence of 5 frame embeddings [192-dim each]
        \item Output: Aggregated temporal representation [192-dim]
    \end{itemize}
    \item \textbf{Edge handling}: Clamp to flight boundaries (no cross-flight sequences)
    \item \textbf{Regression head}: Linear(192→256), ReLU, Dropout(0.3), Linear(256→1)
\end{itemize}

\textbf{Training:}
\begin{itemize}
    \item Optimizer: AdamW (lr=2e-5, weight\_decay=1e-4)
    \item Loss: MSE (on center frame only)
    \item Batch size: 4 (VRAM limited)
    \item Gradient accumulation: 4 steps (effective batch size = 16)
    \item Early stopping: patience=10 epochs
    \item Validation: Stratified 5-Fold CV
\end{itemize}

\textbf{Results:}
\begin{itemize}
    \item Mean $R^2$: $0.727 \pm 0.052$
    \item Mean MAE: $0.126 \pm 0.008$ km (126 meters)
    \item Mean RMSE: $0.193 \pm 0.021$ km
    \item Best fold: $R^2 = 0.823$ (Fold 4)
    \item Worst fold: $R^2 = 0.677$ (Fold 2)
\end{itemize}

\textbf{First deep learning to beat physical baseline (!!)}
\begin{itemize}
    \item $R^2$ improvement: 0.727 vs. 0.668 = +8.8\% relative (+0.059 absolute)
    \item MAE improvement: 126 m vs. 137 m = 11 meters better (8\% relative)
    \item 26\% improvement over single-frame ViT ($R^2$ 0.727 vs. 0.577)
\end{itemize}

\textbf{Analysis:}
\begin{itemize}
    \item Temporal information is critical for performance
    \item Multi-frame attention learns to aggregate complementary views
    \item Temporal smoothing reduces per-frame noise
    \item High variance across folds (std=0.052) suggests some domain shift remains
\end{itemize}

\subsubsection{Physics-Informed Temporal Consistency Loss}

\textbf{Motivation:}

Real cloud base height changes slowly (typically $< 10$ m/s vertical velocity). We can enforce this physical constraint via a temporal consistency loss:

\begin{equation}
\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{MSE}}(\hat{y}_{\text{center}}, y_{\text{center}}) + \lambda \cdot \mathcal{L}_{\text{temporal}}
\end{equation}

where the temporal consistency loss penalizes rapid changes:

\begin{equation}
\mathcal{L}_{\text{temporal}} = \frac{1}{T-1} \sum_{t=1}^{T-1} |\hat{y}_{t+1} - \hat{y}_t|
\end{equation}

\textbf{Architecture modification:}
\begin{itemize}
    \item Model outputs predictions for all 5 frames (not just center)
    \item Primary loss: MSE on center frame (supervision signal)
    \item Regularization: Temporal consistency across all 5 predictions
\end{itemize}

\textbf{Ablation study:} $\lambda \in \{0.05, 0.1, 0.2\}$

\textbf{Results:}

\begin{table}[h]
\centering
\begin{tabular}{cccc}
\toprule
\textbf{$\lambda$} & \textbf{Mean $R^2$} & \textbf{Mean MAE (m)} & \textbf{Mean RMSE (m)} \\
\midrule
0.05 & $0.700 \pm 0.065$ & $140.5 \pm 17.0$ & $202.2 \pm 22.3$ \\
0.10 & $0.728 \pm 0.044$ & $125.7 \pm 9.2$ & $192.9 \pm 16.8$ \\
0.20 & $0.728 \pm 0.043$ & $130.5 \pm 12.6$ & $192.9 \pm 15.4$ \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Optimal configuration:} $\lambda = 0.1$

\textbf{Analysis:}
\begin{itemize}
    \item $\lambda = 0.05$: Under-regularized, predictions too noisy
    \item $\lambda = 0.10$: Optimal balance, best MAE and lowest variance
    \item $\lambda = 0.20$: Over-regularized, predictions too smooth (worse MAE)
    \item Temporal consistency loss provides marginal improvement: $R^2$ 0.728 vs. 0.727
    \item Main benefit: Reduced cross-fold variance (std=0.044 vs. 0.052)
\end{itemize}

\textbf{Best Model Summary:}

\textit{Temporal ViT + Consistency Loss ($\lambda = 0.1$)}
\begin{itemize}
    \item Mean $R^2$: $0.728 \pm 0.044$
    \item Mean MAE: $126 \, \text{m} \pm 9 \, \text{m}$
    \item Mean RMSE: $193 \, \text{m} \pm 17 \, \text{m}$
\end{itemize}

\subsection{Part3: Advanced Fusion (FiLM)}

\subsubsection{Feature-wise Linear Modulation}

\textbf{Motivation:}

Part 4's attention fusion ($R^2 = 0.326$) failed to effectively integrate ERA5 features. FiLM (Feature-wise Linear Modulation) is a more expressive fusion mechanism:

\begin{equation}
\text{FiLM}(x, z) = \gamma(z) \odot x + \beta(z)
\end{equation}

where $x$ is the image features, $z$ is the ERA5 features, and $\gamma, \beta$ are learned parameters.

\textbf{Architecture:}
\begin{itemize}
    \item \textbf{Image encoder}: ViT-Tiny (192-dim embeddings)
    \item \textbf{ERA5 encoder}: MLP: Linear(9→64), ReLU, LayerNorm, Linear(64→64)
    \item \textbf{FiLM generator}:
    \begin{itemize}
        \item Input: 64-dim ERA5 encoding
        \item Gamma branch: Linear(64→192), Sigmoid (scale near 1.0)
        \item Beta branch: Linear(64→192), no activation
    \end{itemize}
    \item \textbf{Modulation}: $x_{\text{fused}} = \gamma \odot x_{\text{image}} + \beta$
    \item \textbf{Regression head}: Linear(192→256), ReLU, Dropout(0.3), Linear(256→1)
\end{itemize}

\textbf{Implementation details:}
\begin{itemize}
    \item ERA5 features normalized (z-score) before encoding
    \item Gamma gating (sigmoid) ensures $\gamma \approx 1.0$ initially (identity initialization)
    \item Gradient clipping (max\_norm=1.0) for training stability
    \item LayerNorm in FiLM generator to prevent exploding activations
\end{itemize}

\textbf{Training:}
\begin{itemize}
    \item Optimizer: AdamW (lr=3e-5, weight\_decay=1e-4)
    \item Loss: MSE
    \item Batch size: 10
    \item Early stopping: patience=10 epochs
    \item Validation: Stratified 5-Fold CV
\end{itemize}

\textbf{Results:}
\begin{itemize}
    \item Mean $R^2$: $0.542 \pm 0.026$
    \item Mean MAE: $0.166 \pm 0.006$ km (166 meters)
    \item Mean RMSE: $0.251 \pm 0.009$ km
    \item Best fold: $R^2 = 0.589$ (Fold 1)
    \item Worst fold: $R^2 = 0.507$ (Fold 0)
\end{itemize}

\textbf{Analysis:}
\begin{itemize}
    \item FiLM underperforms image-only ViT ($R^2$ 0.542 vs. 0.577, $\Delta = -0.035$)
    \item ERA5 features do not improve single-frame models
    \item Possible explanations:
    \begin{itemize}
        \item ERA5 spatial resolution (25 km) too coarse for imagery (200 m pixels)
        \item Hourly temporal resolution misses sub-hourly cloud dynamics
        \item Atmospheric state may not strongly constrain cloud base height
        \item Better fusion may require cross-modal attention (not just affine modulation)
    \end{itemize}
    \item Training was stable (no gradient explosions after fixes)
\end{itemize}

\textbf{WP-3 Conclusion:}

FiLM fusion validates proper integration of ERA5 features but does not improve performance. Atmospheric features may be most useful in physical models (GBDT), not deep learning. Future work should explore cross-modal attention.

\subsection{Part 5 Summary}

\textbf{Model Performance Ranking:}

\begin{table}[h]
\centering
\begin{tabular}{clccc}
\toprule
\textbf{Rank} & \textbf{Model} & \textbf{$R^2$} & \textbf{MAE (m)} & \textbf{RMSE (m)} \\
\midrule
1 & Temporal ViT + Consistency & 0.728 & 126 & 193 \\
2 & Temporal ViT & 0.727 & 126 & 193 \\
-- & Physical GBDT (baseline) & 0.668 & 137 & 213 \\
3 & ViT-Tiny & 0.577 & 166 & 241 \\
4 & FiLM Fusion & 0.542 & 166 & 251 \\
5 & ResNet-50 & 0.524 & 171 & 256 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key findings:}
\begin{enumerate}
    \item \textbf{Temporal modeling is critical}: +26\% $R^2$ improvement over single-frame
    \item \textbf{ViT outperforms ResNet}: Attention-based architectures superior for this task
    \item \textbf{Physics-informed losses help marginally}: Temporal consistency reduces variance
    \item \textbf{ERA5 fusion remains unsolved}: FiLM does not improve over image-only
    \item \textbf{Production recommendation}: Temporal ViT + Consistency ($\lambda = 0.1$)
\end{enumerate}

\newpage

\section{Cross-Part/Task Analysis}

\subsection{Performance Evolution}

$R^2$ progression across test:

\begin{itemize}
    \item Part 3: Physical baseline = 0.668
    \item Part 4: Best CNN (attention) = 0.326 ($-51\%$ vs. baseline)
    \item Part 5: Pre-trained ViT = 0.577 ($-14\%$ vs. baseline)
    \item Part 5: Temporal ViT = 0.727 ($+9\%$ vs. baseline) \checkmark
\end{itemize}

\textbf{Key insights:}
\begin{enumerate}
    \item \textbf{Transfer learning is essential}: Pre-training on ImageNet provides critical initialization for small datasets (933 samples)
    \item \textbf{Temporal context is the breakthrough}: Single-frame models cannot compete with multi-frame temporal attention
    \item \textbf{Architecture matters}: ViT (self-attention) outperforms ResNet (convolution) by 10\% for this task
    \item \textbf{Physical features excel in classical ML}: GBDT effectively combines geometric + atmospheric features, but deep learning struggles to fuse modalities
    \item \textbf{Dataset size is a bottleneck}: 933 samples is small for deep learning; pre-training mitigates but does not eliminate this limitation
\end{enumerate}

\subsection{Feature Importance: Physical vs. Learned}

\textbf{Physical GBDT top features:}
\begin{enumerate}
    \item BLH (boundary layer height)
    \item Shadow-derived geometric height
    \item Solar zenith angle
    \item LCL (lifting condensation level)
\end{enumerate}

\textbf{Deep learning attention maps (qualitative):}
\begin{itemize}
    \item ViT looks to cloud edges and shadow boundaries
    \item Temporal ViT shows strong attention to motion/parallax
    \item Limited attention to atmospheric context (consistent with FiLM failure)
\end{itemize}

\textbf{Conclusion:} Physical models excel at explicit feature engineering, while deep learning discovers geometric features implicitly (but requires temporal context to succeed).

\subsection{Computational Requirements}

\begin{table}[h]
\centering
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{Parameters} & \textbf{Train Time/Fold} & \textbf{Inference (ms)} & \textbf{VRAM (GB)} \\
\midrule
Physical GBDT & -- & 2 min & 0.1 & -- \\
CNN (Section 4) & 1.2M & 15 min & 5 & 2 \\
ResNet-50 & 23.5M & 45 min & 15 & 4 \\
ViT-Tiny & 5.7M & 30 min & 8 & 3 \\
Temporal ViT & 6.1M & 60 min & 40 & 6 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Production considerations:}
\begin{itemize}
    \item \textbf{Physical GBDT}: Fastest, lowest resource, production-ready
    \item \textbf{Temporal ViT}: Best accuracy, but $400\times$ slower inference
    \item \textbf{Trade-off}: +8.8\% $R^2$ improvement costs $400\times$ latency
    \item \textbf{Recommendation}: Deploy both models (GBDT for real-time, ViT for post-processing)
\end{itemize}

\subsection{Validation Protocol Robustness}

\textbf{Stratified K-Fold CV stability:}

All models trained with stratified 5-fold CV (CBH target binned into 10 quantiles). Cross-fold variance analysis:

\begin{table}[h]
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Model} & \textbf{Mean $R^2$} & \textbf{Std $R^2$} \\
\midrule
Physical GBDT & 0.668 & 0.044 \\
ViT-Tiny & 0.577 & 0.019 \\
Temporal ViT & 0.727 & 0.052 \\
Temporal + Consistency & 0.728 & 0.044 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Observation:} Temporal consistency loss reduces variance (0.052→0.044), matching GBDT stability.

\textbf{Domain shift (Flight F4):}

Flight F4 has anomalously low CBH (mean = 0.697 km vs. 0.917 km for F0). Leave-One-Flight-Out CV on F4 yields catastrophic failure ($R^2 = -3.13$), confirming extreme domain shift. Stratified K-Fold CV is the correct validation protocol for model development.

\newpage

\section{Data}

\subsection{ERA5 Processing Details}

\textbf{File:} \texttt{sow\_outputs/wp2\_atmospheric/WP2\_Features.hdf5}

\textbf{Processing pipeline:}
\begin{enumerate}
    \item ERA5 NetCDF files downloaded from ECMWF (119 daily files, Oct 2024 -- Feb 2025)
    \item Spatiotemporal matching: Nearest neighbor (lat/lon) + nearest hourly timestamp
    \item Feature extraction: BLH, temperature, dewpoint, pressure (surface + levels)
    \item Derived features: LCL, inversion height, stability index, moisture gradient
    \item Quality control: 933/933 samples successfully matched (100\%)
\end{enumerate}

\textbf{Verification:}
\begin{itemize}
    \item Mean BLH = 658 m (physically realistic for marine boundary layer)
    \item LCL correlates with observed CBH ($r = 0.42$)
    \item Stability index consistent with stratocumulus regime (3.81 K/km)
    \item All values within expected ranges for coastal/oceanic conditions
\end{itemize}

\newpage

\section{Deployment Recommendations}

\subsection{Recommended Model Architecture}

\textbf{Primary Model:} Temporal ViT with Temporal Consistency Loss ($\lambda = 0.1$)

\textbf{Performance:}
\begin{itemize}
    \item $R^2 = 0.728$ (vs. 0.668 baseline = +9\% improvement)
    \item MAE = 126 m (vs. 137 m baseline = 11 m better)
    \item RMSE = 193 m (vs. 213 m baseline = 20 m better)
\end{itemize}

\textbf{Architecture details:}
\begin{itemize}
    \item Frame encoder: ViT-Tiny (WinKawaks/vit-tiny-patch16-224)
    \item Input: 5-frame sequence (temporal window $\approx$ 10 seconds at 2 Hz)
    \item Temporal aggregation: 4-head self-attention
    \item Loss: MSE + $0.1 \times$ temporal consistency
    \item Parameters: 6.1M (deployable on edge devices)
\end{itemize}

\textbf{Model checkpoints:}
\begin{itemize}
    \item 5 fold models saved: \texttt{sow\_outputs/wp5/models/temporal\_consistency/lambda\_0.1/fold\_*.pth}
    \item Best fold: Fold 1 ($R^2 = 0.782$)
    \item Recommended for deployment: Ensemble of all 5 folds (vote averaging)
\end{itemize}

\subsection{Alternative: Dual-Model Deployment}

\textbf{Operational strategy:}

Deploy both Physical GBDT and Temporal ViT in production:

\begin{itemize}
    \item \textbf{Real-time processing}: Physical GBDT (0.1 ms inference, CPU-only)
    \begin{itemize}
        \item Use for immediate CBH estimates during flight
        \item Requires only geometric features (shadow detection)
        \item Fallback when temporal context unavailable (e.g., first frames)
    \end{itemize}
    \item \textbf{Post-processing}: Temporal ViT (40 ms inference, GPU recommended)
    \begin{itemize}
        \item Apply after flight for highest-accuracy retrievals
        \item Requires 5-frame buffer (2.5 seconds at 2 Hz)
        \item Primary output for scientific analysis
    \end{itemize}
\end{itemize}

\subsection{Known Limitations and Mitigations}

\textbf{Limitation 1: Domain shift (Flight F4)}
\begin{itemize}
    \item \textbf{Issue}: F4 has anomalously low CBH (mean = 0.697 km vs. 0.917 km)
    \item \textbf{Impact}: LOO CV on F4 yields $R^2 = -3.13$ (catastrophic failure)
    \item \textbf{Mitigation}: Stratified K-Fold CV for development; few-shot fine-tuning for deployment on new flight regimes
\end{itemize}

\textbf{Limitation 2: Temporal edge cases}
\begin{itemize}
    \item \textbf{Issue}: First 2 frames of each flight lack full 5-frame context
    \item \textbf{Mitigation}: Fallback to Physical GBDT for edge frames; use padding/extrapolation
\end{itemize}

\textbf{Limitation 3: Computational cost}
\begin{itemize}
    \item \textbf{Issue}: Temporal ViT is $400\times$ slower than GBDT (40 ms vs. 0.1 ms)
    \item \textbf{Mitigation}: Deploy dual-model system (GBDT real-time, ViT post-processing)
\end{itemize}

\textbf{Limitation 4: ERA5 fusion failure}
\begin{itemize}
    \item \textbf{Issue}: FiLM fusion ($R^2 = 0.542$) underperforms image-only ViT ($R^2 = 0.577$)
    \item \textbf{Mitigation}: Use ERA5 features only in GBDT; explore cross-modal attention in future work
\end{itemize}

\newpage

\section{Future Work}

\subsection{High Priority}

\begin{enumerate}
    \item \textbf{Uncertainty Quantification}
    \begin{itemize}
        \item Implement Monte Carlo Dropout for prediction confidence estimates
        \item Calibrate uncertainty to some operational requirements (e.g., 90\% confidence intervals)
        \item Use uncertainty to flag low-confidence predictions for manual review (can tie into current paper)
    \end{itemize}

    \item \textbf{Cross-Modal Attention}
    \begin{itemize}
        \item Implement Query(image) $\times$ Key/Value(ERA5) attention
        \item Compare to FiLM fusion (current best $R^2 = 0.542$)
        \item Hypothesis: Explicit attention to atmospheric features may improve over just modulation
    \end{itemize}

    \item \textbf{Domain Adaptation for Flight F4}
    \begin{itemize}
        \item Few-shot fine-tuning on low-CBH regimes
        \item Meta-learning for rapid adaptation to new flight conditions
        \item Investigate domain-invariant representations
    \end{itemize}

    \item \textbf{Ensemble Methods}
    \begin{itemize}
        \item Combine Temporal ViT + Physical GBDT (stacking or weighted averaging)
        \item Expected performance: $R^2 > 0.74$
        \item Improved robustness and uncertainty estimates
    \end{itemize}
\end{enumerate}

\subsection{Medium Priority (Research Extensions)}

\begin{enumerate}
    \setcounter{enumi}{4}
    \item \textbf{Model Compression and Optimization}
    \begin{itemize}
        \item TorchScript / ONNX export for deployment
        \item Quantization (INT8) for faster inference
        \item Knowledge distillation: Temporal ViT $\rightarrow$ some smaller student model
        \item Target: $10\times$ speedup with $< 5\%$ accuracy loss (only really good to do if you want real time inference for during flight)
    \end{itemize}

    \item \textbf{Explainability and Visualization}
    \begin{itemize}
        \item Attention map visualization (which image regions drive predictions?)
        \item Temporal attention flow (how does model integrate multi-frame information?)
    \end{itemize}

    \item \textbf{Self-Supervised Pre-Training}
    \begin{itemize}
        \item Revisit earlier research on self-supervised learning (MAE, contrastive learning)
        \item Pre-train ViT on unlabeled ER-2 imagery (10,000+ frames available)
        \item Fine-tune on labeled CBH data (933 samples)
        \item Hypothesis: Domain-specific pre-training may improve over ImageNet initialization
    \end{itemize}
\end{enumerate}

\subsection{Low Priority (Exploratory Research)}

\begin{enumerate}
    \setcounter{enumi}{7}
    \item \textbf{Alternative Architectures}
    \begin{itemize}
        \item Mamba / S4 (state-space models for efficient sequence modeling)
        \item Video Swin Transformer (hierarchical spatiotemporal attention)
        \item 3D CNNs (spatiotemporal convolution)
    \end{itemize}

    \item \textbf{Multi-Task Learning}
    \begin{itemize}
        \item Joint prediction: CBH + cloud optical depth + cloud top height
        \item Auxiliary tasks: Shadow detection, cloud segmentation
        \item Hypothesis: Shared representations may improve generalization
    \end{itemize}

    \item \textbf{Just Get More Data}
    \begin{itemize}
        \item Just test on the new aquisition system Drew made - ideally would give better predictions but also very different data from what we have now
    \end{itemize}
\end{enumerate}

\newpage

\section{Conclusions}

\subsection{Summary of Achievements}

\begin{enumerate}
    \item \textbf{Physical baseline established}: XGBoost GBDT with geometric and atmospheric features achieves $R^2 = 0.668$, MAE = 137 m
    \item \textbf{Hybrid CNN development}: Attention fusion outperforms naive concatenation but cannot beat physical baseline
    \item \textbf{Pre-trained backbones}: ResNet-50 and ViT-Tiny significantly improve over CNNs from scratch
    \item \textbf{Breakthrough with temporal modeling}: Temporal ViT is the first deep learning model to beat the physical baseline, achieving $R^2 = 0.727$, MAE = 126 m
    \item \textbf{Physics-informed regularization}: Temporal consistency loss (Task 2.2) provides marginal improvement and reduced variance
    \item \textbf{Production-ready model}: Temporal ViT + Consistency Loss ($\lambda = 0.1$) recommended for deployment with $R^2 = 0.728$, MAE = 126 m
\end{enumerate}

\subsection{Critical Findings}

\begin{itemize}
    \item \textbf{Temporal information is essential}: Multi-frame models outperform single-frame by 26\% $R^2$
    \item \textbf{Vision Transformers excel}: ViT systematically outperforms ResNet for this task
    \item \textbf{Transfer learning critical}: ImageNet pre-training provides crucial initialization for small datasets
    \item \textbf{ERA5 fusion remains challenging}: Atmospheric features improve GBDT but not deep learning models
\end{itemize}

\subsection{Final Remarks}

The transition from physical baselines ($R^2 = 0.668$) to advanced deep learning with temporal modeling ($R^2 = 0.728$) represents a \textbf{9\% performance improvement} and validates the deep learning approach for cloud base height retrieval.

The Temporal ViT model is recommended for production deployment pending offline validation and pilot testing. Future work should focus on uncertainty quantification, domain adaptation, and ensemble methods to further improve robustness and operational readiness. (real test will be when I feed in new data to this model)

\end{document}
