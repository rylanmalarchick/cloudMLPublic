# SimCLR Pretraining Configuration for Paper 2
# 
# Research Question: Can unsupervised contrastive learning improve
# cross-flight generalization? (Baseline cross-flight RÂ² = -0.98)
#
# Target: 1070Ti GPU (8GB VRAM) - overnight training
#
# Usage:
#   python experiments/paper2/simclr_pretrain.py --config configs/paper2_simclr.yaml

# =============================================================================
# Data Paths
# =============================================================================
ssl_data_path: "data_ssl/images/train.h5"
output_dir: "outputs/paper2_simclr"

# =============================================================================
# Model Architecture
# =============================================================================
# SimCLR encoder configuration
feature_dim: 256        # Backbone output dimension (for downstream tasks)
projection_dim: 128     # Projection head output (for contrastive loss)
hidden_dim: 256         # Hidden dimension in projection MLP
temperature: 0.5        # NT-Xent temperature (lower = harder negatives)

# =============================================================================
# Training Parameters
# =============================================================================
# Optimized for 1070Ti (8GB VRAM) with 58,846 samples
batch_size: 256         # ~2.5GB VRAM usage, good for contrastive learning
num_epochs: 100         # ~4-6 hours on 1070Ti
learning_rate: 1.0e-3   # LARS-style LR (scaled with batch size)
weight_decay: 1.0e-4    # Light regularization
warmup_epochs: 10       # 10% warmup

# =============================================================================
# Augmentation Parameters
# =============================================================================
# Tuned for small (20x22) cloud radiance images
noise_std: 0.1                    # Gaussian noise relative to image std
brightness_range: [0.8, 1.2]      # Random brightness multiplier
contrast_range: [0.8, 1.2]        # Random contrast multiplier
blur_prob: 0.5                    # Probability of Gaussian blur

# =============================================================================
# System Configuration
# =============================================================================
num_workers: 4          # DataLoader workers
device: "cuda"          # Use GPU
seed: 42                # Reproducibility
save_every: 10          # Save checkpoint every N epochs
log_every: 50           # Log every N batches

# =============================================================================
# Alternative Configurations (commented out)
# =============================================================================

# --- Small batch (for debugging/CPU) ---
# batch_size: 32
# num_epochs: 10
# num_workers: 2
# device: "cpu"

# --- Large batch (for better GPUs) ---
# batch_size: 512
# learning_rate: 2.0e-3
# warmup_epochs: 5

# --- Longer training ---
# num_epochs: 200
# save_every: 20
