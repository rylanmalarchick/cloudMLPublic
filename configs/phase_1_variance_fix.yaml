# --- PHASE 1 EXPERIMENT: VARIANCE COLLAPSE FIX ---
# This config is designed for a short validation run (10 epochs) to test
# the effectiveness of variance loss in preventing model collapse.
#
# Key changes from colab_optimized_full_tuned.yaml:
#   - epochs: 10 (short run for validation)
#   - variance_lambda: 2.0 (enables and weights the variance penalty)
#   - min_variance_ratio: 0.1 (safety net to halt training if collapse occurs)

# --- File Paths ---
data_directory: "/content/drive/MyDrive/CloudML/data/"
output_directory: "/content/drive/MyDrive/CloudML/plots/"
models_directory: "/content/drive/MyDrive/CloudML/models/trained/"
log_dir: "/content/drive/MyDrive/CloudML/logs/"

# --- Experiment Parameters ---
pretrain_flight: "30Oct24"
overweight_flight: "30Oct24"
overweight_factor: 1.0
validation_flight: "12Feb25"

# --- Training Parameters ---
learning_rate: 0.0005
weight_decay: 0.04
epochs: 10 # SHORT RUN FOR VALIDATION
optimizer: "AdamW"
scheduler: "ReduceLROnPlateau"
temporal_frames: 3
loss_type: "huber"
loss_alpha: 0.5
variance_lambda: 2.0 # ACTIVELY PENALIZE VARIANCE COLLAPSE
min_variance_ratio: 0.1 # SAFETY NET - STOP IF VARIANCE < 10%
early_stopping_patience: 5 # Shorter patience for 10-epoch run
filter_type: "basic"
early_stopping_min_delta: 0.0005
huber_delta: 1.0
gamma: 1.5
under_weight: 1.0
loo: false
loo_epochs: 30
batch_size: 20
warmup_steps: 500
hard_mining_k: 0.35
hard_mining_factor: 1.5
swath_slice: [40, 480]
hpc_mode: true

# --- Memory & Performance Optimization ---
memory_optimized: false
gradient_checkpointing: true
torch_compile: true
torch_compile_mode: "default"

# --- Data Loading Optimization ---
num_workers: 2
pin_memory: true
prefetch_factor: 3

# --- Model Architecture ---
use_spatial_attention: true
use_temporal_attention: true
use_multiscale_temporal: true
attention_heads: 4
flat_field_correction: true
clahe_clip_limit: 0.01
zscore_normalize: true
augment: true
angles_mode: "both"

architecture:
  name: "transformer"

# --- Flight Information ---
flights:
  - name: "10Feb25"
    iFileName: "10Feb25/GLOVE2025_IRAI_L1B_Rev-_20250210.h5"
    cFileName: "10Feb25/CPL_L2_V1-02_01kmLay_259015_10feb25.hdf5"
    nFileName: "10Feb25/CRS_20250210_nav.hdf"
  - name: "30Oct24"
    iFileName: "30Oct24/WHYMSIE2024_IRAI_L1B_Rev-_20241030.h5"
    cFileName: "30Oct24/CPL_L2_V1-02_01kmLay_259006_30oct24.hdf5"
    nFileName: "30Oct24/CRS_20241030_nav.hdf"
  - name: "04Nov24"
    iFileName: "04Nov24/WHYMSIE2024_IRAI_L1B_Rev-_20241104.h5"
    cFileName: "04Nov24/CPL_L2_V1-02_01kmLay_259008_04nov24.hdf5"
    nFileName: "04Nov24/CRS_20241104_nav.hdf"
  - name: "23Oct24"
    iFileName: "23Oct24/WHYMSIE2024_IRAI_L1B_Rev-_20241023.h5"
    cFileName: "23Oct24/CPL_L2_V1-02_01kmLay_259004_23oct24.hdf5"
    nFileName: "23Oct24/CRS_20241023_nav.hdf"
  - name: "18Feb25"
    iFileName: "18Feb25/GLOVE2025_IRAI_L1B_Rev-_20250218.h5"
    cFileName: "18Feb25/CPL_L2_V1-02_01kmLay_259017_18feb25.hdf5"
    nFileName: "18Feb25/CRS_20250218_nav.hdf"
  - name: "12Feb25"
    iFileName: "12Feb25/GLOVE2025_IRAI_L1B_Rev-_20250212.h5"
    cFileName: "12Feb25/CPL_L2_V1-02_01kmLay_259016_12feb25.hdf5"
    nFileName: "12Feb25/CRS_20250212_nav.hdf"

# --- Self-Supervised Pre-training ---
pretraining:
  enabled: false
  epochs: 20
  learning_rate: 0.0001
  batch_size: 8
  save_checkpoints: true
  checkpoint_dir: "/content/drive/MyDrive/CloudML/models/pretrained/"
# --- PHASE 1 EXPERIMENT NOTES ---
# OBJECTIVE: Validate that variance loss prevents model collapse
#
# BASELINE (from diagnostics):
#   - GradientBoosting: R² = 0.777 (simple model works!)
#   - Previous NN runs: R² < 0 (all negative - collapsed)
#
# THIS EXPERIMENT:
#   - 10 epochs only (quick validation)
#   - variance_lambda = 2.0 (strong penalty on uniform predictions)
#   - min_variance_ratio = 0.1 (circuit breaker if collapse detected)
#
# SUCCESS CRITERIA:
#   - R² > 0 and increasing (proves fix is working)
#   - Target: R² > 0.1 for this short run
#
# IF SUCCESSFUL:
#   - Scale up to 50 epochs with hyperparameter tuning
#
# IF UNSUCCESSFUL:
#   - Try Phase 2: simpler architecture (disable attention)
