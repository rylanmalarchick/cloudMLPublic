# --- GPU-Optimized Config for Google Colab T4 (FULL MODEL - TUNED) ---
# This config is tuned based on first baseline run analysis:
# - Issue: Negative R² (-0.0927), erratic validation loss
# - Root causes: LR too high, aggressive overweighting, long warmup
# - Improvements: Lower LR, reduced warmup, gentler overweighting, tighter early stopping

# --- File Paths ---
data_directory: "/content/drive/MyDrive/CloudML/data/"
output_directory: "/content/drive/MyDrive/CloudML/plots/"
models_directory: "/content/drive/MyDrive/CloudML/models/trained/"
log_dir: "/content/drive/MyDrive/CloudML/logs/"

# --- Experiment Parameters ---
pretrain_flight: "30Oct24"
overweight_flight: "30Oct24"
overweight_factor: 1.0 # PHASE 1 FIX: Disabled overweighting (was causing conservative predictions)
validation_flight: "12Feb25"

# --- Training Parameters ---
learning_rate: 0.0005 # TUNED: Reduced from 0.001 (more stable, less overshooting)
weight_decay: 0.04
epochs: 50
optimizer: "AdamW"
scheduler: "ReduceLROnPlateau"
temporal_frames: 3 # PHASE 1 FIX: Reduced from 7 to 3 (7 frames caused over-averaging/variance collapse)
loss_type: "huber"
loss_alpha: 0.5
variance_lambda: 2.0 # RUN 5 FIX: Increased from 0.5 (was too weak - variance still collapsed 46%→8%)
min_variance_ratio: 0.35 # RUN 5 FIX: Minimum variance ratio required to save model (prevents collapsed models)
early_stopping_patience: 10 # TUNED: Reduced from 15 (stop sooner when not improving)
filter_type: "basic"
early_stopping_min_delta: 0.0005 # TUNED: Increased from 0.00025 (require more significant improvement)
huber_delta: 1.0
gamma: 1.5
under_weight: 1.0
loo: false
loo_epochs: 30
batch_size: 20
warmup_steps: 500 # TUNED: Reduced from 2000 (start learning sooner)
hard_mining_k: 0.35
hard_mining_factor: 1.5
swath_slice: [40, 480]
hpc_mode: true

# --- Memory & Performance Optimization ---
memory_optimized: false # Use FULL model (64/128/256 channels)
gradient_checkpointing: true # Saves ~30-40% memory during backprop
torch_compile: true # 20-30% speedup + 10-15% memory reduction
torch_compile_mode: "default" # Compatible with gradient checkpointing

# --- Data Loading Optimization ---
num_workers: 2
pin_memory: true
prefetch_factor: 3

# --- Model Architecture ---
use_spatial_attention: true
use_temporal_attention: true
use_multiscale_temporal: true # TIER 1: Enable multi-scale temporal attention
attention_heads: 4 # TIER 1: Number of attention heads for multi-scale attention
flat_field_correction: true
clahe_clip_limit: 0.01
zscore_normalize: true
augment: true
angles_mode: "both"

architecture:
  name: "transformer"

# --- Flight Information ---
flights:
  - name: "10Feb25"
    iFileName: "10Feb25/GLOVE2025_IRAI_L1B_Rev-_20250210.h5"
    cFileName: "10Feb25/CPL_L2_V1-02_01kmLay_259015_10feb25.hdf5"
    nFileName: "10Feb25/CRS_20250210_nav.hdf"
  - name: "30Oct24"
    iFileName: "30Oct24/WHYMSIE2024_IRAI_L1B_Rev-_20241030.h5"
    cFileName: "30Oct24/CPL_L2_V1-02_01kmLay_259006_30oct24.hdf5"
    nFileName: "30Oct24/CRS_20241030_nav.hdf"
  - name: "04Nov24"
    iFileName: "04Nov24/WHYMSIE2024_IRAI_L1B_Rev-_20241104.h5"
    cFileName: "04Nov24/CPL_L2_V1-02_01kmLay_259008_04nov24.hdf5"
    nFileName: "04Nov24/CRS_20241104_nav.hdf"
  - name: "23Oct24"
    iFileName: "23Oct24/WHYMSIE2024_IRAI_L1B_Rev-_20241023.h5"
    cFileName: "23Oct24/CPL_L2_V1-02_01kmLay_259004_23oct24.hdf5"
    nFileName: "23Oct24/CRS_20241023_nav.hdf"
  - name: "18Feb25"
    iFileName: "18Feb25/GLOVE2025_IRAI_L1B_Rev-_20250218.h5"
    cFileName: "18Feb25/CPL_L2_V1-02_01kmLay_259017_18feb25.hdf5"
    nFileName: "18Feb25/CRS_20250218_nav.hdf"
  - name: "12Feb25"
    iFileName: "12Feb25/GLOVE2025_IRAI_L1B_Rev-_20250212.h5"
    cFileName: "12Feb25/CPL_L2_V1-02_01kmLay_259016_12feb25.hdf5"
    nFileName: "12Feb25/CRS_20250212_nav.hdf"

# --- Self-Supervised Pre-training (TIER 1) ---
pretraining:
  enabled: false # FIXED: Disabled until single-frame bug is resolved
  epochs: 20 # Number of pre-training epochs
  learning_rate: 0.0001 # Lower LR for pre-training
  batch_size: 8 # Smaller batch size for pre-training to avoid OOM (normal training uses 20)
  save_checkpoints: true
  checkpoint_dir: "/content/drive/MyDrive/CloudML/models/pretrained/"
# --- Notes ---
# PHASE 1 FIXES (Oct 20, 2024) - Critical fixes for variance collapse
#
# PROBLEM IDENTIFIED: Runs 1-3 all showed variance collapse (predictions near-constant)
# - Run 1 (broken SSL): R²=-0.045, variance ratio=40% (BEST!)
# - Run 2 (std=0.01): R²=-0.023, variance ratio=3.9% (near-total collapse)
# - Run 3 (std=0.1): R²=-0.203, variance ratio=3.4% (WORST - catastrophic collapse)
#
# ROOT CAUSE: Loss function didn't penalize variance collapse
# - All runs achieved similar val_loss (~0.51-0.53)
# - But R² varied 4.5x! Val loss is NOT correlated with model quality
# - Model learned to predict ~0.72 for everything → low MAE, terrible R²
#
# PHASE 1 FIXES APPLIED:
# 1. variance_lambda: 0.5 - Add variance-preserving term to loss
#    loss = base_loss + 0.5 * (1 - pred_var/target_var)²
#    Prevents model from collapsing to constant predictions
#
# 2. temporal_frames: 7 → 3 - Reduce over-smoothing
#    7 frames averaged out too much variance
#    Start with 3, test 5 later if needed
#
# 3. overweight_factor: 1.5 → 1.0 - Disable overweighting
#    May have encouraged conservative predictions (avoid risky high values)
#
# 4. Output init: std=0.1 → 0.5 (in pytorchmodel.py)
#    Run 1's larger weights worked 10x better than Runs 2-3
#
# 5. Early stopping: Now tracks R² instead of val_loss (in train_model.py)
#    Val loss doesn't reflect quality; R² does
#
# 6. Monitoring: Added variance_ratio, pred_std, pred_range logging
#    Emergency stop if variance_ratio < 5% (catches collapse early)
#
# EXPECTED RESULTS:
# - R² > 0.0 (minimum - better than mean)
# - Variance ratio > 40% (better than Run 1)
# - Prediction spread > 0.5
# - No near-constant predictions
#
# SUCCESS CRITERIA:
# - Minimum viable: R² > 0.3, variance ratio > 70%
# - Good: R² > 0.5, variance ratio > 85%
#
# --- Original Notes Below ---
# TUNED CONFIGURATION based on baseline run analysis + TIER 1 IMPROVEMENTS
#
# Changes from colab_optimized_full.yaml:
# 1. learning_rate: 0.001 → 0.0005
#    - Reason: LR ramped to 0.000015 caused erratic val loss
#    - Impact: More stable training, less overshooting
#
# 2. warmup_steps: 2000 → 500
#    - Reason: 2000 steps delays actual learning too long
#    - Impact: Model starts learning sooner
#
# 3. overweight_factor: 3.5 → 2.0
#    - Reason: 3.5x too aggressive, may cause catastrophic forgetting
#    - Impact: Gentler fine-tuning, better retention of pretrained features
#
# 4. early_stopping_patience: 15 → 10
#    - Reason: First run wasted 15 epochs with no improvement
#    - Impact: Stop sooner, save time
#
# 5. early_stopping_min_delta: 0.00025 → 0.0005
#    - Reason: Require more significant improvement to count as progress
#    - Impact: Less sensitivity to noise, clearer improvement signal
#
# TIER 1 IMPROVEMENTS (Literature-driven):
# 1. temporal_frames: 5 → 7
#    - Based on Himawari-8 paper (used 6 input frames)
#    - More spatial views = better shadow height triangulation
#
# 2. use_multiscale_temporal: true
#    - Based on Himawari-8 paper multi-scale processing
#    - Captures cross-view relationships at different scales
#    - Expected: +5-10% R² improvement
#
# 3. Self-supervised pre-training enabled
#    - Based on LSTM Autoencoder paper (two-stage learning)
#    - Encoder learns spatial features via reconstruction before supervised training
#    - Expected: +10-20% R² improvement
#
# Combined TIER 1 expected improvements:
# - R² improvement: +15-25% (target: 0.15-0.25)
# - More stable validation loss curve
# - Better feature representations
# - Less reliance on limited labeled data
#
# First run results (for comparison):
# - R²: -0.0927 (worse than mean baseline)
# - MAE: 0.3415 km
# - RMSE: 0.5046 km
# - Best val loss: 0.9440 at epoch 14
# - Then 15 epochs of no improvement
#
# Target for TIER 1 run:
# - R² > 0.15 (meaningful predictive power)
# - MAE < 0.30 km
# - RMSE < 0.45 km
# - Stable val loss curve
#
# If still poor results, try:
# - Reduce temporal_frames to 4 (more training samples)
# - Switch to MAE loss (simpler than Huber)
# - Disable one attention mechanism (reduce overfitting)
# - Use memory_optimized model (less capacity = less overfitting)
#
# Monitor with: !nvidia-smi
